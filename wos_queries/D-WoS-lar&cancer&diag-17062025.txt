FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Trager, Megan H.
   Gordon, Emily R.
   Breneman, Alyssa
   Kim, Esther
   Samie, Faramarz H.
TI Accuracy of ChatGPT in diagnosis and management of dermoscopic images
SO ARCHIVES OF DERMATOLOGICAL RESEARCH
VL 317
IS 1
AR 184
DI 10.1007/s00403-024-03729-z
DT Letter
PD JAN 7 2025
PY 2025
ZA 0
ZR 0
Z8 0
ZB 0
ZS 0
TC 0
Z9 0
DA 2025-01-15
UT WOS:001392999400007
PM 39774990
ER

PT J
AU Yuan, Yue
   Zhang, Guolong
   Gu, Yuqi
   Hao, Sicheng
   Huang, Chen
   Xie, Hongxia
   Mi, Wei
   Zeng, Yingchun
TI Artificial intelligence-assisted machine learning models for predicting
   lung cancer survival
SO ASIA-PACIFIC JOURNAL OF ONCOLOGY NURSING
VL 12
AR 100680
DI 10.1016/j.apjon.2025.100680
EA MAR 2025
DT Article
PD DEC 2025
PY 2025
AB Objective: This study aimed to evaluate the feasibility of large
   language model-Advanced Data Analysis (ADA) in developing and
   implementing machine learning models to predict survival outcomes for
   lung cancer patients, with a focus on its implications for nursing
   practice. Methods: A retrospective study design was employed using a
   dataset of lung cancer patients. Data included sociodemographic,
   clinical, treatment-specific, and comorbidity variables. Large language
   model-ADA was used to build and evaluate three machine learning models.
   Model performance was validated, and results were presented using
   calibration plots. Results: Of 737 patients, the survival rate of this
   cohort was 73.3%, with a mean age of 59.32 years. Calibration plots
   indicated robust model reliability across all models. The Random Forest
   model demonstrated the highest predictive accuracy among the models.
   Most critical features identified were preoperative white blood cells
   (2.2%), preoperative lung function of Forced Expiratory Volume in one
   second (2.1%), preoperative arterial oxygen saturation (1.9%),
   preoperative partial pressure of oxygen (1.7%), preoperative albumin
   (1.6%), preoperative preparation time (1.5%), age at admission (1.5%),
   preoperative partial pressure of carbon dioxide (1.5%), preoperative
   hospital stay days (1.5%), and postoperative total days of thoracic tube
   drainage (1.4%). Conclusions: Large language model-ADA effectively
   facilitates the development of machine learning models for lung cancer
   survival prediction, enabling non-technical health care professionals to
   harness the power of advanced analytics. The findings underscore the
   importance of preoperative factors in predicting outcomes, while also
   highlighting the need for external validation across diverse settings.
ZS 0
ZR 0
ZA 0
Z8 0
ZB 0
TC 0
Z9 0
DA 2025-03-31
UT WOS:001451782200001
PM 40201531
ER

PT C
AU Marques, Adriell Gomes
   Candido de Figueiredo, Marcus Vinicius
   Da Costa Nascimento, Jose Jerovane
   de Souza, Cidcley Teixeira
   Jaborandy de Mattos Dourado, Carlos Mauricio, Jr.
   de Albuquerque, Victor Hugo C.
   de Freitas Souza, Luis Fabricio
BE Emmendorfer, LR
   Kieling, A
TI New approach Generative AI Melanoma Data Fusion for classification in
   dermoscopic images with Large Language Model
SO 2024 37TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES, SIBGRAPI
   2024
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
BP 157
EP 162
DI 10.1109/SIBGRAPI62404.2024.10716298
DT Proceedings Paper
PD 2024
PY 2024
AB Skin cancer is a disease that causes thousands of deaths each year.
   Early diagnosis and monitoring the progression of the disease are
   crucial factors for the treatment and health indicators of a society.
   This study presents an innovative approach for the detection,
   segmentation, and classification of melanomas in dermoscopic images
   using advanced Computer Vision and Artificial Intelligence (AI) methods.
   Specifically, it applies Large Language Model (LLM) solutions for
   pre-diagnosis results through generative AI. This work explores
   combinations of methods for melanoma detection and segmentation based on
   the YOLO and SAM architectures, achieving 99% accuracy, surpassing
   various studies in the literature. The classification phase is based on
   a pipeline integrating feature extraction and selecting the best
   combination for melanoma region classification, achieving an accuracy of
   86.0%, also outperforming different studies in the literature.
CT 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)
CY SEP 30-OCT 03, 2024
CL Manaus, BRAZIL
SP SIBGRAPI; Univ Estado Amazonas, Escola Super Tecnologia; Coordinat
   Improvement Higher Educ Personnel; SiDi Tecnologia lnformacao Servicos
   lnterdependencia; Inst Pesquisas Eldorado; Sidia; Tutiplast Ind &
   Comercio Ltda; LUDUS Lab Tecnologia, lnovavao & Economia Criativa;
   ThinkTED Lab Pesquisa, Desenvolvimento & lnovacao Tecnologias
   Emergentes; Brazilian Comp Soc; IEEE Brazil NE; Governo Estado Amazonas
ZB 0
ZR 0
ZS 0
Z8 0
ZA 0
TC 0
Z9 0
DA 2025-01-25
UT WOS:001345126400027
ER

PT J
AU Liu, Jilei
   Shen, Hongru
   Chen, Kexin
   Li, Xiangchun
TI Large language model produces high accurate diagnosis of cancer from
   end-motif profiles of cell-free DNA
SO BRIEFINGS IN BIOINFORMATICS
VL 25
IS 5
AR bbae430
DI 10.1093/bib/bbae430
DT Article
PD SEP 2 2024
PY 2024
AB Instruction-tuned large language models (LLMs) demonstrate exceptional
   ability to align with human intentions. We present an LLM-based
   model-instruction-tuned LLM for assessment of cancer (iLLMAC)-that can
   detect cancer using cell-free deoxyribonucleic acid (cfDNA) end-motif
   profiles. Developed on plasma cfDNA sequencing data from 1135 cancer
   patients and 1106 controls across three datasets, iLLMAC achieved area
   under the receiver operating curve (AUROC) of 0.866 [95% confidence
   interval (CI), 0.773-0.959] for cancer diagnosis and 0.924 (95% CI,
   0.841-1.0) for hepatocellular carcinoma (HCC) detection using 16
   end-motifs. Performance increased with more motifs, reaching 0.886 (95%
   CI, 0.794-0.977) and 0.956 (95% CI, 0.89-1.0) for cancer diagnosis and
   HCC detection, respectively, with 64 end-motifs. On an external-testing
   set, iLLMAC achieved AUROC of 0.912 (95% CI, 0.849-0.976) for cancer
   diagnosis and 0.938 (95% CI, 0.885-0.992) for HCC detection with 64
   end-motifs, significantly outperforming benchmarked methods.
   Furthermore, iLLMAC achieved high classification performance on datasets
   with bisulfite and 5-hydroxymethylcytosine sequencing. Our study
   highlights the effectiveness of LLM-based instruction-tuning for
   cfDNA-based cancer detection.
ZA 0
ZR 0
Z8 0
ZB 1
TC 4
ZS 0
Z9 4
DA 2024-09-08
UT WOS:001304494500001
PM 39222060
ER

PT J
AU Orlhac, Fanny
   Bradshaw, Tyler
   Buvat, Irene
TI Can a large language model be an effective assistant for literature
   reviews? An example in Radiomics
SO JOURNAL OF NUCLEAR MEDICINE
VL 65
MA 241031
SU 2
DT Meeting Abstract
PD JUN 1 2024
PY 2024
CT Annual Meeting of the Society-of-Nuclear-Medicine-and-Molecular-Imaging
   (SNMMI)
CY JUN 08-11, 2024
CL Toronto, CANADA
SP Soc Nuclear Med & Mol Imaging
Z8 0
ZB 0
ZA 0
TC 0
ZS 0
ZR 0
Z9 0
DA 2024-12-16
UT WOS:001289165600066
ER

PT J
AU Yang, Yichen
   Shen, Hongru
   Chen, Kexin
   Li, Xiangchun
TI From pixels to patients: the evolution and future of deep learning in
   cancer diagnostics.
SO Trends in molecular medicine
VL 31
IS 6
BP 548
EP 558
DI 10.1016/j.molmed.2024.11.009
DT Journal Article; Review
PD 2025-Jun
PY 2025
AB Deep learning has revolutionized cancer diagnostics, shifting from
   pixel-based image analysis to more comprehensive, patient-centric care.
   This opinion article explores recent advancements in neural network
   architectures, highlighting their evolution in biomedical research and
   their impact on medical imaging interpretation and multimodal data
   integration. We emphasize the need for domain-specific artificial
   intelligence (AI) systems capable of handling complex clinical tasks,
   advocating for the development of multimodal large language models that
   can integrate diverse data sources. These models have the potential to
   significantly enhance the precision and efficiency of cancer
   diagnostics, transforming AI from a supplementary tool into a core
   component of clinical decision-making, ultimately improving patient
   outcomes and advancing cancer care.
Z8 0
ZR 0
ZS 0
TC 0
ZA 0
ZB 0
Z9 0
DA 2024-12-18
UT MEDLINE:39665958
PM 39665958
ER

PT J
AU Gerstung, Moritz
   Liu, David
   Ghassemi, Marzyeh
   Zou, James
   Chowell, Diego
   Teuwen, Jonas
   Mahmood, Faisal
   Kather, Jakob Nikolas
TI Artificial intelligence
SO CANCER CELL
VL 42
IS 6
BP 915
EP 918
DT Editorial Material
PD JUN 10 2024
PY 2024
ZS 0
ZR 0
ZA 0
Z8 0
TC 2
ZB 1
Z9 2
DA 2025-02-12
UT WOS:001412853800001
PM 38861926
ER

PT J
AU Liu, Wei
   Liu, Jun
   Tang, Yitao
   Liu, Chaozhong
   Song, Meiyi
   Ju, Zhenlin
   Kumar, Shweth V.
   Lu, Yiling
   Akbani, Rehan
   Mills, Gordon
   Liang, Han
TI TCPAplus: An LLM-empowered chatbot for analyzing a large protein
   expression atlas of human cancers
SO CANCER RESEARCH
VL 84
IS 7
MA LB247
DI 10.1158/1538-7445.AM2024-LB247
SU S
DT Meeting Abstract
PD APR 1 2024
PY 2024
CT Annual Meeting of the American-Association-for-Cancer-Research (AACR)
CY APR 05-10, 2024
CL San Diego, CA
SP Amer Assoc Cancer Res
ZA 0
ZB 0
Z8 0
ZR 0
ZS 0
TC 0
Z9 0
DA 2024-05-19
UT WOS:001203184200459
ER

PT J
AU Lefkes, Judith
   D'Amato, Marina
   Sun, Susu
   Litjens, Geert
   Ciompi, Francesco
TI Large Language Models Automate Diagnostic Conclusions Generation from
   Microscopic Descriptions in Multiple Cancer Types
SO LABORATORY INVESTIGATION
VL 105
IS 3
MA 1370
AR 103608
DI 10.1016/j.labinv.2024.103608
EA MAR 2025
SU S
DT Meeting Abstract
PD MAR 2025
PY 2025
CT Annual Meeting of the United-States-and-Canadian-Academy-of-Pathology
   (USCAP)
CY MAR 22-27, 2025
CL Boston, MA
SP United States & Canadian Acad Pathol
ZA 0
ZR 0
Z8 0
ZB 0
ZS 0
TC 0
Z9 0
DA 2025-04-19
UT WOS:001464120600063
ER

PT J
AU Kaiser, Kristen N.
   Hughes, Alexa J.
   Yang, Anthony D.
   Mohanty, Sanjay
   Maatman, Thomas K.
   Gonzalez, Andrew A.
   Patzer, Rachel E.
   Bilimoria, Karl Y.
   Ellis, Ryan J.
TI Use of large language models as clinical decision support tools for
   management pancreatic adenocarcinoma using National Comprehensive Cancer
   Network guidelines
SO SURGERY
VL 182
AR 109267
DI 10.1016/j.surg.2025.109267
DT Article
PD JUN 2025
PY 2025
AB Background: Large language models may form the basis of clinical
   decision support tools to improve rates of guideline concordant care for
   pancreatic ductal adenocarcinoma. The objectives of this study were to
   1) define the first-pass accuracy of 2 publicly available large language
   models in responding to prompts on the basis of National Comprehensive
   Cancer Network guidelines for pancreatic ductal adenocarcinoma, 2)
   describe consistency of responses within each large language models, and
   3) explore differences between the 2 large language models in their
   accuracy and verbosity. Methods: Clinical scenarios were developed on
   the basis of current National Comprehensive Cancer Network guidelines.
   Scenario prompts were entered independently by 2 investigators into
   OpenAI ChatGPT and Microsoft Copilot, yielding 4 responses per scenario.
   Responses were manually graded on accuracy and verbosity and compared to
   clinician-derived responses. Results: From the 104 responses, large
   language model responses were graded as completely correct in 42% of
   responses (n = 44). ChatGPT responses were more accurate than Copilot
   across all prompts (3.33 +/- 0.86 vs 3.02 +/- 0.87, P = .04). Among 54
   generated responses from ChatGPT sessions, 52% (n = 27) were completely
   correct, 35% (n = 18) contained missing information, and 14% (n = 7)
   were inaccurate/misleading. Copilot responses were completely correct in
   33% (n = 17) of responses, whereas 42% (n = 22) were missing information
   and 25% (n = 13) contained inaccurate/misleading information. Clinician
   responses were more concise than all large language model-generated
   responses (32 +/- 13 vs 270 +/- 70 words, P < .001). Conclusion: Large
   language model-powered responses to clinical questions regarding
   pancreatic ductal adenocarcinoma are often inaccurate and verbose. These
   publicly available large language models require significant
   optimization before implementation within health care as clinical
   decision support tools. (c) 2025 Elsevier Inc. All rights are reserved,
   including those for text and data mining, AI training, and similar
   technologies.
ZA 0
Z8 0
ZS 0
TC 1
ZB 0
ZR 0
Z9 1
DA 2025-06-08
UT WOS:001498925700002
PM 40055080
ER

PT J
AU Yang, Xiaoyu
   Xu, Jinjian
   Ji, Hong
   Li, Jun
   Yang, Bingqing
   Wang, Liye
TI Early prediction of colorectal adenoma risk: leveraging large-language
   model for clinical electronic medical record data
SO FRONTIERS IN ONCOLOGY
VL 15
AR 1508455
DI 10.3389/fonc.2025.1508455
DT Article
PD MAY 15 2025
PY 2025
AB Objective To develop a non-invasive, radiation-free model for early
   colorectal adenoma prediction using clinical electronic medical record
   (EMR) data, addressing limitations in current diagnostic approaches for
   large-scale screening.Design Retrospective analysis utilized 92,681
   cases with EMR, spanning from 2012 to 2022, as the training cohort.
   Testing was performed on an independent test cohort of 19,265 cases from
   2023. Several classical machine learning algorithms were applied in
   combination with the BGE-M3 large-language model (LLM) for enhanced
   semantic feature extraction. Area under the receiver operating
   characteristic curve (AUC) is the major metric for evaluating model
   performance. The Shapley additive explanations (SHAP) method was
   employed to identify the most influential risk factors.Results XGBoost
   algorithm, integrated with BGE-M3, demonstrated superior performance
   (AUC = 0.9847) in the validation cohort. Notably, when applied to the
   independent test cohort, XGBoost maintained its strong predictive
   ability with an AUC of 0.9839 and an average advance prediction time of
   6.88 hours, underscoring the effectiveness of the BGE-M3 model. The SHAP
   analysis further identified 16 high-impact risk factors, highlighting
   the interplay of genetic, lifestyle, and environmental influences on
   colorectal adenoma risk.Conclusion This study developed a robust machine
   learning-based model for colorectal adenoma risk prediction, leveraging
   clinical EMR and LLM. The proposed model demonstrates high predictive
   accuracy and has the potential to enhance early detection, making it
   well-suited for large-scale screening programs. By facilitating early
   identification of individuals at risk, this approach may contribute to
   reducing the incidence and mortality associated with colorectal cancer.
ZS 0
ZB 0
ZA 0
TC 0
ZR 0
Z8 0
Z9 0
DA 2025-06-01
UT WOS:001497635100001
PM 40444092
ER

PT J
AU Zhang, Min
   Cheng, Qi
   Wei, Zhenyu
   Xu, Jiayu
   Wu, Shiwei
   Xu, Nan
   Zhao, Chengkui
   Yu, Lei
   Feng, Weixing
TI BertTCR: a Bert-based deep learning framework for predicting
   cancer-related immune status based on T cell receptor repertoire
SO BRIEFINGS IN BIOINFORMATICS
VL 25
IS 5
AR bbae420
DI 10.1093/bib/bbae420
DT Article
PD AUG 23 2024
PY 2024
AB The T cell receptor (TCR) repertoire is pivotal to the human immune
   system, and understanding its nuances can significantly enhance our
   ability to forecast cancer-related immune responses. However, existing
   methods often overlook the intra- and inter-sequence interactions of T
   cell receptors (TCRs), limiting the development of sequence-based
   cancer-related immune status predictions. To address this challenge, we
   propose BertTCR, an innovative deep learning framework designed to
   predict cancer-related immune status using TCRs. BertTCR combines a
   pre-trained protein large language model with deep learning
   architectures, enabling it to extract deeper contextual information from
   TCRs. Compared to three state-of-the-art sequence-based methods, BertTCR
   improves the AUC on an external validation set for thyroid cancer
   detection by 21 percentage points. Additionally, this model was trained
   on over 2000 publicly available TCR libraries covering 17 types of
   cancer and healthy samples, and it has been validated on multiple public
   external datasets for its ability to distinguish cancer patients from
   healthy individuals. Furthermore, BertTCR can accurately classify
   various cancer types and healthy individuals. Overall, BertTCR is the
   advancing method for cancer-related immune status forecasting based on
   TCRs, offering promising potential for a wide range of immune status
   prediction tasks.
ZB 1
TC 5
Z8 0
ZA 0
ZR 0
ZS 0
Z9 5
DA 2024-09-10
UT WOS:001296768600001
PM 39177262
ER

PT J
AU Tozuka, Ryota
   Johno, Hisashi
   Amakawa, Akitomo
   Sato, Junichi
   Muto, Mizuki
   Seki, Shoichiro
   Komaba, Atsushi
   Onishi, Hiroshi
TI Application of NotebookLM, a large language model with
   retrieval-augmented generation, for lung cancer staging
SO JAPANESE JOURNAL OF RADIOLOGY
VL 43
IS 4
BP 706
EP 712
DI 10.1007/s11604-024-01705-1
EA NOV 2024
DT Article
PD APR 2025
PY 2025
AB PurposeIn radiology, large language models (LLMs), including ChatGPT,
   have recently gained attention, and their utility is being rapidly
   evaluated. However, concerns have emerged regarding their reliability in
   clinical applications due to limitations such as hallucinations and
   insufficient referencing. To address these issues, we focus on the
   latest technology, retrieval-augmented generation (RAG), which enables
   LLMs to reference reliable external knowledge (REK). Specifically, this
   study examines the utility and reliability of a recently released
   RAG-equipped LLM (RAG-LLM), NotebookLM, for staging lung
   cancer.Materials and methodsWe summarized the current lung cancer
   staging guideline in Japan and provided this as REK to NotebookLM. We
   then tasked NotebookLM with staging 100 fictional lung cancer cases
   based on CT findings and evaluated its accuracy. For comparison, we
   performed the same task using a gold-standard LLM, GPT-4 Omni (GPT-4o),
   both with and without the REK. For GPT-4o, the REK was provided directly
   within the prompt rather than through RAG.ResultsNotebookLM achieved 86%
   diagnostic accuracy in the lung cancer staging experiment, outperforming
   GPT-4o, which recorded 39% accuracy with the REK and 25% without it.
   Moreover, NotebookLM demonstrated 95% accuracy in searching reference
   locations within the REK.ConclusionNotebookLM, a RAG-LLM, successfully
   performed lung cancer staging by utilizing the REK, demonstrating
   superior performance compared to GPT-4o (without RAG). Additionally, it
   provided highly accurate reference locations within the REK, allowing
   radiologists to efficiently evaluate the reliability of NotebookLM's
   responses and detect possible hallucinations. Overall, this study
   highlights the potential of NotebookLM, a RAG-LLM, in image diagnosis.
ZB 0
Z8 0
TC 5
ZS 0
ZA 0
ZR 0
Z9 5
DA 2024-11-30
UT WOS:001362633800001
PM 39585559
ER

PT J
AU Kim, Sanghwan
   Jang, Sowon
   Kim, Borham
   Sunwoo, Leonard
   Kim, Seok
   Chung, Jin-Haeng
   Nam, Sejin
   Cho, Hyeongmin
   Lee, Donghyoung
   Lee, Keehyuck
   Yoo, Sooyoung
TI Automated Pathologic TN Classification Prediction and Rationale
   Generation From Lung CancerSurgical Pathology Reports Using a Large
   Language Model Fine-Tuned With Chain-of-Thought: Algorithm Development
   and Validation Study
SO JMIR MEDICAL INFORMATICS
VL 12
AR e67056
DI 10.2196/67056
DT Article
PD 2024
PY 2024
AB Background: Traditional rule-based natural language processing
   approaches in electronic health record systems are effective but are
   often time-consuming and proneto errors when handling unstructured data.
   This is primarily dueto the substantial manual effort required to parse
   and extract information from diverse types of documentation. Recent
   advancements in large language model (LLM) technology have made it
   possible to automatically interpret medical context and support
   pathologic staging. However, existing LLMs encounter challenges in
   rapidly adapting to specialized guideline updates. In this study, we
   fine-tuned an LLM specifically for lung cancer pathologic staging,
   enabling it to incorporate the latest guidelines for pathologic TN
   classification. Objective: This study aims to evaluate the performance
   of fine-tuned generative language models in automatically inferring
   pathologic TN classifications and extracting their rationale from lung
   cancer surgical pathology reports. By addressing the inefficiencies and
   extensive parsing efforts associated with rule-based methods, this
   approach seeks to enable rapid and accurate reclassification aligned
   with the latest cancer staging guidelines. Methods: We conducted a
   comparative performance evaluation of 6 open-source LLMs for automated
   TN classification and rationale generation, using 3216 deidentified lung
   cancer surgical pathology reports based on the AmericanJointCommitteeon
   Cancer (AJCC) Cancer Staging Manual8th edition, collected from a
   tertiary hospital. The dataset was preprocessed by segmenting each
   report according to lesion location and morphological diagnosis.
   Performance was assessed using exact match ratio (EMR) and semantic
   match ratio (SMR) as evaluation metrics, which measureclassification
   accuracy and the contextual alignment of the generated rationales,
   respectively. Results: Among the 6 models, the Orca2_13b model achieved
   the highest performance with an EMR of 0.934 and an SMR of 0.864.
   TheOrca2_7b model also demonstrated strong performance, recording an EMR
   of 0.914 and an SMR of 0.854. In contrast, the Llama2_7b model achieved
   an EMR of 0.864 and an SMR of 0.771, while the Llama2_13b model showed
   an EMR of 0.762 and an SMR of 0.690. The Mistral_7b and Llama3_8b
   models, on the other hand, showed lower performance, with EMRs of 0.572
   and 0.489, and SMRs of 0.377 and 0.456, respectively. Overall, the Orca2
   models consistently outperformed the others in both TN stage
   classification and rationale generation. Conclusions: The generative
   language model approach presented in this study has the potential to
   enhance and automate TN classification in complex cancer staging,
   supporting both clinical practice and oncology data curation. With
   additional fine-tuning based on cancer-specific guidelines, this
   approach can be effectively adapted to other cancer types.
Z8 0
ZB 0
ZA 0
TC 0
ZR 0
ZS 0
Z9 0
DA 2025-01-11
UT WOS:001390042000002
PM 39705675
ER

PT J
AU Zeinali, Nahid
   Albashayreh, Alaa
   Fan, Weiguo
   White, Stephanie Gilbertson
TI Symptom-BERT: Enhancing Cancer Symptom Detection in EHR Clinical Notes
SO JOURNAL OF PAIN AND SYMPTOM MANAGEMENT
VL 68
IS 2
DI 10.1016/j.jpainsymman.2024.05.015
EA JUL 2024
DT Article
PD AUG 2024
PY 2024
AB Context. Extracting cancer symptom documentation allows clinicians to
   develop highly individualized symptom prediction algorithms to deliver
   symptom management care. Leveraging advanced language models to detect
   symptom data in clinical narratives can significantly fi cantly enhance
   this process. Objective. This study uses a pretrained large
   language model to detect and extract cancer symptoms in clinical
   notes. Methods. We developed a pretrained language model to
   identify cancer symptoms in clinical notes based on a clinical corpus
   from the Enterprise Data Warehouse for Research at a healthcare system
   in the Midwestern United States. This study was conducted in 4 phases:1
   1 pretraining a Bio-Clinical BERT model on one million unlabeled
   clinical documents,2 2 fi ne-tuning Symptom-BERT for detecting 13 cancer
   symptom groups within 1112 annotated clinical notes,3 3 generating 180
   synthetic clinical notes using ChatGPT-4 for external validation, and4 4
   comparing the internal and external performance of Symptom-BERT against
   a non-pretrained version and six other BERT implementations.
   Results. The Symptom-BERT model effectively detected cancer symptoms in
   clinical notes. It achieved results with a micro- averaged F1-score of
   0.933, an AUC of 0.929 internally, and 0.831 and 0.834 externally. Our
   analysis shows that physical symptoms, like Pruritus, are typically
   identified fi ed with higher performance than psychological symptoms,
   such as anxiety. Conclusion. This study underscores the
   transformative potential of specialized pretraining on domain-specific
   fi c data in boosting the performance of language models for medical
   applications. The Symptom-BERT model's ' s exceptional efficacy fi cacy
   in detecting cancer symptoms heralds a groundbreaking stride in
   patient-centered AI technologies, offering a promising path to elevate
   symptom management and cultivate superior patient self-care outcomes. J
   Pain Symptom Manage 2024;68:190-198. - 198. (c) 2024 American Academy of
   Hospice and Palliative Medicine. Published by Elsevier Inc. All rights
   are reserved, including those for text and data mining, AI training, and
   similar technologies.
ZR 0
ZB 0
Z8 0
ZA 0
TC 6
ZS 0
Z9 6
DA 2024-10-11
UT WOS:001326766200001
PM 38789092
ER

PT J
AU Yasaka, Koichiro
   Kanzawa, Jun
   Kanemaru, Noriko
   Koshino, Saori
   Abe, Osamu
TI Fine-Tuned Large Language Model for Extracting Patients on Pretreatment
   for Lung Cancer from a Picture Archiving and Communication System Based
   on Radiological Reports
SO JOURNAL OF IMAGING INFORMATICS IN MEDICINE
VL 38
IS 1
BP 327
EP 334
DI 10.1007/s10278-024-01186-8
EA JUL 2024
DT Article
PD FEB 2025
PY 2025
AB This study aimed to investigate the performance of a fine-tuned large
   language model (LLM) in extracting patients on pretreatment for lung
   cancer from picture archiving and communication systems (PACS) and
   comparing it with that of radiologists. Patients whose radiological
   reports contained the term lung cancer (3111 for training, 124 for
   validation, and 288 for test) were included in this retrospective study.
   Based on clinical indication and diagnosis sections of the radiological
   report (used as input data), they were classified into four groups (used
   as reference data): group 0 (no lung cancer), group 1 (pretreatment lung
   cancer present), group 2 (after treatment for lung cancer), and group 3
   (planning radiation therapy). Using the training and validation
   datasets, fine-tuning of the pretrained LLM was conducted ten times. Due
   to group imbalance, group 2 data were undersampled in the training. The
   performance of the best-performing model in the validation dataset was
   assessed in the independent test dataset. For testing purposes, two
   other radiologists (readers 1 and 2) were also involved in classifying
   radiological reports. The overall accuracy of the fine-tuned LLM, reader
   1, and reader 2 was 0.983, 0.969, and 0.969, respectively. The
   sensitivity for differentiating group 0/1/2/3 by LLM, reader 1, and
   reader 2 was 1.000/0.948/0.991/1.000, 0.750/0.879/0.996/1.000, and
   1.000/0.931/0.978/1.000, respectively. The time required for
   classification by LLM, reader 1, and reader 2 was 46s/2539s/1538s,
   respectively. Fine-tuned LLM effectively extracted patients on
   pretreatment for lung cancer from PACS with comparable performance to
   radiologists in a shorter time.
ZA 0
ZB 0
TC 6
ZS 0
ZR 0
Z8 0
Z9 6
DA 2024-07-10
UT WOS:001261213000001
PM 38955964
ER

PT J
AU Liang, Shufan
   Zhang, Jiangjiang
   Liu, Xingting
   Huang, Yinkui
   Shao, Jun
   Liu, Xiaohong
   Li, Weimin
   Wang, Guangyu
   Wang, Chengdi
TI The potential of large language models to advance precision oncology
SO EBIOMEDICINE
VL 115
AR 105695
DI 10.1016/j.ebiom.2025.105695
EA APR 2025
DT Review
PD MAY 2025
PY 2025
AB With the rapid development of artificial intelligence (AI) within
   medicine, the emergence of large language models (LLMs) has gradually
   reached the forefront of clinical research. In oncology, by mining the
   underlying connection between a text or image input and the desired
   output, LLMs demonstrate great potential for managing tumours. In this
   review, we provide a brief description of the development of LLMs,
   followed by model construction strategies and general medical functions.
   We then elaborate on the role of LLMs in cancer screening and diagnosis,
   metastasis identification, tumour staging, treatment recommendation, and
   documentation processing tasks by decoding various types of clinical
   data. Moreover, the current barriers faced by LLMs, such as
   hallucinations, ethical problems, limited application, and so on, are
   outlined along with corresponding solutions, where the further purpose
   is to inspire improvement and innovation in this field with respect to
   harnessing LLMs for advancing precision oncology. Copyright (c) 2025 The
   Author(s). Published by Elsevier B.V. This is an open access article
   under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).
TC 0
Z8 0
ZR 0
ZB 0
ZS 0
ZA 0
Z9 0
DA 2025-05-15
UT WOS:001485098500001
PM 40305985
ER

PT J
AU Reicher, Lee
   Lutsker, Guy
   Michaan, Nadav
   Grisaru, Dan
   Laskov, Ido
TI Exploring the role of artificial intelligence, large language models:
   Comparing patient-focused information and clinical decision support
   capabilities to the gynecologic oncology guidelines
SO INTERNATIONAL JOURNAL OF GYNECOLOGY & OBSTETRICS
VL 168
IS 2
BP 419
EP 427
DI 10.1002/ijgo.15869
EA AUG 2024
DT Review
PD FEB 2025
PY 2025
AB Gynecologic cancer requires personalized care to improve outcomes. Large
   language models (LLMs) hold the potential to provide intelligent
   question-answering with reliable information about medical queries in
   clear and plain English, which can be understood by both healthcare
   providers and patients. We aimed to evaluate two freely available LLMs
   (ChatGPT and Google's Bard) in answering questions regarding the
   management of gynecologic cancer. The LLMs' performances were evaluated
   by developing a set questions that addressed common gynecologic
   oncologic findings from a patient's perspective and more complex
   questions to elicit recommendations from a clinician's perspective. Each
   question was presented to the LLM interface, and the responses generated
   by the artificial intelligence (AI) model were recorded. The responses
   were assessed based on the adherence to the National Comprehensive
   Cancer Network and European Society of Gynecological Oncology
   guidelines. This evaluation aimed to determine the accuracy and
   appropriateness of the information provided by LLMs. We showed that the
   models provided largely appropriate responses to questions regarding
   common cervical cancer screening tests and BRCA-related questions. Less
   useful answers were received to complex and controversial gynecologic
   oncology cases, as assessed by reviewing the common guidelines. ChatGPT
   and Bard lacked knowledge of regional guideline variations, However, it
   provided practical and multifaceted advice to patients and caregivers
   regarding the next steps of management and follow up. We conclude that
   LLMs may have a role as an adjunct informational tool to improve
   outcomes.
   ChatGPT and Bard provide appropriate responses to patient's perspective
   gynecologic oncologic questions, but is less useful for complex
   questions compared with the National Comprehensive Cancer
   Network/European Society of Gynecological Oncology guidelines.
TC 5
ZR 0
Z8 0
ZA 0
ZB 0
ZS 0
Z9 5
DA 2024-08-23
UT WOS:001293448800001
PM 39161265
ER

PT C
AU Xu, Hongyan
   Su, Xiu
   Sowmya, Arcot
   Katz, Ian
   Wang, Dadong
GP IEEE
TI SCD-NAS: Towards Zero-Cost Training in Melanoma Diagnosis
SO 2024 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME 2024
SE IEEE International Conference on Multimedia and Expo
DI 10.1109/ICME57554.2024.10687537
DT Proceedings Paper
PD 2024
PY 2024
AB Diagnosing melanoma remains challenging despite advances in
   Convolutional Neural Networks (CNNs) for skin cancer detection. Their
   application in clinical settings is often limited by differences between
   natural and clinical images. To address this, we introduce the Skin
   Cancer Detection Neural Architecture Search (SCD-NAS) framework. In our
   method, Large Language Model (LLM) is leveraged as a proxy, which helps
   SCD-NAS achieve cost-free training. Additionally, to maximize the
   benefits of various architectural design spaces, we introduce a Search
   Space Expansion (SSE) methodology. This effectively combines the merits
   of diverse architectural configurations, thereby enhancing model
   performance. We conducted experiments on the ISIC 2020, MedMNISTv2,
   CIFAR-10 and CIFAR-100 datasets. Our SCD-NAS-derived ResNet50 model
   achieved an Area Under the Curve (AUC) of 91.23% on the ISIC 2020
   dataset, improving the baseline by 5.93%. It also exceeded the CIFAR-10
   benchmark by 2.45% in accuracy.
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 15-19, 2024
CL Niagra Falls, CANADA
SP IEEE
ZR 0
ZS 0
Z8 0
TC 0
ZB 0
ZA 0
Z9 0
DA 2025-04-16
UT WOS:001364925200122
ER

PT J
AU Odisho, Anobel Y.
   Liu, Andrew W.
   Pace, William A.
   Krumm, Robert
   Cowan, Janet E.
   Carroll, Peter R.
   Cooperberg, Matthew R.
TI DEVELOPMENT OF A GENERATIVE ARTIFICIAL INTELLIGENCE DATA PIPELINE TO
   AUTOMATE THE CAPTURE OF UNSTRUCTURED MRI DATA FOR PROSTATE CANCER CARE
SO JOURNAL OF UROLOGY
VL 211
IS 5
MA MP07-14
BP E110
EP E110
DI 10.1097/01.JU.0001008728.41882.d7.14
SU S
DT Meeting Abstract
PD MAY 2024
PY 2024
CT Annual Meeting of the American-Urological-Association (AUA)
CY MAY 03-06, 2024
CL San Antonio, TX
SP Amer Urolog Assoc
TC 0
ZB 0
ZS 0
ZA 0
ZR 0
Z8 0
Z9 0
DA 2024-08-04
UT WOS:001263885300214
ER

PT J
AU Chen, Hui
   Xu, Zanmei
   Chen, Lijuan
   Wang, Mingmin
   Zhang, Peng
   Pang, Fei
   Wang, Kai
TI AI-enabled precision oncology era: Advanced and interactive
   interpretation of next-gneneration sequencing (NGS) reports
SO CANCER RESEARCH
VL 84
IS 6
MA 2315
DI 10.1158/1538-7445.AM2024-2315
SU S
DT Meeting Abstract
PD MAR 15 2024
PY 2024
CT Annual Meeting of the American-Association-for-Cancer-Research (AACR)
CY APR 05-10, 2024
CL San Diego, CA
SP Amer Assoc Cancer Res
Z8 0
ZB 0
ZR 0
ZA 0
TC 0
ZS 0
Z9 0
DA 2024-07-31
UT WOS:001252656304116
ER

PT J
AU Hermann, Catherine E.
   Patel, Jharna M.
   Boyd, Leslie
   Aviki, Emeline
   Stasenko, Marina
TI Let's chat about cervical cancer: Assessing the accuracy of ChatGPT
   responses to cervical cancer questions
SO GYNECOLOGIC ONCOLOGY
VL 179
BP 164
EP 168
DI 10.1016/j.ygyno.2023.11.008
EA NOV 2023
DT Article
PD DEC 2023
PY 2023
AB Objective. To quantify the accuracy of ChatGPT in answering commonly
   asked questions pertaining to cervical cancer prevention, diagnosis,
   treatment, and survivorship/quality-of-life (QOL). Methods. ChatGPT was
   queried with 64 questions adapted from professional society websites and
   the au-thors' clinical experiences. The answers were scored by two
   attending Gynecologic Oncologists according to the following scale: 1)
   correct and comprehensive, 2) correct but not comprehensive, 3) some
   correct, some in-correct, and 4) completely incorrect. Scoring
   discrepancies were resolved by additional reviewers as needed. The
   proportion of responses earning each score were calculated overall and
   within each question category.Results. ChatGPT provided correct and
   comprehensive answers to 34 (53.1%) questions, correct but not
   com-prehensive answers to 19 (29.7%) questions, partially incorrect
   answers to 10 (15.6%) questions, and completely incorrect answers to 1
   (1.6%) question. Prevention and survivorship/QOL had the highest
   proportion of "correct" scores (scores of 1 or 2) at 22/24 (91.7%) and
   15/16 (93.8%), respectively. ChatGPT performed less well in the
   treatment category, with 15/21 (71.4%) correct scores. It performed the
   worst in the diagnosis category with only 1/3 (33.3%) correct
   scores.Conclusion. ChatGPT accurately answers questions about cervical
   cancer prevention, survivorship, and QOL. It performs less accurately
   for cervical cancer diagnosis and treatment. Further development of this
   immensely popular large language model should include physician input
   before it can be utilized as a tool for Gynecologists or recommended as
   a patient resource for information on cervical cancer diagnosis and
   treatment.(c) 2023 Elsevier Inc. All rights reserved.
TC 21
ZR 0
ZA 0
Z8 0
ZB 6
ZS 0
Z9 21
DA 2023-12-23
UT WOS:001122497400001
PM 37988948
ER

PT J
AU Cui, Miao
   Sauter, Jennifer
   Chang, Jason
   Yang, Soo-Ryum
   Baine, Marina
   Rekhtman, Natasha
   Travis, William
TI Harnessing GPT-Driven AI for Enhanced Pathology: Development and
   Applications of the 'Pathology 2nd Brain'
SO LABORATORY INVESTIGATION
VL 105
IS 3
MA 1346
AR 103584
DI 10.1016/j.labinv.2024.103584
EA MAR 2025
SU S
DT Meeting Abstract
PD MAR 2025
PY 2025
CT Annual Meeting of the United-States-and-Canadian-Academy-of-Pathology
   (USCAP)
CY MAR 22-27, 2025
CL Boston, MA
SP United States & Canadian Acad Pathol
ZA 0
ZS 0
ZR 0
TC 0
ZB 0
Z8 0
Z9 0
DA 2025-04-19
UT WOS:001464120600020
ER

PT J
AU Liu, Jialin
   Wang, Changyu
   Liu, Siru
TI Utility of ChatGPT in Clinical Practice
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 25
AR e48568
DI 10.2196/48568
DT Article
PD JUN 28 2023
PY 2023
AB ChatGPT is receiving increasing attention and has a variety of
   application scenarios in clinical practice. In clinical decision
   support, ChatGPT has been used to generate accurate differential
   diagnosis lists, support clinical decision-making, optimize clinical
   decision support, and provide insights for cancer screening decisions.
   In addition, ChatGPT has been used for intelligent question-answering to
   provide reliable information about diseases and medical queries. In
   terms of medical documentation, ChatGPT has proven effective in
   generating patient clinical letters, radiology reports, medical notes,
   and discharge summaries, improving efficiency and accuracy for health
   care providers. Future research directions include real-time monitoring
   and predictive analytics, precision medicine and personalized treatment,
   the role of ChatGPT in telemedicine and remote health care, and
   integration with existing health care systems. Overall, ChatGPT is a
   valuable tool that complements the expertise of health care providers
   and improves clinical decision-making and patient care. However, ChatGPT
   is a double-edged sword. We need to carefully consider and study the
   benefits and potential dangers of ChatGPT. In this viewpoint, we discuss
   recent advances in ChatGPT research in clinical practice and suggest
   possible risks and challenges of using ChatGPT in clinical practice. It
   will help guide and support future artificial intelligence research
   similar to ChatGPT in health.
ZS 1
ZR 0
Z8 5
TC 230
ZB 25
ZA 0
Z9 234
DA 2023-08-24
UT WOS:001045687800005
PM 37379067
ER

PT J
AU Zhu, L.
   Anand, A.
   Gevorkyan, G.
   Mcgee, L. A.
   Rwigema, J. C.
   Rong, Y.
   Patel, S. H.
TI Testing and Validation of a Custom Trained Large Language Model for HN
   Patients with Guardrails
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 118
IS 5
MA 182
BP E52
EP E53
DT Meeting Abstract
PD APR 1 2024
PY 2024
CT Multidisciplinary Head and Neck Cancers Symposium
CY FEB 29-MAR 02, 2024
CL Phoenix, AZ
TC 1
ZS 0
Z8 0
ZA 0
ZR 0
ZB 0
Z9 1
DA 2024-10-18
UT WOS:001300212900102
ER

PT C
AU Gao, Welting
   Gao, Xiangyu
   Chen, Wenjin
   Foran, David J.
   Chen, Yi
BE Yang, DN
   Xie, X
   Tseng, VS
   Pei, J
   Huang, JW
   Lin, JCW
TI BioReX: Biomarker Information Extraction Inspired by Aspect-Based
   Sentiment Analysis
SO ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PT IV, PAKDD 2024
SE Lecture Notes in Artificial Intelligence
VL 14648
BP 129
EP 141
DI 10.1007/978-981-97-2238-9_10
DT Proceedings Paper
PD 2024
PY 2024
AB Biomarkers are critical in cancer diagnosis, prognosis, and treatment
   planning. However, this information is often buried in unstructured text
   form. In this paper, we make an analogy between Biomarker Information
   Extraction and Aspect-Based Sentiment Analysis. We propose a system,
   Biomarker and Result Extraction Model (BioReX). BioReX employs BERT
   post-training methods to augment the BioBERT model with domain-specific
   and task-specific knowledge for biomarker extraction. It uses
   syntactic-based and semantic-based attention to associate results to
   corresponding biomarkers. Evaluation demonstrates the effectiveness of
   the proposed approach.
CT 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining
   (PAKDD)
CY MAY 07-10, 2024
CL Taipei, TAIWAN
ZS 0
ZA 0
Z8 0
ZB 0
TC 0
ZR 0
Z9 0
DA 2024-09-04
UT WOS:001275770400010
ER

PT J
AU Wang, P.
   Liu, Z.
   Li, Y.
   Holmes, J.
   Shu, P.
   Zhang, L.
   Li, X.
   Li, Q.
   Vora, S. A.
   Patel, S. H.
   Sio, T. T. W.
   Liu, T.
   Liu, W.
TI Fine-Tuning Large Language Models for Radiation Oncology, A Specialized
   Health Care Domain
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3452
BP E664
EP E664
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
Z8 0
ZB 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892302131
ER

PT J
AU Wu, Xuzhou
   Li, Guangxin
   Wang, Xing
   Xu, Zeyu
   Wang, Yingni
   Lei, Shuge
   Xian, Jianming
   Wang, Xueyu
   Zhang, Yibao
   Li, Gong
   Yuan, Kehong
TI Diagnosis assistant for liver cancer utilizing a large language model
   with three types of knowledge
SO PHYSICS IN MEDICINE AND BIOLOGY
VL 70
IS 9
AR 095009
DI 10.1088/1361-6560/adcb17
DT Article
PD MAY 4 2025
PY 2025
AB Objective. Liver cancer has a high incidence rate, but experienced
   doctors are lacking in primary healthcare settings. The development of
   large models offers new possibilities for diagnosis. However, in liver
   cancer diagnosis, large models face certain limitations, such as
   insufficient understanding of specific medical images, inadequate
   consideration of liver vessel factors, and inaccuracies in reasoning
   logic. Therefore, this study proposes a diagnostic assistance tool
   specific to liver cancer to enhance the diagnostic capabilities of
   primary care doctors. Approach. A liver cancer diagnosis framework
   combining large and small models is proposed. A more accurate model for
   liver tumor segmentation and a more precise model for liver vessel
   segmentation are developed. The features extracted from the segmentation
   results of the small models are combined with the patient's medical
   records and then provided to the large model. The large model employs
   chain of thought prompts to simulate expert diagnostic reasoning and
   uses Retrieval-Augmented Generation to provide reliable answers based on
   trusted medical knowledge and cases. Main results. In the small model
   part, the proposed liver tumor and liver vessel segmentation methods
   achieve improved performance. In the large model part, this approach
   receives higher evaluation scores from doctors when analyzing patient
   imaging and medical records. Significance. First, a diagnostic framework
   combining small models and large models is proposed to optimize the
   liver cancer diagnosis process. Second, two segmentation models are
   introduced to compensate for the large model's shortcomings in
   extracting semantic information from images. Third, by simulating
   doctors' reasoning and integrating trusted knowledge, the framework
   enhances the reliability and interpretability of the large model's
   responses while reducing hallucination phenomena.
Z8 0
ZS 0
ZA 0
ZR 0
TC 0
ZB 0
Z9 0
DA 2025-05-08
UT WOS:001480266600001
PM 40203862
ER

PT J
AU Gilbert, M.
   Crutchfield, A.
   Luo, B.
   Thind, K.
   Ghanem, A. I.
   Siddiqui, F.
TI Using a Large Language Model (LLM) for Automated Extraction of Discrete
   Elements from Clinical Notes for Creation of Cancer Databases
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3371
BP E625
EP E625
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
Z8 0
ZA 0
ZS 0
ZB 0
ZR 0
TC 1
Z9 1
DA 2024-12-16
UT WOS:001325892302054
ER

PT J
AU Huang, Hanyao
   Zheng, Ou
   Wang, Dongdong
   Yin, Jiayi
   Wang, Zijin
   Ding, Shengxuan
   Yin, Heng
   Xu, Chuan
   Yang, Renjie
   Zheng, Qian
   Shi, Bing
TI ChatGPT for shaping the future of dentistry: the potential of
   multi-modal large language model
SO INTERNATIONAL JOURNAL OF ORAL SCIENCE
VL 15
IS 1
AR 29
DI 10.1038/s41368-023-00239-y
DT Review
PD JUL 28 2023
PY 2023
AB The ChatGPT, a lite and conversational variant of Generative Pretrained
   Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large
   Language Models (LLMs) with billions of parameters. LLMs have stirred up
   much interest among researchers and practitioners in their impressive
   skills in natural language processing tasks, which profoundly impact
   various fields. This paper mainly discusses the future applications of
   LLMs in dentistry. We introduce two primary LLM deployment methods in
   dentistry, including automated dental diagnosis and cross-modal dental
   diagnosis, and examine their potential applications. Especially,
   equipped with a cross-modal encoder, a single LLM can manage
   multi-source data and conduct advanced natural language reasoning to
   perform complex clinical operations. We also present cases to
   demonstrate the potential of a fully automatic Multi-Modal LLM AI system
   for dentistry clinical application. While LLMs offer significant
   potential benefits, the challenges, such as data privacy, data quality,
   and model bias, need further study. Overall, LLMs have the potential to
   revolutionize dental diagnosis and treatment, which indicates a
   promising avenue for clinical application and research in dentistry.
ZR 0
ZA 0
TC 112
ZS 0
ZB 10
Z8 4
Z9 115
DA 2023-08-15
UT WOS:001039606900001
PM 37507396
ER

PT J
AU Wang, Zhixiang
   Zhang, Zhen
   Traverso, Alberto
   Dekker, Andre
   Qian, Linxue
   Sun, Pengfei
TI Assessing the role of GPT-4 in thyroid ultrasound diagnosis and
   treatment recommendations: enhancing interpretability with a chain of
   thought approach
SO QUANTITATIVE IMAGING IN MEDICINE AND SURGERY
VL 14
IS 2
DI 10.21037/qims-23-1180
EA JAN 2024
DT Article
PD FEB 2024
PY 2024
AB Background: As artificial intelligence (AI) becomes increasingly
   prevalent in the medical field, the effectiveness of AI-generated
   medical reports in disease diagnosis remains to be evaluated. ChatGPT is
   a large language model developed by open AI with a notable capacity for
   text abstraction and comprehension. This study aimed to explore the
   capabilities, limitations, and potential of Generative Pre-trained
   Transformer (GPT)-4 in analyzing thyroid cancer ultrasound reports,
   providing diagnoses, and recommending treatment plans. Methods: Using
   109 diverse thyroid cancer cases, we evaluated GPT-4's performance by
   comparing its generated reports to those from doctors with various
   levels of experience. We also conducted a Turing Test and a consistency
   analysis. To enhance the interpretability of the model, we applied the
   Chain of Thought (CoT) method to deconstruct the decision-making chain
   of the GPT model. Results: GPT-4 demonstrated proficiency in report
   structuring, professional terminology, and clarity of expression, but
   showed limitations in diagnostic accuracy. In addition, our consistency
   analysis highlighted certain discrepancies in the AI's performance. The
   CoT method effectively enhanced the interpretability of the AI's
   decision-making process. Conclusions: GPT-4 exhibits potential as a
   supplementary tool in healthcare, especially for generating thyroid
   gland diagnostic reports. Our proposed online platform, "ThyroAIGuide",
   alongside the CoT method, underscores the potential of AI to augment
   diagnostic processes, elevate healthcare accessibility, and advance
   patient education. However, the journey towards fully integrating AI
   into healthcare is ongoing, requiring continuous research, development,
   and careful monitoring by medical professionals to ensure patient safety
   and quality of care.
Z8 2
TC 15
ZA 0
ZB 0
ZR 0
ZS 0
Z9 17
DA 2024-02-02
UT WOS:001146755700001
PM 38415150
ER

PT J
AU Ghorbian, Mohsen
   Ghobaei-Arani, Mostafa
   Ghorbian, Saied
TI Transforming breast cancer diagnosis and treatment with large language
   Models: A comprehensive survey
SO METHODS
VL 239
BP 85
EP 110
DI 10.1016/j.ymeth.2025.04.001
EA APR 2025
DT Article
PD JUL 2025
PY 2025
AB Breast cancer (BrCa), being one of the most prevalent forms of cancer in
   women, poses many challenges in the field of treatment and diagnosis due
   to its complex biological mechanisms. Early and accurate diagnosis plays
   a fundamental role in improving survival rates, but the limitations of
   existing imaging methods and clinical data interpretation often prevent
   optimal results. Large Language Models (LLMs), which are developed based
   on advanced architectures such as transformers, have brought about a
   significant revolution in data processing and medical decision-making.
   By analyzing a large volume of medical and clinical data, these models
   enable early diagnosis by identifying patterns in images and medical
   records and provide personalized treatment strategies by integrating
   genetic markers and clinical guidelines. Despite the transformative
   potential of these models, their use in BrCa management faces challenges
   such as data sensitivity, algorithm transparency, ethical
   considerations, and model compatibility with the details of medical
   applications that need to be addressed to achieve reliable results. This
   review systematically reviews the impact of LLMs on BrCa treatment and
   diagnosis. This study's objectives include analyzing the role of LLM
   technology in diagnosing and treating this disease. The findings
   indicate that the application of LLMs has resulted in significant
   improvements in various aspects of BrCa management, such as a 35%
   increase in the Efficiency of Diagnosis and BrCa Treatment (EDBC), a 30%
   enhancement in the System's Clinical Trust and Reliability (SCTR), and a
   20% improvement in the quality of patient education and information
   (IPEI). Ultimately, this study demonstrates the importance of LLMs in
   advancing precision medicine for BrCa and paves the way for effective
   patient-centered care solutions.
ZS 0
ZR 0
TC 0
ZB 0
ZA 0
Z8 0
Z9 0
DA 2025-04-20
UT WOS:001466448900001
PM 40199412
ER

PT J
AU Laios, Alexandros
   Theophilou, Georgios
   De Jong, Diederick
   Kalampokis, Evangelos
TI The Future of AI in Ovarian Cancer Research: The Large Language Models
   Perspective
SO CANCER CONTROL
VL 30
AR 10732748231197915
DI 10.1177/10732748231197915
DT Editorial Material
PD AUG 2023
PY 2023
AB Conversational large language model (LLM)-based chatbots utilize neural
   networks to process natural language. By generating highly sophisticated
   outputs from contextual input text, they revolutionize the access to
   further learning, leading to the development of new skills and
   personalized interactions. Although they are not developed to provide
   healthcare, their potential to address biomedical issues is rather
   unexplored. Healthcare digitalization and documentation of electronic
   health records is now developing into a standard practice. Developing
   tools to facilitate clinical review of unstructured data such as LLMs
   can derive clinical meaningful insights for ovarian cancer, a
   heterogeneous but devastating disease. Compared to standard approaches,
   they can host capacity to condense results and optimize analysis time.
   To help accelerate research in biomedical language processing and
   improve the validity of scientific writing, task-specific and
   domain-specific language models may be required. In turn, we propose a
   bespoke, proprietary ovarian cancer-specific natural language using
   solely in-domain text, whereas transfer learning drifts away from the
   pretrained language models to fine-tune task-specific models for all
   possible downstream applications. This venture will be fueled by the
   abundance of unstructured text information in the electronic health
   records resulting in ovarian cancer research ultimately reaching its
   linguistic home.
Z8 0
ZB 0
ZR 0
TC 5
ZS 0
ZA 0
Z9 5
DA 2023-09-25
UT WOS:001064803500001
PM 37624621
ER

PT C
AU Zhang, Tiantian
   Lin, Manxi
   Guo, Hongda
   Zhang, Xiaofan
   Chiu, Ka Fung Peter
   Feragen, Aasa
   Dou, Qi
BE Dou, Q
   Feragen, A
   Giannarou, S
   Glocker, B
   Lekadir, K
   Schnabel, JA
   Linguraru, MG
TI Incorporating Clinical Guidelines Through Adapting Multi-modal Large
   Language Model for Prostate Cancer PI-RADS Scoring
SO MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI
   2024, PT V
SE Lecture Notes in Computer Science
VL 15005
BP 360
EP 370
DI 10.1007/978-3-031-72086-4_34
DT Proceedings Paper
PD 2024
PY 2024
AB The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in
   the diagnosis of clinically significant prostate cancer through MRI
   imaging. Current deep learning-based PI-RADS scoring methods often lack
   the incorporation of common PI-RADS clinical guideline (PICG) utilized
   by radiologists, potentially compromising scoring accuracy. This paper
   introduces a novel approach that adapts a multi-modal large language
   model (MLLM) to incorporate PICG into PI-RADS scoring model without
   additional annotations and network parameters. We present a designed
   two-stage fine-tuning process aiming at adapting a MLLM originally
   trained on natural images to the MRI images while effectively
   integrating the PICG. Specifically, in the first stage, we develop a
   domain adapter layer tailored for processing 3D MRI inputs and instruct
   the MLLM to differentiate MRI sequences. In the second stage, we
   translate PICG for guiding instructions from the model to generate
   PICG-guided image features. Through such a feature distillation step, we
   align the scoring network's features with the PICG-guided image
   features, which enables the model to effectively incorporate the PICG
   information. We develop our model on a public dataset and evaluate it on
   an in-house dataset. Experimental results demonstrate that our approach
   effectively improves the performance of current scoring networks. Code
   is available at: https://github.com/medair/PICG2scoring
CT 27th International Conference on Medical Image Computing and Computer
   Assisted Intervention (MICCAI)
CY OCT 06-10, 2024
CL Palmeraie Conf Ctr, Marrakesh, MOROCCO
HO Palmeraie Conf Ctr
SP GH Labs; Childrens Natl Hosp; Pierre Fabre; Comp Assisted Med Intervent
   Labex; Multidisciplinary Inst Artificial Intelligence Grenoble Alpes;
   Western Univ, Frugal Biomed Innovat Program; Int Soc Radiol; Medtronic;
   Pasqual Maragall Fdn; Delft Imaging; Univ Barcelona, Artificial
   Intelligence Med Lab; Cadi Ayyad Univ; Natl Ctr Sci & Tech Res
TC 0
Z8 0
ZB 0
ZA 0
ZS 0
ZR 0
Z9 0
DA 2024-11-28
UT WOS:001342230100034
ER

PT J
AU Chong, Yosep
   Nguyen, Anh
   Song, Jin Sol
   Yim, Kwangil
   Park, Jumi
   Kwak, Jin Tae
TI A Generative Artificial Intelligence Framework for Automated Pathologic
   Diagnosis of Gastric Endoscopic Biopsy Samples
SO LABORATORY INVESTIGATION
VL 105
IS 3
MA 1345
AR 103583
DI 10.1016/j.labinv.2024.103583
EA MAR 2025
SU S
DT Meeting Abstract
PD MAR 2025
PY 2025
CT Annual Meeting of the United-States-and-Canadian-Academy-of-Pathology
   (USCAP)
CY MAR 22-27, 2025
CL Boston, MA
SP United States & Canadian Acad Pathol
ZA 0
ZR 0
ZS 0
ZB 0
TC 0
Z8 0
Z9 0
DA 2025-04-19
UT WOS:001464120600032
ER

PT J
AU Cao, Jennie J.
   Kwon, Daniel H.
   Ghaziani, Tara T.
   Kwo, Paul
   Tse, Gary
   Kesselman, Andrew
   Kamaya, Aya
   Tse, Justin R.
TI Large language models' responses to liver cancer surveillance,
   diagnosis, and management questions: accuracy, reliability, readability
SO ABDOMINAL RADIOLOGY
VL 49
IS 12
BP 4286
EP 4294
DI 10.1007/s00261-024-04501-7
EA AUG 2024
DT Article
PD DEC 2024
PY 2024
AB Purpose To assess the accuracy, reliability, and readability of publicly
   available large language models in answering fundamental questions on
   hepatocellular carcinoma diagnosis and management. Methods Twenty
   questions on liver cancer diagnosis and management were asked in
   triplicate to ChatGPT-3.5 (OpenAI), Gemini (Google), and Bing
   (Microsoft). Responses were assessed by six fellowship-trained
   physicians from three academic liver transplant centers who actively
   diagnose and/or treat liver cancer. Responses were categorized as
   accurate (score 1; all information is true and relevant), inadequate
   (score 0; all information is true, but does not fully answer the
   question or provides irrelevant information), or inaccurate (score - 1;
   any information is false). Means with standard deviations were recorded.
   Responses were considered as a whole accurate if mean score was > 0 and
   reliable if mean score was > 0 across all responses for the single
   question. Responses were also quantified for readability using the
   Flesch Reading Ease Score and Flesch-Kincaid Grade Level. Readability
   and accuracy across 60 responses were compared using one-way ANOVAs with
   Tukey's multiple comparison tests. Results Of the twenty questions,
   ChatGPT answered nine (45%), Gemini answered 12 (60%), and Bing answered
   six (30%) questions accurately; however, only six (30%), eight (40%),
   and three (15%), respectively, were both accurate and reliable. There
   were no significant differences in accuracy between any chatbot. ChatGPT
   responses were the least readable (mean Flesch Reading Ease Score 29;
   college graduate), followed by Gemini (30; college) and Bing (40;
   college; p < 0.001). Conclusion Large language models provide complex
   responses to basic questions on hepatocellular carcinoma diagnosis and
   management that are seldomly accurate, reliable, or readable.
TC 8
ZB 1
ZA 0
ZR 0
Z8 1
ZS 0
Z9 8
DA 2024-08-11
UT WOS:001285078800003
PM 39088019
ER

PT C
AU Dodhia, Parth
   Meepagala, Shawn
   Moallem, Golanz
   Rubin, Daniel
   Bean, Gregory
   Rusu, Mirabela
BE Yoshida, H
   Wu, S
TI Assessing breast cancer chemotherapy response in radiology and pathology
   reports via a Large Language Model
SO IMAGING INFORMATICS FOR HEALTHCARE, RESEARCH, AND APPLICATIONS, MEDICAL
   IMAGING 2024
SE Proceedings of SPIE
VL 12931
AR 1293102
DI 10.1117/12.3006495
DT Proceedings Paper
PD 2024
PY 2024
AB A wealth of medical knowledge is used to make clinical decisions, yet
   treatment or disease outcomes are challenging to assess without clinical
   trials. However, clinical trials take time, are expensive, and are
   impossible to perform for every decision. One approach to systematically
   assess treatment outcomes involves the retrospective analysis of
   clinical notes, e.g., radiology and pathology reports. While such
   studies are often performed by clinicians who manually review the notes
   and other information, such retrospective analysis can benefit from the
   automated parsing of radiology and pathology reports to provide
   systematic framework to extract outcome information.
   In this study, we used a large language model, i.e., ChatGPT (GPT-3.5),
   to parse 267 radiology and pathology reports and extract information
   related to response to neoadjuvant chemotherapy in patients with breast
   cancer. Our study includes a heterogeneous group of 89 women who
   underwent neoadjuvant therapy and underwent two MRI exams, pre- and
   post-therapy, followed by surgery (lumpectomy or mastectomy). We
   assessed the treatment response based on clinical reports from the
   post-therapy surgical excision. From the reports, we extracted the
   number of lesions, their anatomic location, and size.
   Our study provides insight into neoadjuvant chemotherapy response,
   indicating that even cases with complete MRI response can still have
   residual invasive breast carcinoma (1/3 of subjects), and, on the other
   hand, even those with reduced MRI response (<30% reduction in tumor
   size) can have no residual tumor at surgery, indicating that when cancer
   responds to treatment, it may not be captured by the MRI. The large
   language model achieved sensitivities of 84-94% in extracting the
   information from radiology reports, but had lower performance in the
   pathology reports, 72-87%, where more information is provided in free
   format. While this study is preliminary and performed in a small cohort,
   it illustrates the complexity of outcome prediction using radiology
   images.
CT Conference on Medical Imaging - Imaging Informatics for Healthcare,
   Research, and Applications
CY FEB 19-21, 2024
CL San Diego, CA
SP SPIE; Amer Assoc Physicists Med; Radiol Soc N Amer; World Mol Imaging
   Soc; Soc Imaging Informat Med; Int Fdn Comp Assisted Radiol & Surg; Med
   Image Percept Soc
ZA 0
ZB 0
ZR 0
ZS 0
Z8 0
TC 0
Z9 0
DA 2024-05-31
UT WOS:001219280700001
ER

PT J
AU Patricio, Cristiano
   Teixeira, Luis F.
   Neves, Joao C.
TI A two-step concept-based approach for enhanced interpretability and
   trust in skin lesion diagnosis
SO COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL
VL 28
BP 71
EP 79
DI 10.1016/j.csbj.2025.02.013
EA FEB 2025
DT Article
PD 2025
PY 2025
AB The main challenges hindering the adoption of deep learning-based
   systems in clinical settings are the scarcity of annotated data and the
   lack of interpretability and trust in these systems. Concept Bottleneck
   Models (CBMs) offer inherent interpretability by constraining the final
   disease prediction on a set of human-understandable concepts. However,
   this inherent interpretability comes at the cost of greater annotation
   burden. Additionally, adding new concepts requires retraining the entire
   system. In this work, we introduce a novel two-step methodology that
   addresses both of these challenges. By simulating the two stages of a
   CBM, we utilize a pretrained Vision Language Model (VLM) to
   automatically predict clinical concepts, and an off-the-shelf Large
   Language Model (LLM) to generate disease diagnoses grounded on the
   predicted concepts. Furthermore, our approach supports test-time human
   intervention, enabling corrections to predicted concepts, which improves
   final diagnoses and enhances transparency in decision-making. We
   validate our approach on three skin lesion datasets, demonstrating that
   it outperforms traditional CBMs and state-of-the-art explainable
   methods, all without requiring any training and utilizing only a few
   annotated examples. The code is available at
   https://github.com/CristianoPatricio/2step-concept-based-skin-diagnosis.
ZA 0
ZB 0
TC 0
ZS 0
ZR 0
Z8 0
Z9 0
DA 2025-03-07
UT WOS:001433842900001
PM 40093651
ER

PT J
AU Sultan, Iyad
   Al-Abdallat, Haneen
   Alnajjar, Zaina
   Ismail, Layan
   Abukhashabeh, Razan
   Bitar, Layla
   Abu Shanap, Mayada
TI Using ChatGPT to Predict Cancer Predisposition Genes: A Promising Tool
   for Pediatric Oncologists
SO CUREUS JOURNAL OF MEDICAL SCIENCE
VL 15
IS 10
AR e47594
DI 10.7759/cureus.47594
DT Article
PD OCT 24 2023
PY 2023
AB Background: Determining genetic susceptibility for cancer predisposition
   syndromes (CPS) through cancer predisposition genes (CPGs) testing is
   critical in facilitating appropriate prevention and surveillance
   strategies. This study investigates the use of ChatGPT, a large language
   model, in predicting CPGs using clinical notes.Methods: Our study
   involved 53 patients with pathogenic CPG mutations. Two kinds of
   clinical notes were used: the first visit note, containing a thorough
   history and physical exam, and the genetic clinic note, summarizing the
   patient's diagnosis and family history. We asked ChatGPT to recommend
   CPS genes based on these notes and compared these predictions with
   previously identified mutations.Results: Rb1 was the most frequently
   mutated gene in our cohort (34%), followed by NF1 (9.4%), TP53 (5.7%),
   and VHL (5.7%). Out of 53 patients, 30 had genetic clinic notes of a
   median length of 54 words. ChatGPT correctly predicted the gene in 93%
   of these cases. However, it failed to predict EPCAM and VHL genes in
   specific patients. For the first visit notes (median length: 461 words),
   ChatGPT correctly predicted the gene in 64% of these cases. Conclusion:
   ChatGPT shows promise in predicting CPGs from clinical notes,
   particularly genetic clinic notes. This approach may be useful in
   enhancing CPG testing, especially in areas lacking genetic testing
   resources. With further training, there is a possibility for ChatGPT to
   improve its predictive potential and expand its clinical applicability.
   However, additional research is needed to explore the full potential and
   applicability of ChatGPT.
ZA 0
ZS 0
ZB 1
Z8 0
ZR 0
TC 6
Z9 6
DA 2024-01-08
UT WOS:001109817700029
PM 38021917
ER

PT J
AU Kanemaru, Noriko
   Yasaka, Koichiro
   Fujita, Nana
   Kanzawa, Jun
   Abe, Osamu
TI The Fine-Tuned Large Language Model for Extracting the Progressive Bone
   Metastasis from Unstructured Radiology Reports
SO JOURNAL OF IMAGING INFORMATICS IN MEDICINE
VL 38
IS 2
BP 865
EP 872
DI 10.1007/s10278-024-01242-3
EA AUG 2024
DT Article
PD APR 2025
PY 2025
AB Early detection of patients with impending bone metastasis is crucial
   for prognosis improvement. This study aimed to investigate the
   feasibility of a fine-tuned, locally run large language model (LLM) in
   extracting patients with bone metastasis in unstructured Japanese
   radiology report and to compare its performance with manual annotation.
   This retrospective study included patients with "metastasis" in
   radiological reports (April 2018-January 2019, August-May 2022, and
   April-December 2023 for training, validation, and test datasets of 9559,
   1498, and 7399 patients, respectively). Radiologists reviewed the
   clinical indication and diagnosis sections of the radiological report
   (used as input data) and classified them into groups 0 (no bone
   metastasis), 1 (progressive bone metastasis), and 2 (stable or decreased
   bone metastasis). The data for group 0 was under-sampled in training and
   test datasets due to group imbalance. The best-performing model from the
   validation set was subsequently tested using the testing dataset. Two
   additional radiologists (readers 1 and 2) were involved in classifying
   radiological reports within the test dataset for testing purposes. The
   fine-tuned LLM, reader 1, and reader 2 demonstrated an accuracy of
   0.979, 0.996, and 0.993, sensitivity for groups 0/1/2 of
   0.988/0.947/0.943, 1.000/1.000/0.966, and 1.000/0.982/0.954, and time
   required for classification (s) of 105, 2312, and 3094 in under-sampled
   test dataset (n = 711), respectively. Fine-tuned LLM extracted patients
   with bone metastasis, demonstrating satisfactory performance that was
   comparable to or slightly lower than manual annotation by radiologists
   in a noticeably shorter time.
ZA 0
ZS 0
ZB 0
Z8 0
TC 4
ZR 0
Z9 4
DA 2024-09-01
UT WOS:001298719700004
PM 39187702
ER

PT J
AU Shah, Kevin P.
   Dey, Shirin A.
   Pothula, Shravya
   Abud, Arnold
   Jain, Sukrit
   Srivastava, Aniruddha
   Dommaraju, Sagar
   Komanduri, Srinadh
TI Artificial Intelligence Showdown in Gastroenterology: A Comparative
   Analysis of Large Language Models (LLMs) in Tackling Board-Style Review
   Questions
SO AMERICAN JOURNAL OF GASTROENTEROLOGY
VL 119
IS 10S
MA S2194
BP S1567
EP S1568
DI 10.14309/01.ajg.0001038144.51724.aa
SU 10
DT Meeting Abstract
PD OCT 2024
PY 2024
CT Annual Meeting of the American-College-of-Gastroenterology (ACG)
CY OCT 25-30, 2024
CL Pennsylvania Convention Cent, Philadelphia, PA
HO Pennsylvania Convention Cent
SP Amer Coll Gastroenterol
ZS 0
TC 0
ZA 0
Z8 0
ZR 0
ZB 0
Z9 0
DA 2024-11-30
UT WOS:001359889800005
ER

PT J
AU Hong, Huixiao
   Slikker, William
TI Integrating artificial intelligence with bioinformatics promotes public
   health
SO EXPERIMENTAL BIOLOGY AND MEDICINE
VL 248
IS 21
BP 1905
EP 1907
DI 10.1177/15353702231223575
EA JAN 2024
DT Editorial Material
PD NOV 2023
PY 2023
Z8 0
ZA 0
TC 1
ZS 0
ZR 0
ZB 0
Z9 1
DA 2024-01-12
UT WOS:001137033900001
PM 38179798
ER

PT J
AU Gungor, Nur Dokuzeylul
   Esen, Fatih Sinan
   Tasci, Tolga
   Gungor, Kagan
   Cil, Kaan
TI Navigating Gynecological Oncology with Different Versions of ChatGPT: A
   Transformative Breakthrough or the Next Black Box Challenge?
SO ONCOLOGY RESEARCH AND TREATMENT
DI 10.1159/000543173
EA DEC 2024
DT Article; Early Access
PY 2024
AB Introduction: The study evaluates the performance of large language
   model versions of ChatGPT - ChatGPT-3.5, ChatGPT-4, and ChatGPT-Omni -
   in addressing inquiries related to the diagnosis and treatment of
   gynecological cancers, including ovarian, endometrial, and cervical
   cancers. Methods: A total of 804 questions were equally distributed
   across four categories: true/false, multiple-choice, open-ended, and
   case-scenario, with each question type representing varying levels of
   complexity. Performance was assessed using a six-point Likert scale,
   focusing on accuracy, completeness, and alignment with established
   clinical guidelines. Results: For true/false queries, ChatGPT-Omni
   achieved accuracy rates of 100% for easy, 98% for medium, and 97% for
   complicated questions, higher than ChatGPT-4 (94%, 90%, 85%) and
   ChatGPT-3.5 (90%, 85%, 80%) (p = 0.041, 0.023, 0.014, respectively). In
   multiple-choice, ChatGPT-Omni maintained superior accuracy with 100% for
   easy, 98% for medium, and 93% for complicated queries, compared to
   ChatGPT-4 (92%, 88%, 80%) and ChatGPT-3.5 (85%, 80%, 70%) (p = 0.035,
   0.028, 0.011). For open-ended questions, ChatGPT-Omni had mean Likert
   scores of 5.8 for easy, 5.5 for medium, and 5.2 for complex levels,
   outperforming ChatGPT-4 (5.4, 5.0, 4.5) and ChatGPT-3.5 (5.0, 4.5, 4.0)
   (p = 0.037, 0.026, 0.015). Similar trends were observed in case-scenario
   questions, where ChatGPT-Omni achieved scores of 5.6, 5.3, and 4.9 for
   easy, medium, and hard levels, respectively (p = 0.017, 0.008, 0.012).
   Conclusions: ChatGPT-Omni exhibited superior performance in responding
   to clinical queries related to gynecological cancers, underscoring its
   potential utility as a decision support tool and an educational resource
   in clinical practice.
Z8 0
ZB 0
ZS 0
TC 0
ZA 0
ZR 0
Z9 0
DA 2025-02-05
UT WOS:001404655900001
PM 39689699
ER

PT J
AU Xiong, Yichun
   Li, Jiaqi
   Jin, Wang
   Sheng, Xiaoran
   Peng, Hui
   Wang, Zhiyi
   Jia, Caifeng
   Zhuo, Lili
   Zhang, Yibo
   Huang, Jingzhe
   Zhai, Modi
   Lyu, Beibei
   Sun, Jie
   Zhou, Meng
TI PCMR: a comprehensive precancerous molecular resource
SO SCIENTIFIC DATA
VL 12
IS 1
AR 551
DI 10.1038/s41597-025-04899-9
DT Article
PD APR 1 2025
PY 2025
AB Early detection and intervention of precancerous lesions are crucial in
   reducing cancer morbidity and mortality. Comprehensive analysis of
   genomic, transcriptomic, proteomic and epigenomic alterations can
   provide insights into the early stages of carcinogenesis. However, the
   lacke of an integrated, well-curated data resource of molecular
   signatures limits our understanding of precancerous processes. Here, we
   introduce a comprehensive PreCancerous Molecular Resource (PCMR), which
   compiles 25,828 molecular profiles of precancerous samples paired with
   normal or malignant counterparts. These profiles cover precancerous
   lesions of 35 cancer types across 20 organs and tissues, derived from
   tissue samples, liquid biopsies, cell lines and organoids, with data
   from transcriptomics, proteomics and epigenomics. PCMR includes 62,566
   precancer-gene associations derived from differential analysis and
   text-mining using the ChatGPT large language model. We examined PCMR
   dataset reliability and significance by the authoritative precancerous
   molecular signature, along with its biological and clinical relevance.
   Overall, PCMR will serve as a valuable resource for advancing precancer
   research and ultimately improving patient outcomes.
ZR 0
ZB 0
ZS 0
ZA 0
Z8 0
TC 0
Z9 0
DA 2025-04-11
UT WOS:001459759400009
PM 40169679
ER

PT J
AU Baumgaertner, Kilian
   Byczkowski, Michael
   Schmid, Tamara
   Muschko, Marc
   Woessner, Philipp
   Gerlach, Axel
   Bonekamp, David
   Schlemmer, Heinz-Peter
   Hohenfellner, Markus
   Goertz, Magdalena
TI Effectiveness of the Medical Chatbot PROSCA to Inform Patients About
   Prostate Cancer: Results of a Randomized Controlled Trial
SO EUROPEAN UROLOGY OPEN SCIENCE
VL 69
BP 80
EP 88
DI 10.1016/j.euros.2024.08.022
EA SEP 2024
DT Article
PD NOV 2024
PY 2024
AB Background and objective: Artificial intelligence (AI)-powered
   conversational agents are increasingly finding application in health
   care, as these can provide patient education at any time. However, their
   effectiveness in medical settings remains largely unexplored. This study
   aimed to assess the impact of the chatbot "PROState cancer
   Conversational Agent"(PROSCA), which was trained to provide validated
   support from diagnostic tests to treatment options for men facing
   prosate cancer (PC) diagnosis. Methods: The chatbot PROSCA, developed by
   urologists at Heidelberg University Hospital and SAP SE, was evaluated
   through a randomized controlled trial (RCT). Patients were assigned to
   either the chatbot group, receiving additional access to PROSCA
   alongside standard information by urologists, or the control group
   (1:1), receiving standard information. A total of 112 men were included,
   of whom 103 gave feedback at study completion. Key findings and
   limitations: Overtime, patients' information needs decreased
   significantly more in the chatbot group than in the control group (p =
   0.035). In the chatbot group, 43/54 men (79.6%) used PROSCA, and all of
   them found it easy to use. Of the men, 71.4% agreed that the chatbot
   improved their informedness about PC and 90.7% would like to use PROSCA
   again. Limitations are study sample size, singlecenter design, and
   specific clinical application. Conclusions and clinical implications:
   With the introduction of the PROSCA chatbot, we created and evaluated an
   innovative, evidence-based AI health information tool as an additional
   source of information for PC. Our RCT results showed significant
   benefits of the chatbot in reducing patients' information needs and
   enhancing their understanding of PC. This easy-to-use AI tool provides
   accurate, timely, and accessible support, demonstrating its value in the
   PC diagnosis process. Future steps include further customization of the
   chatbot's responses and integration with the existing health care
   systems to maximize its impact on patient outcomes. Patient summary:
   This study evaluated an artificial intelligence-powered chatbot- PROSCA,
   a digital tool designed to support men facing prostate cancer diagnosis
   by providing validated information from diagnosis to treatment. Results
   showed that patients who used the chatbot as an additional tool felt
   better informed than those who received standard information from
   urologists. The majority of users appreciated the ease of use of the
   chatbot and expressed a desire to use it again; this suggests that
   PROSCA could be a valuable resource to improve patient understanding in
   prostate cancer diagnosis. (c) 2024 The Author(s). Published by Elsevier
   B.V. on behalf of European Association of Urology. This is an open
   access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).
ZB 0
ZA 0
TC 1
ZS 0
Z8 0
ZR 0
Z9 1
DA 2024-09-29
UT WOS:001318013100001
PM 39329071
ER

PT J
AU Park, Hyung Jun
   Huh, Jin-Young
   Chae, Ganghee
   Choi, Myeong Geun
TI Extraction of clinical data on major pulmonary diseases from
   unstructured radiologic reports using a large language model
SO PLOS ONE
VL 19
IS 11
AR e0314136
DI 10.1371/journal.pone.0314136
DT Article
PD NOV 25 2024
PY 2024
AB Despite significant strides in big data technology, extracting
   information from unstructured clinical data remains a formidable
   challenge. This study investigated the utility of large language models
   (LLMs) for extracting clinical data from unstructured radiological
   reports without additional training. In this retrospective study, 1800
   radiologic reports, 600 from each of the three university hospitals,
   were collected, with seven pulmonary outcomes defined. Three
   pulmonology-trained specialists discerned the presence or absence of
   diseases. Data extraction from the reports was executed using Google
   Gemini Pro 1.0, OpenAI's GPT-3.5, and GPT-4. The gold standard was
   predicated on agreement between at least two pulmonologists. This study
   evaluated the performance of the three LLMs in diagnosing seven
   pulmonary diseases (active tuberculosis, emphysema, interstitial lung
   disease, lung cancer, pleural effusion, pneumonia, and pulmonary edema)
   utilizing chest radiography and computed tomography scans. All models
   exhibited high accuracy (0.85-1.00) for most conditions. GPT-4
   consistently outperformed its counterparts, demonstrating a sensitivity
   of 0.71-1.00; specificity of 0.89-1.00; and accuracy of 0.89 and 0.99
   across both modalities, thus underscoring its superior capability in
   interpreting radiological reports. Notably, the accuracy of pleural
   effusion and emphysema on chest radiographs and pulmonary edema on chest
   computed tomography scans reached 0.99. The proficiency of LLMs,
   particularly GPT-4, in accurately classifying unstructured radiological
   data hints at their potential as alternatives to the traditional manual
   chart reviews conducted by clinicians.
TC 1
Z8 0
ZA 0
ZS 0
ZR 0
ZB 0
Z9 1
DA 2024-12-13
UT WOS:001363435700050
PM 39585830
ER

PT J
AU Yang, Xiongwen
   Zhang, Yun
   Jiang, Jinyan
   Chen, Zhijun
   Bai, Rinasu
   Yuan, Zihao
   Dong, Longyan
   Xiao, Yi
   Liu, Di
   Deng, Huiyin
   Huang, Jian
   Shi, Huiyou
   Liu, Dan
   Liang, Maoli
   Tang, Weijuan
   Xu, Chuan
TI Harnessing GPT-4 for automated error detection in pathology reports:
   Implications for oncology diagnostics
SO DIGITAL HEALTH
VL 11
AR 20552076251346703
DI 10.1177/20552076251346703
DT Article
PD 2025
PY 2025
AB Objective Accurate pathology reports are crucial for the diagnosis and
   treatment planning of cancer patients. However, these reports are prone
   to errors due to time pressures, subjective interpretation, and
   inconsistencies among professionals. Addressing these errors is vital
   for improving oncology care outcomes. Artificial intelligence (AI)
   systems, such as GPT-4, offer the potential to enhance diagnostic
   accuracy and efficiency.Methods A total of 700 malignant tumor pathology
   reports were collected from four hospitals. Of these, 350 reports had
   deliberate errors introduced by a senior pathologist, mimicking
   real-world reporting challenges. Error detection performance was
   evaluated by comparing GPT-4 to six human pathologists (two seniors, two
   attending pathologists, and two residents). Key metrics included error
   detection rates with Wilson confidence intervals and processing time per
   report.Results GPT-4 detected 88% of errors (350/400; 95% CI: [84, 91]),
   compared to a 95% detection rate by the top senior pathologist (382/400;
   95% CI: [93, 97]). GPT-4 significantly reduced the average processing
   time to 4.03 seconds per report, compared to 65.64 seconds for the
   fastest human pathologist. However, GPT-4 exhibited a higher rate of
   false positives (2.3%; 95% CI: [1.52, 3.01]) compared to the
   best-performing senior pathologist (0.3%; 95% CI: [0.01,
   0.91]).Conclusions GPT-4 demonstrates substantial potential in improving
   the efficiency and accuracy of pathology error detection, which could
   accelerate clinical workflows and enhance cancer diagnostics. However,
   its higher false-positive rate emphasizes the need for human oversight
   to ensure safe implementation in clinical practice.
TC 0
Z8 0
ZB 0
ZS 0
ZA 0
ZR 0
Z9 0
DA 2025-06-03
UT WOS:001498668600001
PM 40453047
ER

PT J
AU Odabashian, Roupen
   Bastin, Donald
   Jones, Georden
   Manzoor, Maria
   Tangestaniapour, Sina
   Assad, Malke
   Lakhani, Sunita
   Odabashian, Maritsa
   Mcgee, Sharon
TI Assessment of ChatGPT-3.5's Knowledge in Oncology: Comparative Study
   with ASCO-SEP Benchmarks
SO JMIR AI
VL 3
AR e50442
DI 10.2196/50442
DT Article
PD 2024
PY 2024
AB Background: ChatGPT (Open AI) is a state-of-the-art large language model
   that uses artificial intelligence (AI) to address questions across
   diversetopics. TheAmerican Society of Clinical Oncology Self-Evaluation
   Program (ASCO-SEP) created a comprehensive educational program to help
   physicians keep up to date with the many rapid advances in the field.
   The question bank consists of multiple choice questions addressing the
   many facets of cancer care, including diagnosis, treatment, and
   supportive care. As ChatGPT applications rapidly expand, it becomesvital
   to ascertain if the knowledge of ChatGPT-3.5 matches the established
   standards that oncologists are recommended to follow. Objective: This
   studyaimstoevaluatewhetherChatGPT-3.5'sknowledgealigns with
   theestablished benchmarksthat oncologists are expected to adhere to.
   This will furnish us with a deeper understanding of the potential
   applications of this tool as a support forclinical decision-making.
   Methods: We conducted a systematic assessment of the performance of
   ChatGPT-3.5 on theASCO-SEP, the leading educational and assessment tool
   for medical oncologists in training and practice. Over 1000 multiple
   choice questions covering the spectrum of cancer care were extracted.
   Questions were categorized by cancer type or discipline, with
   subcategorization as treatment, diagnosis, or other. Answers were scored
   as correct if ChatGPT-3.5 selected the answer as defined by ASCO-SEP.
   Results: Overall, ChatGPT-3.5 achieved a score of 56.1% (583/1040) for
   the correct answers provided. The program demonstrated varying levels of
   accuracy across cancer types or disciplines. The highest accuracy was
   observed in questions related to developmental therapeutics (8/10; 80%
   correct), while the lowest accuracy was observed in questions related to
   gastrointestinal cancer (102/209; 48.8% correct). There was no
   significant difference in the program's performance across the
   predefined subcategoriesof diagnosis, treatment, and other (P=.16, which
   isgreaterthan .05). Conclusions:This study evaluated ChatGPT-3.5's
   oncology knowledge using the ASCO-SEP, aiming to address uncertainties
   regarding AI tools like ChatGPT in clinical decision-making. Our
   findings suggest that while ChatGPT-3.5 offers a hopeful outlook for AI
   in oncology, its present performance in ASCO-SEP tests necessitates
   further refinement to reach the requisite competency levels. Future
   assessments could explore ChatGPT's clinical decision support
   capabilities with real-world clinical scenarios, its ease of integration
   into medical workflows, and its potentialto foster interdisciplinary
   collaboration and patient engagement in health care settings.
ZB 0
ZA 0
TC 3
Z8 0
ZS 0
ZR 0
Z9 3
DA 2024-12-21
UT WOS:001374817300001
PM 38875575
ER

PT J
AU Ding, Liya
   Fan, Lei
   Shen, Miao
   Wang, Yawen
   Sheng, Kaiqin
   Zou, Zijuan
   An, Huimin
   Jiang, Zhinong
TI Evaluating ChatGPT's diagnostic potential for pathology images
SO FRONTIERS IN MEDICINE
VL 11
AR 1507203
DI 10.3389/fmed.2024.1507203
DT Article
PD JAN 23 2025
PY 2025
AB Background Chat Generative Pretrained Transformer (ChatGPT) is a type of
   large language model (LLM) developed by OpenAI, known for its extensive
   knowledge base and interactive capabilities. These attributes make it a
   valuable tool in the medical field, particularly for tasks such as
   answering medical questions, drafting clinical notes, and optimizing the
   generation of radiology reports. However, keeping accuracy in medical
   contexts is the biggest challenge to employing GPT-4 in a clinical
   setting. This study aims to investigate the accuracy of GPT-4, which can
   process both text and image inputs, in generating diagnoses from
   pathological images.Methods This study analyzed 44 histopathological
   images from 16 organs and 100 colorectal biopsy photomicrographs. The
   initial evaluation was conducted using the standard GPT-4 model in
   January 2024, with a subsequent re-evaluation performed in July 2024.
   The diagnostic accuracy of GPT-4 was assessed by comparing its outputs
   to a reference standard using statistical measures. Additionally, four
   pathologists independently reviewed the same images to compare their
   diagnoses with the model's outputs. Both scanned and photographed images
   were tested to evaluate GPT-4's generalization ability across different
   image types.Results GPT-4 achieved an overall accuracy of 0.64 in
   identifying tumor imaging and tissue origins. For colon polyp
   classification, accuracy varied from 0.57 to 0.75 in different subtypes.
   The model achieved 0.88 accuracy in distinguishing low-grade from
   high-grade dysplasia and 0.75 in distinguishing high-grade dysplasia
   from adenocarcinoma, with a high sensitivity in detecting
   adenocarcinoma. Consistency between initial and follow-up evaluations
   showed slight to moderate agreement, with Kappa values ranging from
   0.204 to 0.375.Conclusion GPT-4 demonstrates the ability to diagnose
   pathological images, showing improved performance over earlier versions.
   Its diagnostic accuracy in cancer is comparable to that of pathology
   residents. These findings suggest that GPT-4 holds promise as a
   supportive tool in pathology diagnostics, offering the potential to
   assist pathologists in routine diagnostic workflows.
ZS 0
Z8 0
TC 0
ZR 0
ZB 0
ZA 0
Z9 0
DA 2025-02-10
UT WOS:001414088100001
PM 39917264
ER

PT J
AU Fung, Damien
   Arbour, Gregory
   Malik, Krisha
   Muzio, Kaitlin
   Ng, Raymond
TI Using a Longformer Large Language Model for Segmenting Unstructured
   Cancer Pathology Reports
SO JCO CLINICAL CANCER INFORMATICS
VL 9
AR e2400143
DI 10.1200/CCI-24-00143
DT Article
PD MAR 2025
PY 2025
AB PURPOSEMany Natural Language Processing (NLP) methods achieve greater
   performance when the input text is preprocessed to remove extraneous or
   unnecessary text. A technique known as text segmentation can facilitate
   this step by isolating key sections from a document. Give that
   transformer models-such as Bidirectional Encoder Representations from
   Transformers (BERT)-have demonstrated state-of-the-art performance on
   many NLP tasks, it is desirable to leverage such models for
   segmentation. However, transformer models are typically limited to only
   512 input tokens and are not well suited for lengthy documents such as
   cancer pathology reports. The Longformer is a modified transformer model
   designed to intake longer documents while retaining the positive
   characteristics of standard transformers. This study presents a
   Longformer model fine-tuned for cancer pathology report
   segmentation.METHODSWe fine-tuned a Longformer Question-Answer (QA)
   model on 504 manually annotated pathology reports to isolate sections
   such as diagnosis, addenda, and clinical history. We compared baseline
   methods including regular expressions (regex) and BERT QA. However,
   those methods may fail to correctly identify section boundaries. Model
   performance was evaluated using sequence recall, precision, and F1
   score.RESULTSFinal test results were obtained on a hold-out test set of
   304 cancer pathology reports. We report sequence F1 scores for the
   following sections: diagnosis (0.77), addenda (0.48), clinical history
   (0.89), and overall (0.68).CONCLUSIONWe present a fine-tuned Longformer
   model to isolate key sections from cancer pathology reports for
   downstream analyses. Our model performs segmentation with greater
   accuracy.
ZS 0
ZA 0
ZB 0
ZR 0
TC 0
Z8 0
Z9 0
DA 2025-03-09
UT WOS:001436183800001
PM 40036729
ER

PT J
AU Liu, Hui
   Peng, Jialun
   Li, Lu
   Deng, Ao
   Huang, XiangXin
   Yin, Guobing
   Luo, Haojun
TI Large Language Models as a Consulting Hotline for Patients With Breast
   Cancer and Specialists in China: Cross-Sectional Questionnaire Study
SO JMIR MEDICAL INFORMATICS
VL 13
AR e66429
DI 10.2196/66429
DT Article
PD 2025
PY 2025
AB Background: The disease burden of breast cancer is increasing in China.
   Guiding people to obtain accurate information on breast cancer and
   improving the public's health literacy are crucial for the early
   detection and timely treatment of breast cancer. Large language model
   (LLM) is a currently popular source of health information. However, the
   accuracy and practicality of the breast cancer-related information
   provided by LLMs have not yet been evaluated. Objective: This study aims
   to evaluate and compare the accuracy, practicality, and
   generalization-specificity of responses to breast cancer-related
   questions from two LLMs, ChatGPT and ERNIE Bot (EB). Methods: The
   questions asked to the LLMs consisted of a patient questionnaire and an
   expert questionnaire, each containing 15 questions. ChatGPT was queried
   in both Chinese and English, recorded as ChatGPT-Chinese (ChatGPT-C) and
   ChatGPT-English (ChatGPT-E) respectively, while EB was queried in
   Chinese. The accuracy, practicality, and generalization-specificity of
   each inquiry's responses were rated by a breast cancer multidisciplinary
   treatment team using Likert scales. Results: Overall, for both the
   patient and expert questionnaire, the accuracy and practicality of
   responses from ChatGPT-E were significantly higher than those from
   ChatGPT-C and EB (all Ps<.001). However, the responses from all LLMs are
   relatively generalized, leading to lower accuracy and practicality for
   the expert questionnaire compared to the patient questionnaire.
   Additionally, there were issues such as the lack of supporting evidence
   and potential ethical risks in the responses of LLMs. Conclusions:
   Currently, compared to other LLMs, ChatGPT-E has demonstrated greater
   potential for application in educating Chinese patients with breast
   cancer, and may serve as an effective tool for them to obtain health
   information. However, for breast cancer specialists, these LLMs are not
   yet suitable for assisting in clinical diagnosis or treatment
   activities. Additionally, data security, ethical, and legal risks
   associated with using LLMs in clinical practice cannot be ignored. In
   the future, further research is needed to determine the true efficacy of
   LLMs in clinical scenarios related to breast cancer in China.
TC 0
Z8 0
ZB 0
ZA 0
ZR 0
ZS 0
Z9 0
DA 2025-06-14
UT WOS:001506204800004
PM 40424585
ER

PT J
AU Xia, Shujun
   Hua, Qing
   Mei, Zihan
   Xu, Wenwen
   Lai, Limei
   Wei, Minyan
   Qin, Yu
   Luo, Lin
   Wang, Changhua
   Huo, ShengNan
   Fu, Lijun
   Zhou, Feidu
   Wu, Jiang
   Zhang, Li
   Lv, De
   Li, Jianxin
   Wang, Xin
   Li, Ning
   Song, Yanyan
   Zhou, Jianqiao
TI Clinical application potential of large language model: a study based on
   thyroid nodules
SO ENDOCRINE
VL 87
IS 1
BP 206
EP 213
DI 10.1007/s12020-024-03981-3
EA JUL 2024
DT Article
PD JAN 2025
PY 2025
AB Background Limited data indicated the performance of large language
   model (LLM) taking on the role of doctors. We aimed to investigate the
   potential for ChatGPT-3.5 and New Bing Chat acting as doctors using
   thyroid nodules as an example. Methods A total of 145 patients with
   thyroid nodules were included for generating questions. Each question
   was entered into chatbot of ChatGPT-3.5 and New Bing Chat five times and
   five responses were acquired respectively. These responses were compared
   with answers given by five junior doctors. Responses from five senior
   doctors were regarded as gold standard. Accuracy and reproducibility of
   responses from ChatGPT-3.5 and New Bing Chat were evaluated. Results The
   accuracy of ChatGPT-3.5 and New Bing Chat in answering Q2, Q3, Q5 were
   lower than that of junior doctors (all P < 0.05), while both LLMs were
   comparable to junior doctors when answering Q4 and Q6. In terms of "high
   reproducibility and accuracy", ChatGPT-3.5 outperformed New Bing Chat in
   Q1 and Q5 (P < 0.001 and P = 0.008, respectively), but showed no
   significant difference in Q2, Q3, Q4, and Q6 (P > 0.05 for all). New
   Bing Chat generated higher accuracy than ChatGPT-3.5 (72.41% vs 58.62%)
   (P = 0.003) in decision making of thyroid nodules, and both were less
   accurate than junior doctors (89.66%, P < 0.001 for both). Conclusions
   The exploration of ChatGPT-3.5 and New Bing Chat in the diagnosis and
   management of thyroid nodules illustrates that LLMs currently
   demonstrate the potential for medical applications, but do not yet reach
   the clinical decision-making capacity of doctors.
Z8 0
TC 3
ZA 0
ZS 0
ZR 0
ZB 0
Z9 3
DA 2024-08-06
UT WOS:001281038100001
PM 39080210
ER

PT J
AU Kuerbanjiang, Warisijiang
   Peng, Shengzhe
   Jiamaliding, Yiershatijiang
   Yi, Yuexiong
TI Performance Evaluation of Large Language Models in Cervical Cancer
   Management Based on a Standardized Questionnaire: Comparative Study
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 27
AR e63626
DI 10.2196/63626
DT Article
PD FEB 5 2025
PY 2025
AB Background: Cervical cancer remains the fourth leading cause of death
   among women globally, with a particularly severe burden in low-resource
   settings. A comprehensive approach-from screening to diagnosis and
   treatment-is essential for effective prevention and management. Large
   language models (LLMs) have emerged as potential tools to support health
   care, though their specific role in cervical cancer management remains
   underexplored. Objective: This study aims to systematically evaluate the
   performance and interpretability of LLMs in cervical cancer management.
   Methods: Models were selected from the AlpacaEval leaderboard version
   2.0 and based on the capabilities of our computer. The questions
   inputted into the models cover aspects of general knowledge, screening,
   diagnosis, and treatment, according to guidelines. The prompt was
   developed using the Context, Objective, Style, Tone, Audience, and
   Response (CO-STAR) framework. Responses were evaluated for accuracy,
   guideline compliance, clarity, and practicality, graded as A, B, C, and
   D with corresponding scores of 3, 2, 1, and 0. The effective rate was
   calculated as the ratio of A and B responses to the total number of
   designed questions. Local Interpretable Model-Agnostic Explanations
   (LIME) was used to explain and enhance physicians' trust in model
   outputs within the medical context. Results: Nine models were included
   in this study, and a set of 100 standardized questions covering general
   information, screening, diagnosis, and treatment was designed based on
   international and national guidelines. Seven models (ChatGPT-4.0 Turbo,
   Claude 2, Gemini Pro, Mistral-7B-v0.2, Starling-LM-7B alpha, HuatuoGPT,
   and BioMedLM 2.7B) provided stable responses. Among all the models
   included, ChatGPT-4.0 Turbo ranked first with a mean score of 2.67 (95%
   CI 2.54-2.80; effective rate 94.00%) with a prompt and 2.52 (95% CI
   2.37-2.67; effective rate 87.00%) without a prompt, outperforming the
   other 8 models (P<.001). Regardless of prompts, QiZhenGPT consistently
   ranked among the lowest-performing models, with P<.01 in comparisons
   against all models except BioMedLM. Interpretability analysis showed
   that prompts improved alignment with human annotations for proprietary
   models (median intersection over union 0.43), while medical-specialized
   models exhibited limited improvement. Conclusions: Proprietary LLMs,
   particularly ChatGPT-4.0 Turbo and Claude 2, show promise in clinical
   decision-making involving logical analysis. The use of prompts can
   enhance the accuracy of some models in cervical cancer management to
   varying degrees. Medical-specialized models, such as HuatuoGPT and
   BioMedLM, did not perform as well as expected in this study. By
   contrast, proprietary models, particularly those augmented with prompts,
   demonstrated notable accuracy and interpretability in medical tasks,
   such as cervical cancer management. However, this study underscores the
   need for further research to explore the practical application of LLMs
   in medical practice.
ZS 0
Z8 0
ZA 0
TC 0
ZB 0
ZR 0
Z9 0
DA 2025-02-27
UT WOS:001424878900005
PM 39908540
ER

PT J
AU Chatziisaak, Dimitrios
   Burri, Pascal
   Sparn, Moritz
   Hahnloser, Dieter
   Steffen, Thomas
   Bischofberger, Stephan
TI Concordance of ChatGPT artificial intelligence decision-making in
   colorectal cancer multidisciplinary meetings: retrospective study
SO BJS OPEN
VL 9
IS 3
AR zraf040
DI 10.1093/bjsopen/zraf040
DT Article
PD JUN 2025
PY 2025
AB Background The objective of this study was to evaluate the concordance
   between therapeutic recommendations proposed by a multidisciplinary team
   meeting and those generated by a large language model (ChatGPT) for
   colorectal cancer. Although multidisciplinary teams represent the
   'standard' for decision-making in cancer treatment, they require
   significant resources and may be susceptible to human bias. Artificial
   intelligence, particularly large language models such as ChatGPT, has
   the potential to enhance or optimize the decision-making processes. The
   present study examines the potential for integrating artificial
   intelligence into clinical practice by comparing multidisciplinary team
   decisions with those generated by ChatGPT.Methods A retrospective,
   single-centre study was conducted involving consecutive patients with
   newly diagnosed colorectal cancer discussed at our multidisciplinary
   team meeting. The pre- and post-therapeutic multidisciplinary team
   meeting recommendations were assessed for concordance compared with
   ChatGPT-4.Results One hundred consecutive patients with newly diagnosed
   colorectal cancer of all stages were included. In the pretherapeutic
   discussions, complete concordance was observed in 72.5%, with partial
   concordance in 10.2% and discordance in 17.3%. For post-therapeutic
   discussions, the concordance increased to 82.8%; 11.8% of decisions
   displayed partial concordance and 5.4% demonstrated discordance.
   Discordance was more frequent in patients older than 77 years and with
   an American Society of Anesthesiologists classification >=
   III.Conclusion There is substantial concordance between the
   recommendations generated by ChatGPT and those provided by traditional
   multidisciplinary team meetings, indicating the potential utility of
   artificial intelligence in supporting clinical decision-making for
   colorectal cancer management.
   This study assessed the concordance between recommendations by a
   multidisciplinary tumour board and ChatGPT-4 for colorectal cancer
   management. A retrospective analysis of 100 cases showed substantial
   agreement, with concordance rates of 72.5% pretherapeutically and 82.8%
   posttherapeutically. These findings highlight the potential role of
   artificial intelligence in supporting and complementing traditional
   multidisciplinary team decision-making processes.
ZB 0
TC 0
ZR 0
ZS 0
ZA 0
Z8 0
Z9 0
DA 2025-05-13
UT WOS:001484582700001
PM 40331891
ER

PT J
AU Rahman, Md Mushfiqur
   Irbaz, Mohammad Sabik
   North, Kai
   Williams, Michelle S.
   Zampieri, Marcos
   Lybarger, Kevin
TI Health text simplification: An annotated corpus for digestive cancer
   education and novel strategies for reinforcement learning
SO JOURNAL OF BIOMEDICAL INFORMATICS
VL 158
AR 104727
DI 10.1016/j.jbi.2024.104727
EA SEP 2024
DT Article
PD OCT 2024
PY 2024
AB Objective: The reading level of health educational materials
   significantly influences the understandability and accessibility of the
   information, particularly for minoritized populations. Many patient
   educational resources surpass widely accepted standards for reading
   level and complexity. There is a critical need for high-performing text
   simplification models for health information to enhance dissemination
   and literacy. This need is particularly acute in cancer education, where
   effective prevention and screening education can substantially reduce
   morbidity and mortality. Methods: We introduce Simplified Digestive
   Cancer (SimpleDC), a parallel corpus of cancer education materials
   tailored for health text simplification research, comprising educational
   content from the American Cancer Society, Centers for Disease Control
   and Prevention, and National Cancer Institute. The corpus includes 31
   web pages with the corresponding manually simplified versions. It
   consists of 1183 annotated sentence pairs (361 train, 294 development,
   and 528 test). Utilizing SimpleDC and the existing Med-EASi corpus, we
   explore Large Language Model (LLM)-based simplification methods,
   including fine-tuning, reinforcement learning (RL), reinforcement
   learning with human feedback (RLHF), domain adaptation, and prompt-based
   approaches. Our experimentation encompasses Llama 2, Llama 3, and GPT-4.
   We introduce a novel RLHF reward function featuring a lightweight model
   adept at distinguishing between original and simplified texts when
   enables training on unlabeled data. Results: Fine-tuned Llama models
   demonstrated high performance across various metrics. Our RLHF reward
   function outperformed existing RL text simplification reward functions.
   The results underscore that RL/RLHF can achieve performance comparable
   to fine-tuning and improve the performance of fine-tuned models.
   Additionally, these methods effectively adapt out-of-domain text
   simplification models to a target domain. The best-performing
   RL-enhanced Llama models outperformed GPT-4 in both automatic metrics
   and manual evaluation by subject matter experts. Conclusion: The newly
   developed SimpleDC corpus will serve as a valuable asset to the research
   community, particularly in patient education simplification. The RL/RLHF
   methodologies presented herein enable effective training of
   simplification models on unlabeled text and the utilization of
   out-of-domain simplification corpora.
ZR 0
Z8 0
TC 2
ZS 0
ZB 0
ZA 0
Z9 2
DA 2024-09-29
UT WOS:001318940100001
PM 39293643
ER

PT J
AU Shah, Syed Jawad Hussain
   Albishri, Ahmed
   Wang, Rong
   Lee, Yugyung
TI Integrating local and global attention mechanisms for enhanced oral
   cancer detection and explainability.
SO Computers in biology and medicine
VL 189
BP 109841
EP 109841
DI 10.1016/j.compbiomed.2025.109841
DT Journal Article
PD 2025-May
PY 2025
AB BACKGROUND AND OBJECTIVE: Early detection of Oral Squamous Cell
   Carcinoma (OSCC) improves survival rates, but traditional diagnostic
   methods often produce inconsistent results. This study introduces the
   Oral Cancer Attention Network (OCANet), a U-Net-based architecture
   designed to enhance tumor segmentation in hematoxylin and eosin
   (H&E)-stained images. By integrating local and global attention
   mechanisms, OCANet captures complex cancerous patterns that existing
   deep-learning models may overlook. A Large Language Model (LLM) analyzes
   feature maps and Grad-CAM visualizations to improve interpretability,
   providing insights into the model's decision-making process.
   METHODS: OCANet incorporates the Channel and Spatial Attention Fusion
   (CSAF) module, Squeeze-and-Excitation (SE) blocks, Atrous Spatial
   Pyramid Pooling (ASPP), and residual connections to refine feature
   extraction and segmentation. The model was evaluated on the Oral
   Cavity-Derived Cancer (OCDC) and Oral Cancer Annotated (ORCA) datasets
   and the DigestPath colon tumor dataset to assess generalizability.
   Performance was measured using accuracy, Dice Similarity Coefficient
   (DSC), and mean Intersection over Union (mIoU), focusing on
   class-specific segmentation performance.
   RESULTS: OCANet outperformed state-of-the-art models across all
   datasets. On ORCA, it achieved 90.98% accuracy, 86.14% DSC, and 77.10%
   mIoU. On OCDC, it reached 98.24% accuracy, 94.09% DSC, and 88.84% mIoU.
   On DigestPath, it demonstrated strong generalization with 84.65% DSC
   despite limited training data. The model showed superior carcinoma
   detection performance, distinguishing cancerous from non-cancerous
   regions with high specificity.
   CONCLUSION: OCANet enhances tumor segmentation accuracy and
   interpretability in histopathological images by integrating advanced
   attention mechanisms. Combining visual and textual insights, its
   multimodal explainability framework improves transparency while
   supporting clinical decision-making. With strong generalization across
   datasets and computational efficiency, OCANet presents a promising tool
   for oral and other cancer diagnostics, particularly in resource-limited
   settings.
TC 0
ZA 0
Z8 0
ZR 0
ZB 0
ZS 0
Z9 0
DA 2025-03-11
UT MEDLINE:40056841
PM 40056841
ER

PT J
AU Zhou, Juexiao
   He, Xiaonan
   Sun, Liyuan
   Xu, Jiannan
   Chen, Xiuying
   Chu, Yuetan
   Zhou, Longxi
   Liao, Xingyu
   Zhang, Bin
   Afvari, Shawn
   Gao, Xin
TI Pre-trained multimodal large language model enhances dermatological
   diagnosis using SkinGPT-4
SO NATURE COMMUNICATIONS
VL 15
IS 1
AR 5649
DI 10.1038/s41467-024-50043-3
DT Article
PD JUL 5 2024
PY 2024
AB Large language models (LLMs) are seen to have tremendous potential in
   advancing medical diagnosis recently, particularly in dermatological
   diagnosis, which is a very important task as skin and subcutaneous
   diseases rank high among the leading contributors to the global burden
   of nonfatal diseases. Here we present SkinGPT-4, which is an interactive
   dermatology diagnostic system based on multimodal large language models.
   We have aligned a pre-trained vision transformer with an LLM named
   Llama-2-13b-chat by collecting an extensive collection of skin disease
   images (comprising 52,929 publicly available and proprietary images)
   along with clinical concepts and doctors' notes, and designing a
   two-step training strategy. We have quantitatively evaluated SkinGPT-4
   on 150 real-life cases with board-certified dermatologists. With
   SkinGPT-4, users could upload their own skin photos for diagnosis, and
   the system could autonomously evaluate the images, identify the
   characteristics and categories of the skin conditions, perform in-depth
   analysis, and provide interactive treatment recommendations.
   Here, authors develop SkinGPT-4, an interactive dermatology diagnostic
   system that uses multimodal large language models and aligns a vision
   transformer with Llama-2-13b-chat. Evaluated by dermatologists, it
   offers autonomous diagnosis and treatment recommendations.
ZA 0
Z8 0
ZB 2
ZR 0
ZS 0
TC 21
Z9 21
DA 2024-07-18
UT WOS:001263353700018
PM 38969632
ER

PT J
AU Barat, Maxime
   Crombe, Amandine
   Boeken, Tom
   Dacher, Jean-Nicolas
   Si-Mohamed, Salim
   Dohan, Anthony
   Chassagnon, Guillaume
   Lecler, Augustin
   Greffier, Joel
   Nougaret, Stephanie
   Soyer, Philippe
TI Imaging in France: 2024 Update
SO CANADIAN ASSOCIATION OF RADIOLOGISTS JOURNAL-JOURNAL DE L ASSOCIATION
   CANADIENNE DES RADIOLOGISTES
VL 76
IS 2
BP 221
EP 231
DI 10.1177/08465371241288425
DT Review
PD MAY 2025
PY 2025
AB Radiology in France has made major advances in recent years through
   innovations in research and clinical practice. French institutions have
   developed innovative imaging techniques and artificial intelligence
   applications in the field of diagnostic imaging and interventional
   radiology. These include, but are not limited to, a more precise
   diagnosis of cancer and other diseases, research in dual-energy and
   photon-counting computed tomography, new applications of artificial
   intelligence, and advanced treatments in the field of interventional
   radiology. This article aims to explore the major research initiatives
   and technological advances that are shaping the landscape of radiology
   in France. By highlighting key contributions in diagnostic imaging,
   artificial intelligence, and interventional radiology, we provide a
   comprehensive overview of how these innovations are improving patient
   outcomes, enhancing diagnostic accuracy, and expanding the possibilities
   for minimally invasive therapies. As the field continues to evolve,
   France's position at the forefront of radiological research ensures that
   these innovations will play a central role in addressing current
   healthcare challenges and improving patient care on a global scale.
   La radiologie en France a r & eacute;alis & eacute; des progr & egrave;s
   majeurs durant ces derni & egrave;res ann & eacute;es gr & acirc;ce &
   agrave; des innovations dans les domaines de la recherche et de la
   clinique. Les institutions m & eacute;dicales fran & ccedil;aises ont d
   & eacute;velopp & eacute; des techniques innovantes et des applications
   d'intelligence artificielle dans le domaine de l'imagerie diagnostique
   et de la radiologie interventionnelle. Celles-ci incluent, de mani &
   egrave;re non exhaustive, un diagnostic plus pr & eacute;cis du cancer
   et d'autres maladies, la recherche fondamentale et clinique en
   tomodensitom & eacute;trie & agrave; double & eacute;nergie et par
   comptage photonique, de nouvelles applications de l'intelligence
   artificielle et de nouveaux traitements dans le domaine de la radiologie
   interventionnelle. Cet article vise & agrave; rapporter les grandes
   initiatives de recherche et les avanc & eacute;es technologiques qui fa
   & ccedil;onnent le paysage de la radiologie en France. En mettant en &
   eacute;vidence les contributions cl & eacute;s en imagerie diagnostique,
   en intelligence artificielle et en radiologie interventionnelle, cet
   article donne un aper & ccedil;u complet de la mani & egrave;re dont ces
   innovations am & eacute;liorent les r & eacute;sultats pour les
   patients, am & eacute;liorent la pr & eacute;cision du diagnostic et &
   eacute;largissent les possibilit & eacute;s de th & eacute;rapies
   mini-invasives. La position de la France & agrave; l'avant-garde de la
   recherche radiologique garantit que ces innovations joueront un r &
   ocirc;le central pour relever les d & eacute;fis actuels en mati &
   egrave;re de sant & eacute; et am & eacute;liorer les soins des patients
   & agrave; l'& eacute;chelle mondiale.
ZS 0
TC 1
ZR 0
ZA 0
ZB 0
Z8 0
Z9 1
DA 2025-03-21
UT WOS:001442832300009
PM 39367786
ER

PT J
AU Mora, J.
   Chen, S.
   Mak, R. H.
   Bitterman, D. S.
TI Cancer Treatment Information Differences by Bilingual Prompting in Large
   Language Model Chatbots
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3415
BP E645
EP E646
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
ZB 0
TC 0
ZA 0
ZR 0
Z8 0
Z9 0
DA 2024-12-16
UT WOS:001325892302096
ER

PT J
AU Scherbakov, Dmitry
   Heider, Paul M.
   Wehbe, Ramsey
   Alekseyenko, Alexander V.
   Lenert, Leslie A.
   Obeid, Jihad S.
TI Using large language models for extracting stressful life events to
   assess their impact on preventive colon cancer screening adherence
SO BMC PUBLIC HEALTH
VL 25
IS 1
AR 12
DI 10.1186/s12889-024-21123-2
DT Article
PD JAN 2 2025
PY 2025
AB BackgroundIncrease in early onset colorectal cancer makes adherence to
   screening a significant public health concern, with various social
   determinants playing a crucial role in its incidence, diagnosis,
   treatment, and outcomes. Stressful life events, such as divorce,
   marriage, or sudden loss of job, have a unique position among the social
   determinants of health.MethodsWe applied a large language model (LLM) to
   social history sections of clinical notes in the health records database
   of the Medical University of South Carolina to extract recent stressful
   life events and assess their impact on colorectal cancer screening
   adherence. We used pattern-matching regular expressions to detect a
   possible signal in social histories and ran LLM four times on each
   social history to achieve self-consistency and then used logistic
   regression to estimate the impact of life events on the probability of
   having a code in health records related to colorectal cancer
   screening.ResultsThe LLM detected 380 patients with one or more
   stressful life events and 5,344 patients with no life events. The events
   with the most negative impact on screening were arrest or incarceration
   (OR 0.26 95% CI 0.06-0.77), becoming homeless (OR 0.18 95% CI
   0.01-0.92), separation from spouse or partner (OR 0.32 95% CI
   0.05-1.18), getting married or starting to live with a partner (OR 0.60
   95% CI 0.19-1.53). Death of somebody close to the patient (excluding
   their spouse) increased the chance of screening (OR 1.21 95% CI
   0.71-2.05). Many of the observed effects did not reach statistical
   significance.ConclusionOur findings suggest that stressful life events
   might have a counterintuitive impact on screening, with some events,
   such as bereavement, were associated with increased screening. Future
   work should be focused on validating the research findings using data
   from other health institutions. In addition, expanding the list of
   stressful life events by including a validated scale of stressful life
   events for patients from historically marginalized groups is warranted.
Z8 0
ZB 0
ZS 0
TC 2
ZA 0
ZR 0
Z9 2
DA 2025-01-11
UT WOS:001389970300003
PM 39748338
ER

PT C
AU Makram, Manal
   Mohammed, Ammar
BE AbdelRaouf, A
   Shorim, N
   Hatem, S
   Kandil, Y
   Bahaa-Eldin, A
TI AI Applications in Medical Reporting and Diagnosis
SO 2024 INTERNATIONAL MOBILE, INTELLIGENT, AND UBIQUITOUS COMPUTING
   CONFERENCE, MIUCC 2024
BP 185
EP 192
DI 10.1109/MIUCC62295.2024.10783552
DT Proceedings Paper
PD 2024
PY 2024
AB The integration of artificial intelligence (AI) in healthcare is
   revolutionizing diagnosis and patient care by improving clinical
   documentation and the management of electronic health records that
   depend on medical image interpretation, increasing accuracy, and
   reducing time. Egypt ranks first in liver disease and second in liver
   cancer mortality worldwide in 2020. Large language models, a subset of
   AI techniques, can assist in disease diagnosis. LLM models with
   multimodal capabilities can classify and describe patient scan images
   and extract information from clinical notes. These models can extract
   vital diagnoses with the support of prompt engineering, as one of these
   models can answer questions, summarize information, and translate
   complex medical terminology into plain language, enabling patients to
   understand their medical reports and diagnoses. There are two primary
   approaches to achieving this. First, fine-tuning can adapt the model to
   medical data, which can be resource-intensive. The second approach,
   pre-trained LLM models can be utilized to leverage pre-trained models to
   perform the necessary tasks, focusing on effectively using prompts to
   guide the model for precise and relevant outputs. This study highlights
   the role of generative AI models by focusing on prompt engineering, and
   how carefully crafting prompts can enhance the effectiveness of LLM
   models in medical applications with high accuracy. It demonstrates this
   through experiments using pre-trained models based on semantic
   similarity with GPT-4o and BioGPT. Implementing a zero-shot model for
   liver tumor classification is one of the prompt engineering techniques.
   The performance metrics achieved were impressive, accuracy, precision,
   recall, and F1-scores are 88, 81, 88, and 83 percent, respectively.
CT 4th International Mobile, Intelligent, and Ubiquitous Computing
   Conference
CY NOV 13-14, 2024
CL Cairo, EGYPT
SP Misr International University
ZS 0
Z8 0
ZB 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2025-03-07
UT WOS:001416363600027
ER

PT J
AU Barabadi, Maede Ashofteh
   Zhu, Xiaodan
   Chan, Wai Yip
   Simpson, Amber L.
   Do, Richard K. G.
TI Targeted generative data augmentation for automatic metastases detection
   from free-text radiology reports
SO FRONTIERS IN ARTIFICIAL INTELLIGENCE
VL 8
AR 1513674
DI 10.3389/frai.2025.1513674
DT Article
PD FEB 6 2025
PY 2025
AB Automatic identification of metastatic sites in cancer patients from
   electronic health records is a challenging yet crucial task with
   significant implications for diagnosis and treatment. In this study, we
   demonstrate how advancements in natural language processing, namely the
   instruction-following capability of recent large language models and
   extensive model pretraining, made it possible to automate metastases
   detection from radiology reports texts with a limited amount of
   gold-labeled data. Specifically, we prompt Llama3, an open-source
   instruction-tuned large language model, to generate synthetic training
   data to expand our limited labeled data and adapt BERT, a small
   pretrained language model, to the task. We further investigate three
   targeted data augmentation techniques which selectively expand the
   original training samples, leading to comparable or superior performance
   compared to vanilla data augmentation, in most cases, while being
   substantially more computationally efficient. In our experiments, data
   augmentation improved the average F1-score by 2.3, 3.5, and 3.9 points
   for lung, liver, and adrenal glands, the organs for which we had access
   to expert-annotated data. This observation suggests that Llama3, which
   has not been specifically tailored to this task or clinical data in
   general, can generate high-quality synthetic data through paraphrasing
   in the clinical context. We also compare metastasis identification
   accuracy between models utilizing institutionally standardized reports
   vs. non-structured reports, which complicate the extraction of relevant
   information, and show how including patient history with a customized
   model architecture narrows the gap between those two setups from 7.3 to
   4.5 points on F1-score under LoRA tuning. Our work delivers a broadly
   applicable solution with remarkable performance that does not require
   model customization for each institution, making large-scale, low-cost
   spatio-temporal cancer progression pattern extraction possible.
ZR 0
Z8 0
TC 0
ZS 0
ZB 0
ZA 0
Z9 0
DA 2025-02-24
UT WOS:001425597700001
PM 39981192
ER

PT J
AU Li, Ya
   Zheng, Xuecong
   Li, Jiaping
   Dai, Qingyun
   Wang, Chang-Dong
   Chen, Min
TI LKAN: LLM-Based Knowledge-Aware Attention Network for Clinical Staging
   of Liver Cancer
SO IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS
VL 29
IS 4
BP 3007
EP 3020
DI 10.1109/JBHI.2024.3478809
DT Article
PD APR 2025
PY 2025
AB Clinical staging of liver cancer (CSoLC), an important indicator for
   evaluating primary liver cancer (PLC), is key in the diagnosis,
   treatment, and rehabilitation of liver cancer. In China, the current
   CSoLC adopts the China liver cancer (CNLC) staging, which is usually
   evaluated by clinicians based on radiology reports. Therefore, inferring
   clinical information from unstructured radiology reports can provide
   auxiliary decision support for clinicians. The key to solving the
   challenging task is to guide the model to pay attention to the
   staging-related words or sentences, and the following issues may occur:
   1) Imbalanced categories: Early- and mid-stage liver cancer symptoms are
   subtle, resulting in more data in the end-stage. 2) Domain sensitivity
   of liver cancer data: The liver cancer dataset contains substantial
   domain knowledge, leading to out-of-vocabulary issues and reduced
   classification accuracy. 3) Free-text and lengthy report: Radiology
   reports sparsely describe various lesions using domain-specific terms,
   making it hard to mine staging-related information. To address these,
   this article proposes a large language model (LLM)-based Knowledge-aware
   Attention Network (LKAN) for CSoLC. First, for maintaining semantic
   consistency, LLM and a rule-based algorithm are integrated to generate
   more diverse and reasonable data. Second, an unlabeled radiology corpus
   is pre-trained to introduce domain knowledge for subsequent
   representation learning. Third, attention is improved by incorporating
   both global and local features to guide the model's focus on
   staging-relevant information. Compared with the baseline models, LKAN
   has achieved the best results with 90.3% Accuracy, 90.0% Macro_F1 score,
   and 90.0% Macro_Recall.
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
TC 1
Z9 1
DA 2025-04-19
UT WOS:001459663700029
PM 39392729
ER

PT J
AU Mao, Yuqiang
   Xu, Nan
   Wu, Yanan
   Wang, Lu
   Wang, Hongtao
   He, Qianqian
   Zhao, Tianqi
   Ma, Shuangchun
   Zhou, Meihong
   Jin, Hongjie
   Pei, Dongmei
   Zhang, Lina
   Song, Jiangdian
TI Assessments of lung nodules by an artificial intelligence chatbot using
   longitudinal CT images
SO CELL REPORTS MEDICINE
VL 6
IS 3
AR 101988
DI 10.1016/j.xcrm.2025.101988
EA MAR 2025
DT Article
PD MAR 18 2025
PY 2025
AB Large language models have shown efficacy across multiple medical tasks.
   However, their value in the assessment of longitudinal follow-up
   computed tomography (CT) images of patients with lung nodules is
   unclear. In this study, we evaluate the ability of the latest generative
   pre-trained transformer (GPT)-4o model to assess changes in malignancy
   probability, size, and features of lung nodules on longitudinal CT scans
   from 647 patients (547 from two local centers and 100 from a public
   dataset). GPT-4o achieves an average accuracy of 0.88 in predicting lung
   nodule malignancy compared to pathological results and an average
   intraclass correlation coefficient of 0.91 in measuring nodule size
   compared with manual measurements by radiologists. Six radiologists'
   evaluations demonstrate GPT-4o's ability to capture changes in nodule
   features with a median Likert score of 4.17 (out of 5.00). In summary,
   GPT-4o could capture dynamic changes in lung nodules across longitudinal
   follow-up CT images, thus providing high-quality radiological evidence
   to assist in clinical management.
Z8 0
TC 2
ZR 0
ZS 0
ZB 0
ZA 0
Z9 2
DA 2025-03-29
UT WOS:001450322100001
PM 40043704
ER

PT J
AU Proctor, Erin S.
   Nusbaum, David J.
   Lee, John M.
   Benirschke, Robert C.
   Freedman, Alexa
   Raster, Gregory
   Glaser, Alexander P.
   Labbate, Craig, V
   Higgins, Andrew M.
   Helfand, Brian T.
   Glassy, Eric F.
   Joseph, Lija
   Edelstein, Robert A.
   Krupinski, Elizabeth A.
   Alnajar, Hussein
   Kearns, James T.
   Groth, John, V
TI Bridging the gap: Evaluating ChatGPT-generated, personalized,
   patient-centered prostate biopsy reports
SO AMERICAN JOURNAL OF CLINICAL PATHOLOGY
VL 163
IS 5
BP 766
EP 774
DI 10.1093/ajcp/aqae185
EA JAN 2025
DT Article
PD MAY 2025
PY 2025
AB Objective The highly specialized language used in prostate biopsy
   pathology reports coupled with low rates of health literacy leave some
   patients unable to comprehend their medical information. Patients' use
   of online search engines can lead to misinterpretation of results and
   emotional distress. Artificial intelligence (AI) tools such as ChatGPT
   (OpenAI) could simplify complex texts and help patients. This study
   evaluates patient-centered prostate biopsy reports generated by
   ChatGPT.Methods Thirty-five self-generated prostate biopsy reports were
   synthesized using National Comprehensive Cancer Network guidelines. Each
   report was entered into ChatGPT, version 4, with the same instructions,
   and the explanations were evaluated by 5 urologists and 5
   pathologists.Results Respondents rated the AI-generated reports as
   mostly accurate and complete. All but 1 report was rated complete and
   grammatically correct by the majority of physicians. Pathologists did
   not rate any reports as having severe potential for harm, but 1 or more
   urologists rated severe concern in 20% of the reports. For 80% of the
   reports, all 5 pathologists felt comfortable sharing them with a patient
   or another clinician, but all 5 urologists reached the same consensus
   for only 40% of reports. Although every report required edits, all
   physicians agreed that they could modify the ChatGPT report faster than
   they could write an original report.Conclusions ChatGPT can save
   physicians substantial time by generating patient-centered reports
   appropriate for patient and physician audiences with low potential to
   cause harm. Surveyed physicians have confidence in the overall utility
   of ChatGPT, supporting further investigation of how AI could be
   integrated into physicians' workflows.
ZS 0
TC 0
ZB 0
ZA 0
ZR 0
Z8 0
Z9 0
DA 2025-01-27
UT WOS:001401240600001
PM 39838829
ER

PT J
AU Ra, Sinyoung
   Kim, Jonghun
   Na, Inye
   Ko, Eun Sook
   Park, Hyunjin
TI Enhancing radiomics features via a large language model for classifying
   benign and malignant breast tumors in mammography
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
VL 265
AR 108765
DI 10.1016/j.cmpb.2025.108765
EA APR 2025
DT Article
PD JUN 2025
PY 2025
AB Background and Objectives: Radiomics is widely used to assist in
   clinical decision-making, disease diagnosis, and treatment planning for
   various target organs, including the breast. Recent advances in large
   language models (LLMs) have helped enhance radiomics analysis. Materials
   and Methods: Herein, we sought to improve radiomics analysis by
   incorporating LLM-learned clinical knowledge, to classify benign and
   malignant tumors in breast mammography. We extracted radiomics features
   from the mammograms based on the region of interest and retained the
   features related to the target task. Using prompt engineering, we
   devised an input sequence that reflected the selected features and the
   target task. The input sequence was fed to the chosen LLM (LLaMA
   variant), which was fine-tuned using low-rank adaptation to enhance
   radiomics features. This was then evaluated on two mammogram datasets
   (VinDr-Mammo and INbreast) against conventional baselines. Results: The
   enhanced radiomics-based method performed better than baselines using
   conventional radiomics features tested on two mammogram datasets,
   achieving accuracies of 0.671 for the VinDr-Mammo dataset and 0.839 for
   the INbreast dataset. Conventional radiomics models require retraining
   from scratch for an unseen dataset using a new set of features. In
   contrast, the model developed in this study effectively reused the
   common features between the training and unseen datasets by explicitly
   linking feature names with feature values, leading to extensible
   learning across datasets. Our method performed better than the baseline
   method in this retraining setting using an unseen dataset. Conclusions:
   Our method, one of the first to incorporate LLM into radiomics, has the
   potential to improve radiomics analysis.
ZS 0
Z8 0
ZR 0
TC 0
ZA 0
ZB 0
Z9 0
DA 2025-04-21
UT WOS:001466026900001
PM 40203779
ER

PT J
AU Cho, Hyeongmin
   Yoo, Sooyoung
   Kim, Borham
   Jang, Sowon
   Sunwoo, Leonard
   Kim, Sanghwan
   Lee, Donghyoung
   Kim, Seok
   Nam, Sejin
   Chung, Jin-Haeng
TI Extracting lung cancer staging descriptors from pathology reports: A
   generative language model approach
SO JOURNAL OF BIOMEDICAL INFORMATICS
VL 157
AR 104720
DI 10.1016/j.jbi.2024.104720
EA SEP 2024
DT Article
PD SEP 2024
PY 2024
AB Background: In oncology, electronic health records contain textual key
   information for the diagnosis, staging, and treatment planning of
   patients with cancer. However, text data processing requires a lot of
   time and effort, which limits the utilization of these data. Recent
   advances in natural language processing (NLP) technology, including
   large language models, can be applied to cancer research. Particularly,
   extracting the information required for the pathological stage from
   surgical pathology reports can be utilized to update cancer staging
   according to the latest cancer staging guidelines. Objectives: This
   study has two main objectives. The first objective is to evaluate the
   performance of extracting information from text-based surgical pathology
   reports and determining pathological stages based on the extracted
   information using fine-tuned generative language models (GLMs) for
   patients with lung cancer. The second objective is to determine the
   feasibility of utilizing relatively small GLMs for information
   extraction in a resource-constrained computing environment. Methods:
   Lung cancer surgical pathology reports were collected from the Common
   Data Model database of Seoul National University Bundang Hospital
   (SNUBH), a tertiary hospital in Korea. We selected 42 descriptors
   necessary for tumor-node (TN) classification based on these reports and
   created a gold standard with validation by two clinical experts. The
   pathology reports and gold standard were used to generate
   prompt-response pairs for training and evaluating GLMs which then were
   used to extract information required for staging from pathology reports.
   Results: We evaluated the information extraction performance of six
   trained models as well as their performance in TN classification using
   the extracted information. The Deductive Mistral-7B model, which was
   pre-trained with the deductive dataset, showed the best performance
   overall, with an exact match ratio of 92.24% in the information
   extraction problem and an accuracy of 0.9876 (predicting T and N
   classification concurrently) in classification. Conclusion: This study
   demonstrated that training GLMs with deductive datasets can improve
   information extraction performance, and GLMs with a relatively small
   number of parameters at approximately seven billion can achieve high
   performance in this problem. The proposed GLM-based information
   extraction method is expected to be useful in clinical decision-making
   support, lung cancer staging and research.
ZS 0
ZA 0
ZR 0
TC 4
ZB 1
Z8 0
Z9 4
DA 2024-09-21
UT WOS:001312772300001
PM 39233209
ER

PT J
AU Bellamkonda, Nikhil
   Farlow, Janice L.
   Haring, Catherine T.
   Sim, Michael W.
   Seim, Nolan B.
   Cannon, Richard B.
   Monroe, Marcus M.
   Agrawal, Amit
   Rocco, James W.
   McCrary, Hilary C.
TI Evaluating the Accuracy of ChatGPT in Common Patient Questions Regarding
   HPV plus Oropharyngeal Carcinoma
SO ANNALS OF OTOLOGY RHINOLOGY AND LARYNGOLOGY
VL 133
IS 9
BP 814
EP 819
DI 10.1177/00034894241259137
EA JUL 2024
DT Article
PD SEP 2024
PY 2024
AB Objectives: Large language model (LLM)-based chatbots such as ChatGPT
   have been publicly available and increasingly utilized by the general
   public since late 2022. This study sought to investigate ChatGPT
   responses to common patient questions regarding Human Papilloma Virus
   (HPV) positive oropharyngeal cancer (OPC). Methods: This was a
   prospective, multi-institutional study, with data collected from high
   volume institutions that perform >50 transoral robotic surgery cases per
   year. The 100 most recent discussion threads including the term "HPV" on
   the American Cancer Society's Cancer Survivors Network's Head and Neck
   Cancer public discussion board were reviewed. The 11 most common
   questions were serially queried to ChatGPT 3.5; answers were recorded. A
   survey was distributed to fellowship trained head and neck oncologic
   surgeons at 3 institutions to evaluate the responses. Results: A total
   of 8 surgeons participated in the study. For questions regarding HPV
   contraction and transmission, ChatGPT answers were scored as clinically
   accurate and aligned with consensus in the head and neck surgical
   oncology community 84.4% and 90.6% of the time, respectively. For
   questions involving treatment of HPV+ OPC, ChatGPT was clinically
   accurate and aligned with consensus 87.5% and 91.7% of the time,
   respectively. For questions regarding the HPV vaccine, ChatGPT was
   clinically accurate and aligned with consensus 62.5% and 75% of the
   time, respectively. When asked about circulating tumor DNA testing, only
   12.5% of surgeons thought responses were accurate or consistent with
   consensus. Conclusion: ChatGPT 3.5 performed poorly with questions
   involving evolving therapies and diagnostics-thus, caution should be
   used when using a platform like ChatGPT 3.5 to assess use of advanced
   technology. Patients should be counseled on the importance of consulting
   their surgeons to receive accurate and up to date recommendations, and
   use LLM's to augment their understanding of these important
   health-related topics.
ZB 0
Z8 0
ZR 0
ZA 0
TC 1
ZS 0
Z9 1
DA 2024-08-06
UT WOS:001280661600001
PM 39075853
ER

PT J
AU Hu, Danqing
   Liu, Bing
   Zhu, Xiaofeng
   Lu, Xudong
   Wu, Nan
TI Zero-shot information extraction from radiological reports using ChatGPT
SO INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
VL 183
AR 105321
DI 10.1016/j.ijmedinf.2023.105321
EA DEC 2023
DT Article
PD MAR 2024
PY 2024
AB Introduction: Electronic health records contain an enormous amount of
   valuable information recorded in free text. Information extraction is
   the strategy to transform free text into structured data, but some of
   its components require annotated data to tune, which has become a
   bottleneck. Large language models achieve good performances on various
   downstream NLP tasks without parameter tuning, becoming a possible way
   to extract information in a zero-shot manner. Methods: In this study, we
   aim to explore whether the most popular large language model, ChatGPT,
   can extract information from the radiological reports. We first design
   the prompt template for the interested information in the CT reports.
   Then, we generate the prompts by combining the prompt template with the
   CT reports as the inputs of ChatGPT to obtain the responses. A
   post-processing module is developed to transform the responses into
   structured extraction results. Besides, we add prior medical knowledge
   to the prompt template to reduce wrong extraction results. We also
   explore the consistency of the extraction results. Results: We conducted
   the experiments with 847 real CT reports. The experimental results
   indicate that ChatGPT can achieve competitive performances for some
   extraction tasks like tumor location, tumor long and short diameters
   compared with the baseline information extraction system. By adding some
   prior medical knowledge to the prompt template, extraction tasks about
   tumor spiculations and lobulations obtain significant improvements but
   tasks about tumor density and lymph node status do not achieve better
   performances. Conclusion: ChatGPT can achieve competitive information
   extraction for radiological reports in a zero-shot manner. Adding prior
   medical knowledge as instructions can further improve performances for
   some extraction tasks but may lead to worse performances for some
   complex extraction tasks.
ZA 0
TC 29
ZR 0
ZB 4
Z8 1
ZS 0
Z9 30
DA 2024-03-04
UT WOS:001165970200001
PM 38157785
ER

PT J
AU Sezgin, Emre
   Jackson, Daniel I.
   Kocaballi, A. Baki
   Bibart, Mindy
   Zupanec, Sue
   Landier, Wendy
   Audino, Anthony
   Ranalli, Mark
   Skeens, Micah
TI Can Large Language Models Aid Caregivers of Pediatric Cancer Patients in
   Information Seeking? A Cross-Sectional Investigation
SO CANCER MEDICINE
VL 14
IS 1
AR e70554
DI 10.1002/cam4.70554
DT Article
PD JAN 2025
PY 2025
AB PurposeCaregivers in pediatric oncology need accurate and understandable
   information about their child's condition, treatment, and side effects.
   This study assesses the performance of publicly accessible large
   language model (LLM)-supported tools in providing valuable and reliable
   information to caregivers of children with cancer. MethodsIn this
   cross-sectional study, we evaluated the performance of the four
   LLM-supported tools-ChatGPT (GPT-4), Google Bard (Gemini Pro), Microsoft
   Bing Chat, and Google SGE-against a set of frequently asked questions
   (FAQs) derived from the Children's Oncology Group Family Handbook and
   expert input (In total, 26 FAQs and 104 generated responses). Five
   pediatric oncology experts assessed the generated LLM responses using
   measures including accuracy, clarity, inclusivity, completeness,
   clinical utility, and overall rating. Additionally, the content quality
   was evaluated including readability, AI disclosure, source credibility,
   resource matching, and content originality. We used descriptive analysis
   and statistical tests including Shapiro-Wilk, Levene's, Kruskal-Wallis
   H-tests, and Dunn's post hoc tests for pairwise comparisons.
   ResultsChatGPT shows high overall performance when evaluated by the
   experts. Bard also performed well, especially in accuracy and clarity of
   the responses, whereas Bing Chat and Google SGE had lower overall
   scores. Regarding the disclosure of responses being generated by AI, it
   was observed less frequently in ChatGPT responses, which may have
   affected the clarity of responses, whereas Bard maintained a balance
   between AI disclosure and response clarity. Google SGE generated the
   most readable responses whereas ChatGPT answered with the most
   complexity. LLM tools varied significantly (p < 0.001) across all expert
   evaluations except inclusivity. Through our thematic analysis of expert
   free-text comments, emotional tone and empathy emerged as a unique theme
   with mixed feedback on expectations from AI to be empathetic.
   ConclusionLLM-supported tools can enhance caregivers' knowledge of
   pediatric oncology. Each model has unique strengths and areas for
   improvement, indicating the need for careful selection based on specific
   clinical contexts. Further research is required to explore their
   application in other medical specialties and patient demographics,
   assessing broader applicability and long-term impacts.
ZA 0
TC 2
Z8 0
ZS 0
ZB 1
ZR 0
Z9 2
DA 2025-01-13
UT WOS:001391811100001
PM 39776222
ER

PT J
AU Khanmohammadi, R.
   Ghanem, A. I.
   Verdecchia, K.
   Hall, R.
   Elshaikh, M. A.
   Movsas, B.
   Bagher-Ebadian, H.
   Chetty, I. J.
   Ghassemi, M. M.
   Thind, K.
TI A Novel Localized Student-Teacher LLM for Enhanced Toxicity Extraction
   in Radiation Oncology
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3388
BP E632
EP E633
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
Z8 0
TC 0
ZB 0
ZR 0
ZA 0
ZS 0
Z9 0
DA 2024-12-16
UT WOS:001325892302069
ER

PT J
AU Chiarelli, Giuseppe
   Stephens, Alex
   Finati, Marco
   Cirulli, Giuseppe Ottone
   Beatrici, Edoardo
   Filipas, Dejan K.
   Arora, Sohrab
   Tinsley, Shane
   Bhandari, Mahendra
   Carrieri, Giuseppe
   Trinh, Quoc-Dien
   Briganti, Alberto
   Montorsi, Francesco
   Lughezzani, Giovanni
   Buffi, Nicolo
   Rogers, Craig
   Abdollah, Firas
TI Adequacy of prostate cancer prevention and screening recommendations
   provided by an artificial intelligence-powered large language model
SO INTERNATIONAL UROLOGY AND NEPHROLOGY
VL 56
IS 8
BP 2589
EP 2595
DI 10.1007/s11255-024-04009-5
EA APR 2024
DT Article
PD AUG 2024
PY 2024
AB Purpose We aimed to assess the appropriateness of ChatGPT in providing
   answers related to prostate cancer (PCa) screening, comparing GPT-3.5
   and GPT-4. Methods A committee of five reviewers designed 30 questions
   related to PCa screening, categorized into three difficulty levels. The
   questions were formulated identically for both GPTs three times, varying
   the prompts. Each reviewer assigned a score for accuracy, clarity, and
   conciseness. The readability was assessed by the Flesch Kincaid Grade
   (FKG) and Flesch Reading Ease (FRE). The mean scores were extracted and
   compared using the Wilcoxon test. We compared the readability across the
   three different prompts by ANOVA. Results In GPT-3.5 the mean score (SD)
   for accuracy, clarity, and conciseness was 1.5 (0.59), 1.7 (0.45), 1.7
   (0.49), respectively for easy questions; 1.3 (0.67), 1.6 (0.69), 1.3
   (0.65) for medium; 1.3 (0.62), 1.6 (0.56), 1.4 (0.56) for hard. In GPT-4
   was 2.0 (0), 2.0 (0), 2.0 (0.14), respectively for easy questions; 1.7
   (0.66), 1.8 (0.61), 1.7 (0.64) for medium; 2.0 (0.24), 1.8 (0.37), 1.9
   (0.27) for hard. GPT-4 performed better for all three qualities and
   difficulty levels than GPT-3.5. The FKG mean for GPT-3.5 and GPT-4
   answers were 12.8 (1.75) and 10.8 (1.72), respectively; the FRE for
   GPT-3.5 and GPT-4 was 37.3 (9.65) and 47.6 (9.88), respectively. The 2nd
   prompt has achieved better results in terms of clarity (all p < 0.05).
   Conclusions GPT-4 displayed superior accuracy, clarity, conciseness, and
   readability than GPT-3.5. Though prompts influenced the quality response
   in both GPTs, their impact was significant only for clarity.
ZR 0
Z8 0
ZS 0
ZA 0
ZB 0
TC 5
Z9 5
DA 2024-04-12
UT WOS:001195665100004
PM 38564079
ER

PT J
AU Huang, Yixing
   Gomaa, Ahmed
   Semrau, Sabine
   Haderlein, Marlen
   Lettmaier, Sebastian
   Weissmann, Thomas
   Grigo, Johanna
   Tkhayat, Hassen Ben
   Frey, Benjamin
   Gaipl, Udo
   Distel, Luitpold
   Maier, Andreas
   Fietkau, Rainer
   Bert, Christoph
   Putz, Florian
TI Benchmarking ChatGPT-4 on a radiation oncology in-training exam and Red
   Journal Gray Zone cases: potentials and challenges for ai-assisted
   medical education and decision making in radiation oncology
SO FRONTIERS IN ONCOLOGY
VL 13
AR 1265024
DI 10.3389/fonc.2023.1265024
DT Article
PD SEP 14 2023
PY 2023
AB PurposeThe potential of large language models in medicine for education
   and decision-making purposes has been demonstrated as they have achieved
   decent scores on medical exams such as the United States Medical
   Licensing Exam (USMLE) and the MedQA exam. This work aims to evaluate
   the performance of ChatGPT-4 in the specialized field of radiation
   oncology.MethodsThe 38th American College of Radiology (ACR) radiation
   oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone
   cases are used to benchmark the performance of ChatGPT-4. The TXIT exam
   contains 300 questions covering various topics of radiation oncology.
   The 2022 Gray Zone collection contains 15 complex clinical
   cases.ResultsFor the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved
   the scores of 62.05% and 78.77%, respectively, highlighting the
   advantage of the latest ChatGPT-4 model. Based on the TXIT exam,
   ChatGPT-4's strong and weak areas in radiation oncology are identified
   to some extent. Specifically, ChatGPT-4 demonstrates better knowledge of
   statistics, CNS & eye, pediatrics, biology, and physics than knowledge
   of bone & soft tissue and gynecology, as per the ACR knowledge domain.
   Regarding clinical care paths, ChatGPT-4 performs better in diagnosis,
   prognosis, and toxicity than brachytherapy and dosimetry. It lacks
   proficiency in in-depth details of clinical trials. For the Gray Zone
   cases, ChatGPT-4 is able to suggest a personalized treatment approach to
   each case with high correctness and comprehensiveness. Importantly, it
   provides novel treatment aspects for many cases, which are not suggested
   by any human experts.ConclusionBoth evaluations demonstrate the
   potential of ChatGPT-4 in medical education for the general public and
   cancer patients, as well as the potential to aid clinical
   decision-making, while acknowledging its limitations in certain domains.
   Owing to the risk of hallucinations, it is essential to verify the
   content generated by models such as ChatGPT for accuracy.
ZA 0
ZR 0
TC 56
ZB 10
Z8 0
ZS 1
Z9 56
DA 2023-12-23
UT WOS:001119288400001
PM 37790756
ER

PT J
AU Chang, Patrick W.
   Amini, Maziar M.
   Davis, Rio O.
   Nguyen, Denis D.
   Dodge, Jennifer L.
   Lee, Helen
   Sheibani, Sarah
   Phan, Jennifer
   Buxbaum, James L.
   Sahakian, Ara B.
TI ChatGPT4 Outperforms Endoscopists for Determination of Postcolonoscopy
   Rescreening and Surveillance Recommendations
SO CLINICAL GASTROENTEROLOGY AND HEPATOLOGY
VL 22
IS 9
BP 1917
EP 1925
DI 10.1016/j.cgh.2024.04.022
EA AUG 2024
DT Article
PD SEP 2024
PY 2024
AB BACKGROUND & AIMS: Large language models including Chat Generative
   Pretrained Transformers version 4 (ChatGPT4) improve access to
   artificial fi cial intelligence, but their impact on the clinical
   practice of gastroenterology is undefined. fi ned. This study compared
   the accuracy, concordance, and reliability of ChatGPT4 colonoscopy
   recommendations for colorectal cancer rescreening and surveillance with
   contemporary guidelines and real-world gastroenterology practice.
   METHODS: History of present illness, colonoscopy data, and pathology
   reports from patients undergoing procedures at 2 large academic centers
   were entered into ChatGPT4 and it was queried for the next recommended
   colonoscopy follow-up interval. Using the McNemar test and inter-rater
   reliability, we compared the recommendations made by ChatGPT4 with the
   actual surveillance interval provided in the endoscopist's ' s procedure
   report (gastroenterology practice) and the appropriate US Multisociety
   Task Force (USMSTF) guidance. The latter was generated for each case by
   an expert panel using the clinical information and guideline documents
   as reference. RESULTS: Text input of de-identified fi ed data into
   ChatGPT4 from 505 consecutive patients undergoing colonoscopy between
   January 1 and April 30, 2023, elicited a successful follow-up
   recommendation in 99.2% of the queries. ChatGPT4 recommendations were in
   closer agreement with the USMSTF Panel (85.7%) than gastroenterology
   practice recommendations with the USMSTF Panel (75.4%) (P P < .001). Of
   the 14.3% discordant recommendations between ChatGPT4 and the USMSTF
   Panel, recommendations were for later screening in 26 (5.1%) and for
   earlier screening in 44 (8.7%) cases. The inter-rater reliability was
   good for ChatGPT4 vs USMSTF Panel (Fleiss K , 0.786; 95% CI,
   0.734-0.838; - 0.838; P < .001). CONCLUSIONS: Initial real-world results
   suggest that ChatGPT4 can define fi ne routine colonoscopy screening
   intervals accurately based on verbatim input of clinical data. Large
   language models have potential for clinical applications, but further
   training is needed for broad use.
ZB 1
Z8 0
ZR 0
ZA 0
ZS 0
TC 5
Z9 5
DA 2024-09-02
UT WOS:001300740300001
PM 38729387
ER

PT J
AU Hwang, Eui fin
   Goo, Mo
   Park, Chang Min
TI AI Applications for Thoracic Imaging: Considerations for Best Practice
SO RADIOLOGY
VL 314
IS 2
AR e240650
DI 10.1148/radiol.240650
DT Article
PD FEB 2025
PY 2025
AB Artificial intelligence (AI) technology is rapidly being introduced into
   thoracic radiology practice. Current representative use cases for AI in
   thoracic imaging show cumulative evidence of effectiveness. These
   include AI assistance for reading chest radiographs and low-dose
   (1.5-mSv) chest CT scans for lung cancer screening and triaging
   pulmonary embolism on chest CT scans. Other potential use cases are also
   under investigation, including filtering out normal chest radiographs,
   monitoring reading errors, and automated opportunistic screening of
   nontarget diseases. However, implementing AI tools in daily practice
   requires establishing practical strategies. Practical AI implementation
   will require objective on-site performance evaluation, institutional
   information technology infrastructure integration, and postdeployment
   monitoring. Meanwhile, the remaining challenges of adopting AI
   technology need to be addressed. These challenges include educating
   radiologists and radiology trainees, alleviating liability risk, and
   addressing potential disparities due to the uneven distribution of data
   and AI technology. Finally, next-generation AI technology represented by
   large language models (LLMs), including multimodal models, which can
   interpret both text and images, is expected to innovate the current
   landscape of AI in thoracic radiology practice. These LLMs offer
   opportunities ranging from generating text reports from images to
   explaining examination results to patients. However, these models
   require more research into their feasibility and efficacy.
ZS 0
Z8 0
ZA 0
ZR 0
TC 1
ZB 0
Z9 1
DA 2025-03-09
UT WOS:001434835900015
PM 39998373
ER

PT J
AU Hu, Dingyi
   Jiang, Zhiguo
   Shi, Jun
   Xie, Fengying
   Wu, Kun
   Tang, Kunming
   Cao, Ming
   Huai, Jianguo
   Zheng, Yushan
TI Pathology report generation from whole slide images with knowledge
   retrieval and multi-level regional feature selection
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
VL 263
AR 108677
DI 10.1016/j.cmpb.2025.108677
EA MAR 2025
DT Article
PD MAY 2025
PY 2025
AB Background and objectives: With the development of deep learning
   techniques, the computer-assisted pathology diagnosis plays a crucial
   role in clinical diagnosis. An important task within this field is
   report generation, which provides doctors with text descriptions of
   whole slide images (WSIs). Report generation from WSIs presents
   significant challenges due to the structural complexity and pathological
   diversity of tissues, as well as the large size and high information
   density of WSIs. The objective of this study is to design histopathology
   report generation method that can efficiently generate reports from WSIs
   and is suitable for clinical practice. Methods: In this paper, we
   propose a novel approach for generating pathology reports from WSIs,
   leveraging knowledge retrieval and multi-level regional feature
   selection. To deal with the uneven distribution pathological information
   in WSIs, we introduce a multi-level regional feature encoding network
   and a feature selection module that extracts multi-level region
   representations and filters out region features irrelevant the
   diagnosis, enabling more efficient report generation. Moreover, we
   design a knowledge retrieval module improve the report generation
   performance that can leverage the diagnostic information from historical
   cases. Additionally, we propose an out-of-domain application mode based
   on large language model (LLM). The use of LLM enhances the scalability
   of the generation model and improves its adaptability to data from
   different sources. Results: The proposed method is evaluated on a public
   datasets and one in-house dataset. On the public GastricADC (991 WSIs),
   our method outperforms state-of-the-art text generation methods and
   achieved 0.568 and 0.345 on metric Rouge-L and Bleu-4, respectively. On
   the in-house Gastric-3300 (3309 WSIs), our method achieved significantly
   better performance with Rouge-L of 0.690, which surpassed the
   second-best state-of-the-art method Wcap 6.3%. Conclusions: We present
   an advanced method for pathology report generation from WSIs, addressing
   the key challenges associated with the large size and complex
   pathological structures of these images. In particular, the multi-level
   regional feature selection module effectively captures diagnostically
   significant regions of varying sizes. The knowledge retrieval-based
   decoder leverages historical diagnostic data to enhance report accuracy.
   Our method not only improves the informativeness and relevance of the
   generated pathology reports but also outperforms the state-of-the-art
   techniques.
ZS 0
ZB 0
ZA 0
Z8 0
ZR 0
TC 0
Z9 0
DA 2025-03-12
UT WOS:001437933700001
PM 40023962
ER

PT J
AU Moore, N. S.
   Laird, J. H., Jr.
   Verma, N.
   Hager, T.
   Sritharan, D.
   Lee, V.
   Maresca, R.
   Chadha, S.
   Park, H. S. M.
   Aneja, S.
TI Applying Language Models to Radiology Text for Identifying
   Oligometastatic Non-Small Cell Lung Cancer
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3413
BP E644
EP E644
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
TC 0
ZB 0
Z8 0
ZA 0
ZR 0
Z9 0
DA 2024-12-16
UT WOS:001325892302094
ER

PT J
AU Haider, Syed Ali
   Pressman, Sophia M.
   Borna, Sahar
   Gomez-Cabello, Cesar A.
   Sehgal, Ajai
   Leibovich, Bradley C.
   Forte, Antonio Jorge
TI Evaluating Large Language Model (LLM) Performance on Established Breast
   Classification Systems
SO DIAGNOSTICS
VL 14
IS 14
AR 1491
DI 10.3390/diagnostics14141491
DT Article
PD JUL 2024
PY 2024
AB Medical researchers are increasingly utilizing advanced LLMs like
   ChatGPT-4 and Gemini to enhance diagnostic processes in the medical
   field. This research focuses on their ability to comprehend and apply
   complex medical classification systems for breast conditions, which can
   significantly aid plastic surgeons in making informed decisions for
   diagnosis and treatment, ultimately leading to improved patient
   outcomes. Fifty clinical scenarios were created to evaluate the
   classification accuracy of each LLM across five established
   breast-related classification systems. Scores from 0 to 2 were assigned
   to LLM responses to denote incorrect, partially correct, or completely
   correct classifications. Descriptive statistics were employed to compare
   the performances of ChatGPT-4 and Gemini. Gemini exhibited superior
   overall performance, achieving 98% accuracy compared to ChatGPT-4's 71%.
   While both models performed well in the Baker classification for
   capsular contracture and UTSW classification for gynecomastia, Gemini
   consistently outperformed ChatGPT-4 in other systems, such as the
   Fischer Grade Classification for gender-affirming mastectomy, Kajava
   Classification for ectopic breast tissue, and Regnault Classification
   for breast ptosis. With further development, integrating LLMs into
   plastic surgery practice will likely enhance diagnostic support and
   decision making.
ZA 0
TC 11
ZS 0
Z8 0
ZB 2
ZR 0
Z9 11
DA 2024-08-02
UT WOS:001276540600001
PM 39061628
ER

PT J
AU Kaiser, Philippe
   Yang, Shan
   Bach, Michael
   Breit, Christian
   Mertz, Kirsten
   Stieltjes, Bram
   Ebbing, Jan
   Wetterauer, Christian
   Henkel, Maurice
TI The interaction of structured data using openEHR and large Language
   models for clinical decision support in prostate cancer
SO WORLD JOURNAL OF UROLOGY
VL 43
IS 1
AR 67
DI 10.1007/s00345-024-05423-1
DT Article
PD JAN 13 2025
PY 2025
AB BackgroundMultidisciplinary teams (MDTs) are essential for cancer care
   but are resource-intensive. Decision-making processes within MDTs, while
   critical, contribute to increased healthcare costs due to the need for
   specialist time and coordination. The recent emergence of large language
   models (LLMs) offers the potential to improve the efficiency and
   accuracy of clinical decision-making processes, potentially reducing
   costs associated with traditional MDT models.MethodsWe conducted a
   retrospective study of 171 consecutively treated patients with newly
   diagnosed prostate cancer. Relevant structured clinical data and the
   European Association of Urology (EAU) pocket guidelines were provided to
   two LLMs (chatGPT-4, Claude-3-Opus). LLM treatment recommendations were
   compared to actual treatment recommendations of the MDT meeting
   (MDM).ResultsBoth LLMs demonstrated an overall adherence of 93% with the
   MDT treatment recommendations. Discrepancies between LLM and MDT
   recommendations were observed in 15 cases (9%), primarily due to lack of
   clinical information that could be provided to the LLMs. In 5 cases
   (3%), the LLM recommendations were not in line with EAU guidelines
   despite having access to all relevant information.ConclusionsOur
   findings provide evidence that LLMs can provide accurate treatment
   recommendations for newly diagnosed prostate cancer patients. LLMs have
   the potential to streamline MDT workflows, enabling specialists to focus
   on complex cases and patient-centered discussions.Patient SummaryIn this
   study, we explored the potential of artificial intelligence models
   called large language models (LLMs) to assist in treatment
   decision-making for prostate cancer patients. We found that LLMs, when
   provided with patient information and clinical guidelines, can recommend
   treatments that closely match those made by a team of cancer
   specialists, suggesting that LLMs could help streamline the
   decision-making process and potentially reduce healthcare costs.
ZR 0
Z8 0
TC 0
ZB 0
ZS 0
ZA 0
Z9 0
DA 2025-01-20
UT WOS:001397199400002
PM 39804478
ER

PT J
AU Fu, Sidney W.
   Tang, Cong
   Tan, Xiaohui
   Srivastava, Sudhir
TI Liquid biopsy for early cancer detection: technological revolutions and
   clinical dilemma
SO EXPERT REVIEW OF MOLECULAR DIAGNOSTICS
VL 24
IS 10
BP 937
EP 955
DI 10.1080/14737159.2024.2408744
EA OCT 2024
DT Review
PD OCT 2 2024
PY 2024
AB IntroductionLiquid biopsy is an innovative advancement in oncology,
   offering a noninvasive method for early cancer detection and monitoring
   by analyzing circulating tumor cells, DNA, RNA, and other biomarkers in
   bodily fluids. This technique has the potential to revolutionize
   precision oncology by providing real-time analysis of tumor dynamics,
   enabling early detection, monitoring treatment responses, and tailoring
   personalized therapies based on the molecular profiles of individual
   patients.Areas coveredIn this review, the authors discuss current
   methodologies, technological challenges, and clinical applications of
   liquid biopsy. This includes advancements in detecting minimal residual
   disease, tracking tumor evolution, and combining liquid biopsy with
   other diagnostic modalities for precision oncology. Key areas explored
   are the sensitivity, specificity, and integration of multi-omics, AI,
   ML, and LLM technologies.Expert opinionLiquid biopsy holds great
   potential to revolutionize cancer care through early detection and
   personalized treatment strategies. However, its success depends on
   overcoming technological and clinical hurdles, such as ensuring high
   sensitivity and specificity, interpreting results amidst tumor
   heterogeneity, and making tests accessible and affordable. Continued
   innovation and collaboration are crucial to fully realize the potential
   of liquid biopsy in improving early cancer detection, treatment, and
   monitoring.
ZR 0
ZA 0
Z8 0
ZB 2
TC 6
ZS 0
Z9 6
DA 2024-10-09
UT WOS:001325602700001
PM 39360748
ER

PT J
AU Moore, Christopher L.
   Socrates, Vimig
   Hesami, Mina
   Denkewicz, Ryan P.
   Cavallo, Joe J.
   Venkatesh, Arjun K.
   Taylor, R. Andrew
TI Using natural language processing to identify emergency department
   patients with incidental lung nodules requiring follow-up
SO ACADEMIC EMERGENCY MEDICINE
VL 32
IS 3
BP 274
EP 283
DI 10.1111/acem.15080
EA JAN 2025
DT Article
PD MAR 2025
PY 2025
AB ObjectivesFor emergency department (ED) patients, lung cancer may be
   detected early through incidental lung nodules (ILNs) discovered on
   chest CTs. However, there are significant errors in the communication
   and follow-up of incidental findings on ED imaging, particularly due to
   unstructured radiology reports. Natural language processing (NLP) can
   aid in identifying ILNs requiring follow-up, potentially reducing errors
   from missed follow-up. We sought to develop an open-access, three-step
   NLP pipeline specifically for this purpose.MethodsThis retrospective
   used a cohort of 26,545 chest CTs performed in three EDs from 2014 to
   2021. Randomly selected chest CT reports were annotated by MD raters
   using Prodigy software to develop a stepwise NLP "pipeline" that first
   excluded prior or known malignancy, determined the presence of a lung
   nodule, and then categorized any recommended follow-up. NLP was
   developed using a RoBERTa large language model on the SpaCy platform and
   deployed as open-access software using Docker. After NLP development it
   was applied to 1000 CT reports that were manually reviewed to determine
   accuracy using accepted NLP metrics of precision (positive predictive
   value), recall (sensitivity), and F1 score (which balances precision and
   recall).ResultsPrecision, recall, and F1 score were 0.85, 0.71, and
   0.77, respectively, for malignancy; 0.87, 0.83, and 0.85 for nodule; and
   0.82, 0.90, and 0.85 for follow-up. Overall accuracy for follow-up in
   the absence of malignancy with a nodule present was 93.3%. The overall
   recommended follow-up rate was 12.4%, with 10.1% of patients having
   evidence of known or prior malignancy.ConclusionsWe developed an
   accurate, open-access pipeline to identify ILNs with recommended
   follow-up on ED chest CTs. While the prevalence of recommended follow-up
   is lower than some prior studies, it more accurately reflects the
   prevalence of truly incidental findings without prior or known
   malignancy. Incorporating this tool could reduce errors by improving the
   identification, communication, and tracking of ILNs.
ZS 0
Z8 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 0
DA 2025-01-23
UT WOS:001397630200001
PM 39821298
ER

PT J
AU Sharma, P.
   Yoder, R.
   Shen, X.
   Einck, J. P.
   Rhodes-Stark, K. L.
   Gan, G. N.
   Shiao, J. C.
   Cunningham, D.
   Butler-Xu, Y. S.
   Tejwani, A.
   Chen, R. C.
   Stecklein, S. R.
TI Medical Accuracy of Cancer Radiotherapy-Related ChatGPT Al Outputs in
   English and Spanish
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3436
BP E656
EP E656
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
TC 0
ZA 0
ZR 0
Z8 0
ZB 0
Z9 0
DA 2024-12-16
UT WOS:001325892302117
ER

PT J
AU Yu, Zehao
   Peng, Cheng
   Yang, Xi
   Dang, Chong
   Adekkanattu, Prakash
   Patra, Braja Gopal
   Peng, Yifan
   Pathak, Jyotishman
   Wilson, Debbie L.
   Chang, Ching -Yuan
   Lo-Ciganic, Wei-Hsuan
   George, Thomas J.
   Hogan, William R.
   Guo, Yi
   Bian, Jiang
   Wu, Yonghui
TI Identifying social determinants of health from clinical narratives: A
   study of performance, documentation ratio, and potential bias
SO JOURNAL OF BIOMEDICAL INFORMATICS
VL 153
AR 104642
DI 10.1016/j.jbi.2024.104642
EA APR 2024
DT Article
PD MAY 2024
PY 2024
AB Objective: To develop a natural language processing (NLP) package to
   extract social determinants of health (SDoH) from clinical narratives,
   examine the bias among race and gender groups, test the generalizability
   of extracting SDoH for different disease groups, and examine
   population-level extraction ratio.
   Methods: We developed SDoH corpora using clinical notes identified at
   the University of Florida (UF) Health. We systematically compared 7
   transformer-based large language models (LLMs) and developed an
   open-source package - SODA (i.e., SOcial DeterminAnts) to facilitate
   SDoH extraction from clinical narratives. We examined the performance
   and potential bias of SODA for different race and gender groups, tested
   the generalizability of SODA using two disease domains including cancer
   and opioid use, and explored strategies for improvement. We applied SODA
   to extract 19 categories of SDoH from the breast (n = 7,971), lung (n
   =11,804), and colorectal cancer (n = 6,240) cohorts to assess
   patient-level extraction ratio and examine the differences among race
   and gender groups.
   Results: We developed an SDoH corpus using 629 clinical notes of cancer
   patients with annotations of 13,193 SDoH concepts/attributes from 19
   categories of SDoH, and another cross-disease validation corpus using
   200 notes from opioid use patients with 4,342 SDoH concepts/attributes.
   We compared 7 transformer models and the GatorTron model achieved the
   best mean average strict/lenient F1 scores of 0.9122 and 0.9367 for SDoH
   concept extraction and 0.9584 and 0.9593 for linking attributes to SDoH
   concepts. There is a small performance gap (similar to 4%) between Males
   and Females, but a large performance gap (>16 %) among race groups. The
   performance dropped when we applied the cancer SDoH model to the opioid
   cohort; fine-tuning using a smaller opioid SDoH corpus improved the
   performance. The extraction ratio varied in the three cancer cohorts, in
   which 10 SDoH could be extracted from over 70 % of cancer patients, but
   9 SDoH could be extracted from less than 70 % of cancer patients.
   Individuals from the White and Black groups have a higher extraction
   ratio than other minority race groups.
   Conclusions: Our SODA package achieved good performance in extracting 19
   categories of SDoH from clinical narratives. The SODA package with
   pre-trained transformer models is available at https://github.com/uf-hob
   i-informatics-lab/SODA_Docker.
TC 8
ZB 3
ZS 0
ZA 0
ZR 0
Z8 0
Z9 8
DA 2024-06-05
UT WOS:001230611200001
PM 38621641
ER

PT J
AU Ostrowska, Magdalena
   Kacala, Paulina
   Onolememen, Deborah
   Vaughan-Lane, Katie
   Sisily Joseph, Anitta
   Ostrowski, Adam
   Pietruszewska, Wioletta
   Banaszewski, Jacek
   Wrobel, Maciej J.
TI To trust or not to trust: evaluating the reliability and safety of AI
   responses to laryngeal cancer queries
SO EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY
VL 281
IS 11
BP 6069
EP 6081
DI 10.1007/s00405-024-08643-8
EA APR 2024
DT Article
PD NOV 2024
PY 2024
AB Purpose As online health information-seeking surges, concerns mount over
   the quality and safety of accessible content, potentially leading to
   patient harm through misinformation. On one hand, the emergence of
   Artificial Intelligence (AI) in healthcare could prevent it; on the
   other hand, questions raise regarding the quality and safety of the
   medical information provided. As laryngeal cancer is a prevalent head
   and neck malignancy, this study aims to evaluate the utility and safety
   of three large language models (LLMs) as sources of patient information
   about laryngeal cancer.Methods A cross-sectional study was conducted
   using three LLMs (ChatGPT 3.5, ChatGPT 4.0, and Bard). A questionnaire
   comprising 36 inquiries about laryngeal cancer was categorised into
   diagnosis (11 questions), treatment (9 questions), novelties and
   upcoming treatments (4 questions), controversies (8 questions), and
   sources of information (4 questions). The population of reviewers
   consisted of 3 groups, including ENT specialists, junior physicians, and
   non-medicals, who graded the responses. Each physician evaluated each
   question twice for each model, while non-medicals only once. Everyone
   was blinded to the model type, and the question order was shuffled.
   Outcome evaluations were based on a safety score (1-3) and a Global
   Quality Score (GQS, 1-5). Results were compared between LLMs. The study
   included iterative assessments and statistical validations.Results
   Analysis revealed that ChatGPT 3.5 scored highest in both safety (mean:
   2.70) and GQS (mean: 3.95). ChatGPT 4.0 and Bard had lower safety scores
   of 2.56 and 2.42, respectively, with corresponding quality scores of
   3.65 and 3.38. Inter-rater reliability was consistent, with less than 3%
   discrepancy. About 4.2% of responses fell into the lowest safety
   category (1), particularly in the novelty category. Non-medical
   reviewers' quality assessments correlated moderately (r = 0.67) with
   response length.Conclusions LLMs can be valuable resources for patients
   seeking information on laryngeal cancer. ChatGPT 3.5 provided the most
   reliable and safe responses among the models evaluated.
TC 11
ZS 1
ZB 1
ZR 0
ZA 0
Z8 0
Z9 10
DA 2024-04-27
UT WOS:001207064800002
PM 38652298
ER

PT J
AU Jairath, Neil K
   Pahalyants, Vartan
   Cheraghlou, Shayan
   Maas, Derek
   Lee, Nayoung
   Criscito, Maressa C
   Stevenson, Mary L
   Mehta, Apoorva
   Leibovit-Reiben, Zachary
   Stockard, Alyssa
   Doudican, Nicole
   Mangold, Aaron
   Carucci, John A
TI Retrieval Augmented Generation-Enabled Large Language Model for Risk
   Stratification of Cutaneous Squamous Cell Carcinoma.
SO JAMA dermatology
DI 10.1001/jamadermatol.2025.1614
DT Journal Article
PD 2025-Jun-11
PY 2025
AB Importance: There exists substantial heterogeneity in outcomes within T
   stages for patients with cutaneous squamous cell carcinoma (cSCC).
   Objective: To determine whether a customized generative pretrained
   transformer model, trained on a comprehensive dataset with more than 1
   trillion parameters and equipped with relevant focused context and
   retrieval augmented generation (RAG), could excel in aggregating and
   interpreting vast quantities of data to develop a novel class-based risk
   stratification system that outperforms the current standards.
   Design, Setting, and Participants: To build the RAG knowledge base, a
   systematic review of the literature was conducted that addressed risk
   factors for poor outcomes in cSCC. Using the RAG-enabled generative
   pretrained transformer (GPT) model, we developed a novel class-based
   risk stratification system that assigned point values for risk factors,
   culminating in a GPT-based prognostication system called the artificial
   intelligence-derived risk score (AIRIS). The system's performance was
   validated on a combined prospective and retrospective cohort of 2379
   primary cSCC tumors (1996-2023) with at least 36 months of follow-up,
   against Brigham and Women's Hospital (BWH) and American Joint Committee
   on Cancer Staging Manual, eighth edition (AJCC8) systems in stratifying
   risk for locoregional recurrence (LR), nodal metastasis (NM), distant
   metastasis (DM), and disease-specific death (DSD).
   Main Outcomes and Measures: Performance metrics evaluated included
   distinctiveness, homogeneity, and monotonicity, as defined by the AJCC8,
   as well as sensitivity, specificity, positive predictive value, negative
   predictive value, accuracy, the area under the receiver operating
   characteristic curve, and concordance.
   Results: The median age at diagnosis was 73 (IQR, 64-81) years, with
   38.5% female patients and 61.5% male patients. The AIRIS prognostication
   system demonstrated superior sensitivity across all outcomes (LR, 49.1%;
   NM, 73.7%; DM, 82.5%; and DSD, 72.2%) and the highest area under the
   receiver operating characteristic curve values (LR, 0.69; NM, 0.81; DM,
   0.85; and DSD, 0.80), indicating significantly enhanced discriminative
   capability compared with the BWH and AJCC8 systems. While all systems
   were comparably distinctive, the AIRIS prognostication system
   consistently demonstrated the lowest proportion of tumors exhibiting
   poor outcomes in low-risk categories, suggesting its improved
   homogeneity and monotonicity.
   Conclusions and Relevance: The results of this diagnostic study suggest
   that the AIRIS system outperforms the existing BWH and AJCC8
   prognostication systems, potentially providing a more effective tool for
   predicting poor outcomes in cSCC. This study illustrates the potential
   of large language models in refining prognostic tools, offering
   implications for treating patients with cancer.
ZB 0
ZS 0
ZR 0
ZA 0
Z8 0
TC 0
Z9 0
DA 2025-06-13
UT MEDLINE:40498504
PM 40498504
ER

PT J
AU Zhang, Jingqing
   Sun, Kai
   Jagadeesh, Akshay
   Falakaflaki, Parastoo
   Kayayan, Elena
   Tao, Guanyu
   Ghahfarokhi, Mahta Haghighat
   Gupta, Deepa
   Gupta, Ashok
   Gupta, Vibhor
   Guo, Yike
TI The potential and pitfalls of using a large language model such as
   ChatGPT, GPT-4, or LLaMA as a clinical assistant
SO JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION
VL 31
IS 9
BP 1884
EP 1891
DI 10.1093/jamia/ocae184
EA JUL 2024
DT Article
PD JUL 17 2024
PY 2024
AB Objectives This study aims to evaluate the utility of large language
   models (LLMs) in healthcare, focusing on their applications in enhancing
   patient care through improved diagnostic, decision-making processes, and
   as ancillary tools for healthcare professionals.Materials and Methods We
   evaluated ChatGPT, GPT-4, and LLaMA in identifying patients with
   specific diseases using gold-labeled Electronic Health Records (EHRs)
   from the MIMIC-III database, covering three prevalent diseases-Chronic
   Obstructive Pulmonary Disease (COPD), Chronic Kidney Disease (CKD)-along
   with the rare condition, Primary Biliary Cirrhosis (PBC), and the
   hard-to-diagnose condition Cancer Cachexia.Results In patient
   identification, GPT-4 had near similar or better performance compared to
   the corresponding disease-specific Machine Learning models (F1-score >=
   85%) on COPD, CKD, and PBC. GPT-4 excelled in the PBC use case,
   achieving a 4.23% higher F1-score compared to disease-specific
   "Traditional Machine Learning" models. ChatGPT and LLaMA3 demonstrated
   lower performance than GPT-4 across all diseases and almost all metrics.
   Few-shot prompts also help ChatGPT, GPT-4, and LLaMA3 achieve higher
   precision and specificity but lower sensitivity and Negative Predictive
   Value.Discussion The study highlights the potential and limitations of
   LLMs in healthcare. Issues with errors, explanatory limitations and
   ethical concerns like data privacy and model transparency suggest that
   these models would be in clinical settings. Future studies should
   improve training datasets and model designs for LLMs to gain better
   utility in healthcare.Conclusion The study shows that LLMs have the
   potential to assist clinicians for tasks such as patient identification
   but false positives and false negatives must be mitigated before LLMs
   are adequate for real-world clinical assistance.
ZA 0
ZS 0
ZB 1
ZR 0
TC 15
Z8 1
Z9 16
DA 2024-07-23
UT WOS:001269939400001
PM 39018498
ER

PT J
AU Li, Caixia
   Zhao, Yina
   Bai, Yang
   Zhao, Baoquan
   Tola, Yetunde Oluwafunmilayo
   Chan, Carmen W. H.
   Zhang, Meifen
   Fu, Xia
TI Unveiling the Potential of Large Language Models in Transforming Chronic
   Disease Management: Mixed Methods Systematic Review
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 27
AR e70535
DI 10.2196/70535
DT Review
PD APR 16 2025
PY 2025
AB Background: Chronic diseases are a major global health burden,
   accounting for nearly three-quarters of the deaths worldwide. Large
   language models (LLMs) are advanced artificial intelligence systems with
   transformative potentialto optimize chronic disease management; however,
   robust evidence is lacking. Objective: This review aims to synthesize
   evidence on the feasibility, opportunities, and challenges of LLMs
   across the disease management spectrum, from prevention to screening,
   diagnosis, treatment, and long-term care. Methods: Following the PRISMA
   (Preferred Reporting Items for Systematic Reviews and Meta-Analysis)
   guidelines, 11 databases (Cochrane Central Register of Controlled
   Trials, CINAHL, Embase, IEEE Xplore, MEDLINE via Ovid, ProQuest Health &
   MedicineCollection, ScienceDirect, Scopus, Web of Science Core
   Collection, China National KnowledgeInternet, and SinoMed) were searched
   on April 17, 2024. Intervention and simulation studies that examined
   LLMs in the management of chronic diseases were included. The
   methodological quality of the included studies was evaluated using a
   rating rubric designed for simulation-based research and the risk of
   bias in nonrandomized studies of interventions tool for
   quasi-experimental studies. Narrative analysis with descriptivefigures
   was used to synthesizethe study findings. Random-effects meta-analyses
   were conducted to assess the pooled effect estimates of the feasibility
   of LLMs in chronic disease management. Results: A total of 20 studies
   examined general-purpose (n=17) and retrieval-augmented
   generation-enhanced LLMs (n=3) for the management of chronic diseases,
   including cancer, cardiovascular diseases, and metabolic disorders. LLMs
   demonstrated feasibility across the chronic disease management spectrum
   by generating relevant, comprehensible, and accurate health
   recommendations (pooled accurate rate 71%, 95% CI 0.59-0.83; I2=88.32%)
   with retrieval-augmented generation-enhanced LLMs having higher accuracy
   rates compared to general-purpose LLMs (odds ratio 2.89, 95% CI
   1.83-4.58; I2=54.45%). LLMs facilitated equitable information access;
   increased patient awareness regarding ailments, preventive measures, and
   treatment options; and promoted self-management behaviors in lifestyle
   modification and symptom coping. Additionally, LLMs facilitate
   compassionate emotional support, social connections, and health care
   resources to improve the health outcomesof chronic diseases. However,
   LLMs face challenges in addressing privacy, language, and cultural
   issues; undertaking advanced tasks, including diagnosis, medication, and
   comorbidity management; and generating personalized regimens with
   real-timeadjustments and multiple modalities. Conclusions:LLMs have
   demonstrated the potentialto transform chronic disease management at the
   individual, social, and health care levels; however, their direct
   application in clinical settings is still in its infancy. A multifaceted
   approach that incorporates robust data security, domain-specific model
   fine-tuning, multimodal data integration, and wearables is crucial for
   the evolution of LLMs into invaluable adjuncts for health care
   professionals to transform chronic disease management. Trial
   Registration: PROSPERO CRD42024545412;
   https://www.crd.york.ac.uk/PROSPERO/view/CRD42024545412
ZB 0
Z8 0
ZS 0
ZR 0
ZA 0
TC 0
Z9 0
DA 2025-05-09
UT WOS:001478857800005
PM 40239198
ER

PT J
AU Zarfati, Mor
   Soffer, Shelly
   Nadkarni, Girish N.
   Klang, Eyal
TI Retrieval-Augmented Generation: Advancing personalized care and research
   in oncology
SO EUROPEAN JOURNAL OF CANCER
VL 220
AR 115341
DI 10.1016/j.ejca.2025.115341
EA MAR 2025
DT Article
PD MAY 2 2025
PY 2025
AB Retrieval-Augmented Generation (RAG) pairs large language models (LLMs)
   with recent data to produce more accurate, context-aware outputs. By
   converting text into numeric embeddings, RAG locates and retrieves
   relevant "chunks" of data, that along with the query, ground the model's
   responses in current, specific information. This process helps reduce
   outdated or fabricated answers. In oncology, RAG has shown particular
   promise. Studies have demonstrated its ability to improve treatment
   recommendations by integrating genetic profiles, strengthened clinical
   trial matching through biomarker analysis, and accelerated drug
   development by clarifying modeldriven insights. Despite its advantages,
   RAG depends on high-quality data. Biased or incomplete sources can lead
   to inaccurate outcomes. Careful implementation and human oversight are
   crucial for ensuring the effectiveness and reliability of RAG in
   oncology.
ZS 0
ZB 0
Z8 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2025-03-21
UT WOS:001444559600001
PM 40068371
ER

PT J
AU Rauniyar, Ashish
   Hagos, Desta Haileselassie
   Jha, Debesh
   Hakegard, Jan Erik
   Bagci, Ulas
   Rawat, Danda B.
   Vlassov, Vladimir
TI Federated Learning for Medical Applications: A Taxonomy, Current Trends,
   Challenges, and Future Research Directions
SO IEEE INTERNET OF THINGS JOURNAL
VL 11
IS 5
BP 7374
EP 7398
DI 10.1109/JIOT.2023.3329061
DT Article
PD MAR 1 2024
PY 2024
AB With the advent of the Internet of Things (IoT), artificial intelligence
   (AI), machine learning (ML), and deep learning (DL) algorithms, the
   landscape of data-driven medical applications has emerged as a promising
   avenue for designing robust and scalable diagnostic and prognostic
   models from medical data. This has gained a lot of attention from both
   academia and industry, leading to significant improvements in healthcare
   quality. However, the adoption of AI-driven medical applications still
   faces tough challenges, including meeting security, privacy, and
   Quality-of-Service (QoS) standards. Recent developments in federated
   learning (FL) have made it possible to train complex machine-learned
   models in a distributed manner and have become an active research
   domain, particularly processing the medical data at the edge of the
   network in a decentralized way to preserve privacy and address security
   concerns. To this end, in this article, we explore the present and
   future of FL technology in medical applications where data sharing is a
   significant challenge. We delve into the current research trends and
   their outcomes, unraveling the complexities of designing reliable and
   scalable FL models. This article outlines the fundamental statistical
   issues in FL, tackles device-related problems, addresses security
   challenges, and navigates the complexity of privacy concerns, all while
   highlighting its transformative potential in the medical field. Our
   study primarily focuses on medical applications of FL, particularly in
   the context of global cancer diagnosis. We highlight the potential of FL
   to enable computer-aided diagnosis tools that address this challenge
   with greater effectiveness than traditional data-driven methods. Recent
   literature has shown that FL models are robust and generalize well to
   new data, which is essential for medical applications. We hope that this
   comprehensive review will serve as a checkpoint for the field,
   summarizing the current state of the art and identifying open problems
   and future research directions.
ZR 0
ZA 0
ZB 1
Z8 1
TC 51
ZS 0
Z9 52
DA 2024-06-25
UT WOS:001203463700006
ER

PT J
AU Maghsoudi, Arash
   Zhou, Emily
   Guffey, Danielle
   Ma, Shengling
   Xiao, Xiangjun
   Peng, Bo
   Amos, Christopher I.
   Ouyomi, Abiodun O.
   Razjouyan, Javad
   Li, Ang
TI A Transformer Natural Language Processing Algorithm for Cancer
   Associated Thrombosis Phenotype
SO BLOOD
VL 142
DI 10.1182/blood-2023-184756
EA NOV 2023
SU 1
DT Meeting Abstract
PD NOV 2 2023
PY 2023
CT 65th Annual Meeting of the American-Society-of-Hematology (ASH)
CY DEC 09-12, 2023
CL San Diego, CA
SP Amer Soc Hematol
ZR 0
ZA 0
ZB 0
Z8 0
TC 1
ZS 0
Z9 1
DA 2024-02-29
UT WOS:001159306705028
ER

PT J
AU Bhayana, Rajesh
   Alwahbi, Omar
   Ladak, Aly Muhammad
   Deng, Yangqing
   Dias, Adriano Basso
   Elbanna, Khaled
   Gomez, Jorge Abreu
   Jajodia, Ankush
   Jhaveri, Kartik
   Johnson, Sarah
   Kajal, Dilkash
   Wang, David
   Soong, Christine
   Kielar, Ania
   Krishna, Satheesh
TI Leveraging Large Language Models to Generate Clinical Histories for
   Oncologic Imaging Requisitions
SO RADIOLOGY
VL 314
IS 2
AR e242134
DI 10.1148/radiol.242134
DT Article
PD FEB 2025
PY 2025
AB Background: Clinical information improves imaging interpretation, but
   physician-provided histories on requisitions for oncologic imaging often
   lack key details. Purpose: To evaluate large language models (LLMs) for
   automatically generating clinical histories for oncologic imaging
   requisitions from clinical notes and compare them with original
   requisition histories. Materials and Methods: In total, 207 patients
   with CT performed at a cancer center from January to November 2023 and
   with an electronic health record clinical note coinciding with ordering
   date were randomly selected. A multidisciplinary team informed selection
   of 10 parameters important for oncologic imaging history, including
   primary oncologic diagnosis, treatment history, and acute symptoms.
   Clinical notes were independently reviewed to establish the reference
   standard regarding presence of each parameter. After prompt engineering
   with seven patients, GPT-4 (version 0613; OpenAI) was prompted on April
   9, 2024, to automatically generate structured clinical histories for the
   200 remaining patients. Using the reference standard, LLM extraction
   performance was calculated (recall, precision, F1 score). LLM-generated
   and original requisition histories were compared for completeness
   (proportion including each parameter), and 10 radiologists performed
   pairwise comparison for quality, preference, and subjective likelihood
   of harm. Results: For the 200 LLM-generated histories, GPT-4 performed
   well, extracting oncologic parameters from clinical notes (F1 = 0.983).
   Compared with original requisition histories, LLM-generated histories
   more frequently included parameters critical for radiologist
   interpretation, including primary oncologic diagnosis (99.5% vs 89% [199
   and 178 of 200 histories, respectively]; P < .001), acute or worsening
   symptoms (15% vs 4% [29 and seven of 200]; P < .001), and relevant
   surgery (61% vs 12% [122 and 23 of 200]; P < .001). Radiologists
   preferred LLM-generated histories for imaging interpretation (89% vs 5%,
   7% equal; P < .001), indicating they would enable more complete
   interpretation (86% vs 0%, 15% equal; P < .001) and have a lower
   likelihood of harm (3% vs 55%, 42% neither; P < .001). Conclusion: An
   LLM enabled accurate automated clinical histories for oncologic imaging
   from clinical notes. Compared with original requisition histories,
   LLM-generated histories were more complete and were preferred by
   radiologists for imaging interpretation and perceived safety.
ZA 0
Z8 0
ZR 0
ZB 0
ZS 0
TC 1
Z9 1
DA 2025-03-08
UT WOS:001434851700023
PM 39903072
ER

PT J
AU Coen, Emma
   Del Fiol, Guilherme
   Kaphingst, Kimberly A
   Borsato, Emerson
   Shannon, Jackilen
   Smith, Hadley
   Masino, Aaron
   Allen, Caitlin G
TI Chatbot for the Return of Positive Genetic Screening Results for
   Hereditary Cancer Syndromes: Prompt Engineering Project.
SO JMIR cancer
VL 11
BP e65848
EP e65848
DI 10.2196/65848
DT Journal Article
PD 2025 Jun 10
PY 2025
AB Unlabelled: The increasing demand for population-wide genomic screening
   and the limited availability of genetic counseling resources have
   created a pressing need for innovative service delivery models. Chatbots
   powered by large language models (LLMs) have shown potential in genomic
   services, particularly in pretest counseling, but their application in
   returning positive population-wide genomic screening results remains
   underexplored. Leveraging advanced LLMs like GPT-4 offers an opportunity
   to address this gap by delivering accurate, contextual, and
   user-centered communication to individuals receiving positive genetic
   test results. This project aimed to design, implement, and evaluate a
   chatbot integrated with GPT-4, tailored to support the return of
   positive genomic screening results in the context of South Carolina's In
   Our DNA SC program. This initiative offers free genetic screening to
   100,000 individuals, with over 33,000 results returned and numerous
   positive findings for conditions such as Lynch syndrome, hereditary
   breast and ovarian cancer syndrome, and familial hypercholesterolemia. A
   3-step prompt engineering process using retrieval-augmented generation
   and few-shot techniques was used to create the chatbot. Training
   materials included patient frequently asked questions, genetic
   counseling scripts, and patient-derived queries. The chatbot underwent
   iterative refinement based on 13 training questions, while performance
   was evaluated through expert ratings on responses to 2 hypothetical
   patient scenarios. The 2 scenarios were intended to represent common but
   distinct patient profiles in terms of gender, race, ethnicity, age, and
   background knowledge. Domain experts rated the chatbot using a 5-point
   Likert scale across 8 predefined criteria: tone, clarity, program
   accuracy, domain accuracy, robustness, efficiency, boundaries, and
   usability. The chatbot achieved an average score of 3.86 (SD 0.89)
   across all evaluation metrics. The highest-rated criteria were tone
   (mean 4.25, SD 0.71) and usability (mean 4.25, SD 0.58), reflecting the
   chatbot's ability to communicate effectively and provide a seamless user
   experience. Boundary management (mean 4.0, SD 0.76) and efficiency (mean
   3.88, SD 1.08) also scored well, while clarity and robustness received
   ratings of 3.81 (SD 1.05) and 3.81 (SD 0.66), respectively. Domain
   accuracy was rated 3.63 (SD 0.96), indicating satisfactory performance
   in delivering genetic information, whereas program accuracy received the
   lowest score of 3.25 (SD 1.39), highlighting the need for improvements
   in delivering program-specific details. This project demonstrates the
   feasibility of using LLM-powered chatbots to support the return of
   positive genomic screening results. The chatbot effectively handled
   open-ended patient queries, maintained conversational boundaries, and
   delivered user-friendly responses. However, enhancements in
   program-specific accuracy are essential to maximize its utility. Future
   research will explore hybrid chatbot designs that combine the strengths
   of LLMs with rule-based components to improve scalability, accuracy, and
   accessibility in genomic service delivery. The findings underscore the
   potential of generative artificial intelligence tools to address
   resource limitations and improve the accessibility of genomic health
   care services.
TC 0
ZR 0
Z8 0
ZS 0
ZB 0
ZA 0
Z9 0
DA 2025-06-13
UT MEDLINE:40493514
PM 40493514
ER

PT J
AU Olszewski, Robert
   Watros, Klaudia
   Manczak, Malgorzata
   Owoc, Jakub
   Jeziorski, Krzysztof
   Brzezinski, Jakub
TI Assessing the response quality and readability of chatbots in
   cardiovascular health, oncology, and psoriasis: A comparative study
SO INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
VL 190
AR 105562
DI 10.1016/j.ijmedinf.2024.105562
EA JUL 2024
DT Article
PD OCT 2024
PY 2024
AB Background: Chatbots using the Large Language Model (LLM) generate human
   responses to questions from all categories. Due to staff shortages in
   healthcare systems, patients waiting for an appointment increasingly use
   chatbots to get information about their condition. Given the number of
   chatbots currently available, assessing the responses they generate is
   essential. Methods: Five chatbots with free access were selected
   (Gemini, Microsoft Copilot, PiAI, ChatGPT, ChatSpot) and blinded using
   letters (A, B, C, D, E). Each chatbot was asked questions about
   cardiology, oncology, and psoriasis. Responses were compared to
   guidelines from the European Society of Cardiology, American Academy of
   Dermatology and American Society of Clinical Oncology. All answers were
   assessed using readability scales (Flesch Reading Scale, Gunning Fog
   Scale Level, Flesch-Kincaid Grade Level and Dale-Chall Score). Using a
   3point Likert scale, two independent medical professionals assessed the
   compliance of the responses with the guidelines. Results: A total of 45
   questions were asked of all chatbots. Chatbot C gave the shortest
   answers, 7.0 (6.0 - 8.0), and Chatbot A the longest 17.5 (13.0 - 24.5).
   The Flesch Reading Ease Scale ranged from 16.3 (12.2 - 21.9) (Chatbot D)
   to 39.8 (29.0 - 50.4) (Chatbot A). Flesch-Kincaid Grade Level ranged
   from 12.5 (10.6 - 14.6) (Chatbot A) to 15.9 (15.1 - 17.1) (Chatbot D).
   Gunning Fog Scale Level ranged from 15.77 (Chatbot A) to 19.73 (Chatbot
   D). Dale-Chall Score ranged from 10.3 (9.3 - 11.3) (Chatbot A) to 11.9
   (11.5 - 12.4) (Chatbot D). Conclusion: This study indicates that
   chatbots vary in length, quality, and readability. They answer each
   question in their own way, based on the data they have pulled from the
   web. Reliability of the responses generated by chatbots is high. This
   suggests that people who want information from a chatbot need to be
   careful and verify the answers they receive, particularly when they ask
   about medical and health aspects.
ZS 0
ZR 0
TC 3
Z8 1
ZB 0
ZA 0
Z9 4
DA 2024-08-07
UT WOS:001281403200001
PM 39059084
ER

PT J
AU Liu, ChaoXu
   Wei, MinYan
   Qin, Yu
   Zhang, MeiXiang
   Jiang, Huan
   Xu, JiaLe
   Zhang, YuNing
   Hua, Qing
   Hou, YiQing
   Dong, YiJie
   Xia, ShuJun
   Li, Ning
   Zhou, JianQiao
TI Harnessing Large Language Models for Structured Reporting in Breast
   Ultrasound: A Comparative Study of Open AI (GPT-4.0) and Microsoft Bing
   (GPT-4)
SO ULTRASOUND IN MEDICINE AND BIOLOGY
VL 50
IS 11
BP 1697
EP 1703
DI 10.1016/j.ultrasmedbio.2024.07.007
EA SEP 2024
DT Article
PD NOV 2024
PY 2024
AB Objectives To assess the capabilities of large language models (LLMs),
   including Open AI (GPT-4.0) and Microsoft Bing (GPT-4), in generating
   structured reports, the Breast Imaging Reporting and Data System
   (BI-RADS) categories, and management recommendations from free-text
   breast ultrasound reports. Materials and Methods In this retrospective
   study, 100 free-text breast ultrasound reports from patients who
   underwent surgery between January and May 2023 were gathered. The
   capabilities of Open AI (GPT-4.0) and Microsoft Bing (GPT-4) to convert
   these unstructured reports into structured ultrasound reports were
   studied. The quality of structured reports, BI-RADS categories, and
   management recommendations generated by GPT-4.0 and Bing were evaluated
   by senior radiologists based on the guidelines. Results Open AI
   (GPT-4.0) was better than Microsoft Bing (GPT-4) in terms of performance
   in generating structured reports (88% vs. 55%; p < 0.001), giving
   correct BI-RADS categories (54% vs. 47%; p = 0.013) and providing
   reasonable management recommendations (81% vs. 63%; p < 0.001). As the
   ability to predict benign and malignant characteristics, GPT-4.0
   performed significantly better than Bing (AUC, 0.9317 vs. 0.8177; p <
   0.001), while both performed significantly inferior to senior
   radiologists (AUC, 0.9763; both p < 0.001). Conclusion This study
   highlights the potential of LLMs, specifically Open AI (GPT-4.0), in
   converting unstructured breast ultrasound reports into structured ones,
   offering accurate diagnoses and providing reasonable recommendations.
TC 1
ZA 0
ZB 0
Z8 1
ZR 0
ZS 0
Z9 2
DA 2024-10-05
UT WOS:001322000900001
PM 39138026
ER

PT J
AU Bhayana, Rajesh
   Jajodia, Ankush
   Chawla, Tanya
   Deng, Yangqing
   Bouchard-Fortier, Genevieve
   Haider, Masoom
   Krishna, Satheesh
TI Accuracy of Large Language Model-based Automatic Calculation of
   Ovarian-Adnexal Reporting and Data System MRI Scores from Pelvic MRI
   Reports
SO RADIOLOGY
VL 315
IS 1
AR e241554
DI 10.1148/radiol.241554
DT Article
PD APR 2025
PY 2025
AB Background: Ovarian-Adnexal Reporting and Data System (O-RADS) for MRI
   helps assign malignancy risk, but radiologist adoption is inconsistent.
   Automatic assignment of O-RADS scores from reports could increase
   adoption and accuracy. Purpose: To evaluate the accuracy of large
   language models (LLMs), after strategic optimization, for automatically
   calculating O-RADS scores from reports. Materials and Methods: This
   retrospective single-center study from a large quaternary care cancer
   center included consecutive gadolinium chelate-enhanced pelvic MRI
   reports with at least one assigned O-RADS score from July 2021 to
   October 2023. Reports from January 2018 to October 2019 (before O-RADS
   MRI implementation) were randomly selected for additional testing.
   Reference standard O-RADS scores were determined by radiologists
   interpreting reports. After prompt optimization using a subset of
   reports, two LLM-based strategies were evaluated: few-shot learning with
   GPT-4 (version 0613; OpenAI) prompted with O-RADS rules ("LLM only") and
   a hybrid strategy leveraging GPT-4 to classify features fed into a
   deterministic formula ("hybrid"). Accuracy of each model and originally
   reported scores were calculated and compared using the McNemar test.
   Results: A total of 284 reports from 284 female patients (mean age, 53.2
   years +/- 16.3 [SD]) with 372 adnexal lesions were included: 10 reports
   in the training set (16 lesions), 134 reports in the internal test set 1
   (173 lesions; 158 O-RADS assigned), and 140 reports in internal test set
   2 (183 lesions). For assigning O-RADS MRI scores, the hybrid model
   accuracy (97%; 168 of 173) outperformed LLM-only model (90%; 155 of 173;
   P = .006). For lesions with an originally reported O-RADS score, hybrid
   model accuracy exceeded that of reporting radiologists (97% [153 of 158]
   vs 88% [139 of 158]; P = .004). Hybrid model also outperformed LLM-only
   model for 183 lesions from before O-RADS implementation (95% [173 of
   183] vs 87% [159 of 183], respectively; P = .01). Conclusion: A hybrid
   LLM-based application, combining LLM feature classification with
   deterministic elements, accurately assigned O-RADS MRI scores from
   report descriptions, exceeding both an LLM-only strategy and the
   original reporting radiologist. (c) RSNA, 2025
ZR 0
ZS 0
TC 1
ZA 0
ZB 0
Z8 0
Z9 1
DA 2025-04-20
UT WOS:001464808700007
PM 40167432
ER

PT J
AU Amini, Maziar
   Chang, Patrick
   Nguyen, Denis
   Davis, Rio O.
   Dodge, Jennifer
   Phan, Jennifer
   Buxbaum, James L.
   Sahakian, Ara B.
TI COMPARING CHATGPT3.5 AND BARD IN RECOMMENDING COLONOSCOPY INTERVALS:
   BRIDGING THE GAP IN HEALTHCARE SETTINGS
SO GASTROENTEROLOGY
VL 166
IS 5
MA Tu1991
BP S1482
EP S1482
SU S
DT Meeting Abstract
PD MAY 2024
PY 2024
CT Digestive Disease Week (DDW)
CY MAY 18-21, 2024
CL Washington, DC
ZS 0
ZB 0
Z8 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2024-10-30
UT WOS:001282837706089
ER

PT J
AU Arasteh, Soroosh Tayebi
   Han, Tianyu
   Lotfinia, Mahshad
   Kuhl, Christiane
   Kather, Jakob Nikolas
   Truhn, Daniel
   Nebelung, Sven
TI Large language models streamline automated machine learning for clinical
   studies
SO NATURE COMMUNICATIONS
VL 15
IS 1
AR 1603
DI 10.1038/s41467-024-45879-8
DT Article
PD FEB 21 2024
PY 2024
AB A knowledge gap persists between machine learning (ML) developers (e.g.,
   data scientists) and practitioners (e.g., clinicians), hampering the
   full utilization of ML for clinical data analysis. We investigated the
   potential of the ChatGPT Advanced Data Analysis (ADA), an extension of
   GPT-4, to bridge this gap and perform ML analyses efficiently.
   Real-world clinical datasets and study details from large trials across
   various medical specialties were presented to ChatGPT ADA without
   specific guidance. ChatGPT ADA autonomously developed state-of-the-art
   ML models based on the original study's training data to predict
   clinical outcomes such as cancer development, cancer progression,
   disease complications, or biomarkers such as pathogenic gene sequences.
   Following the re-implementation and optimization of the published
   models, the head-to-head comparison of the ChatGPT ADA-crafted ML models
   and their respective manually crafted counterparts revealed no
   significant differences in traditional performance metrics (p >= 0.072).
   Strikingly, the ChatGPT ADA-crafted ML models often outperformed their
   counterparts. In conclusion, ChatGPT ADA offers a promising avenue to
   democratize ML in medicine by simplifying complex data analyses, yet
   should enhance, not replace, specialized training and resources, to
   promote broader applications in medical research and practice.
   A knowledge gap persists between machine learning developers and
   clinicians. Here, the authors show that the Advanced Data Analysis
   extension of ChatGPT could bridge this gap and simplify complex data
   analyses, making them more accessible to clinicians.
ZA 0
ZS 0
ZB 8
TC 34
Z8 1
ZR 0
Z9 35
DA 2024-03-28
UT WOS:001173879300030
PM 38383555
ER

PT J
AU Giannuzzi, Federico
   Carla, Matteo Mario
   Hu, Lorenzo
   Cestrone, Valentina
   Caputo, Carmela Grazia
   Sammarco, Maria Grazia
   Savino, Gustavo
   Rizzo, Stanislao
   Blasi, Maria Antonietta
   Pagliara, Monica Maria
TI Artificial intelligence with ChatGPT 4: a large language model in
   support of ocular oncology cases
SO INTERNATIONAL OPHTHALMOLOGY
VL 45
IS 1
AR 59
DI 10.1007/s10792-024-03399-w
DT Article
PD FEB 7 2025
PY 2025
AB PurposeTo evaluate ChatGPT's ability to analyze comprehensive case
   descriptions of patients with uveal melanoma and provide recommendations
   for the most appropriate management.DesignRetrospective analysis of
   ocular oncology patients' medical records.Subjects.Forty patients
   treated for uveal melanoma between May 2019 and October
   2023.DesignRetrospective analysis of ocular oncology patients' medical
   records.Subjects.Forty patients treated for uveal melanoma between May
   2019 and October 2023.DesignRetrospective analysis of ocular oncology
   patients' medical records.Subjects.Forty patients treated for uveal
   melanoma between May 2019 and October 2023.MethodsWe uploaded each case
   description into the ChatGPT interface (version 4.0) and asked the model
   to provide realistic treatment options by asking the question, "What
   type of treatment do you recommend?" The accuracy of decisions produced
   by ChatGPT was compared to those recorded in patients' files and the
   treatment recommendations provided by three ocular oncologists, each
   with more than 10 years of experience.Main outcome measures.The primary
   objective of this research was to assess the accuracy of ChatGPT replies
   in ocular oncology cases, analyzing its competence in both
   straightforward and intricate situations. Our secondary purpose was to
   assess the concordance between the responses of ChatGPT and those of
   ocular oncology specialists when faced with analogous clinical
   scenarios.MethodsWe uploaded each case description into the ChatGPT
   interface (version 4.0) and asked the model to provide realistic
   treatment options by asking the question, "What type of treatment do you
   recommend?" The accuracy of decisions produced by ChatGPT was compared
   to those recorded in patients' files and the treatment recommendations
   provided by three ocular oncologists, each with more than 10 years of
   experience.Main outcome measures.The primary objective of this research
   was to assess the accuracy of ChatGPT replies in ocular oncology cases,
   analyzing its competence in both straightforward and intricate
   situations. Our secondary purpose was to assess the concordance between
   the responses of ChatGPT and those of ocular oncology specialists when
   faced with analogous clinical scenarios.MethodsWe uploaded each case
   description into the ChatGPT interface (version 4.0) and asked the model
   to provide realistic treatment options by asking the question, "What
   type of treatment do you recommend?" The accuracy of decisions produced
   by ChatGPT was compared to those recorded in patients' files and the
   treatment recommendations provided by three ocular oncologists, each
   with more than 10 years of experience.Main outcome measures.The primary
   objective of this research was to assess the accuracy of ChatGPT replies
   in ocular oncology cases, analyzing its competence in both
   straightforward and intricate situations. Our secondary purpose was to
   assess the concordance between the responses of ChatGPT and those of
   ocular oncology specialists when faced with analogous clinical
   scenarios.ResultsChatGPT's surgical choices matched those in patients'
   files in 55% of cases (22 out of 40). ChatGPT options were agreed upon
   by 50%, 55%, and 57% of the three ocular oncology specialists. The
   investigation revealed significant differences between ChatGPT's
   responses and those of the three cancer specialists when compared to
   patients' files (p = 0.003, p = 0.001, and p = 0.001). ChatGPT's
   surgical responses matched with patient data in 18 out of 24 cases
   (75%), excluding enucleation cases.
   The decisions matched with the three ocular oncology specialists in
   17/24, 18/24, and 18/24 cases, reflecting agreements of 70%, 75%, and
   75%, respectively. The decisions made by ChatGPT were not significantly
   different from those of the three professionals in this cohort (p =
   0.50, p = 0.36, and p = 0.36 for ChatGPT compared to specialists 1, 2,
   and 3).ConclusionChatGPT exhibited a level of proficiency that was
   comparable to that of trained ocular oncology specialists. However, it
   exhibited its limitations when evaluating more complex scenarios, such
   as extrascleral extension or infiltration of the optic nerve, when a
   comprehensive evaluation of the patient is therefore necessary.
ZS 0
Z8 0
ZB 0
ZR 0
TC 0
ZA 0
Z9 0
DA 2025-04-25
UT WOS:001468330700001
PM 39918656
ER

PT J
AU Horesh, Nir
   Emile, Sameh
   Gupta, Shashank
   Garoufalia, Zoe
   Gefen, Rachel
   Zhou, Peige
   da Silva, Giovanna
   Wexner, Steven
TI Comparing the Management Recommendations of Large Language Model and
   Colorectal Cancer Multidisciplinary Team: A Pilot Study
SO DISEASES OF THE COLON & RECTUM
VL 68
IS 1
BP 41
EP 47
DI 10.1097/DCR.0000000000003504
DT Article
PD JAN 2025
PY 2025
AB BACKGROUND:Management of anorectal cancers requires a multidisciplinary
   team approach. Recently, large language models have been suggested as
   potential tools for various applications in health care.OBJECTIVE:Assess
   suggested management recommendations provided by a generative artificial
   intelligence chatbot with those of a colorectal cancer multidisciplinary
   team to evaluate applicability in clinical settings.DESIGN:Comparative
   pilot study where management recommendations from a generative
   artificial intelligence chatbot for patients with anal or colorectal
   cancers were compared against historical consensus decisions from
   multidisciplinary team meetings.SETTING:Single referral tertiary
   center.PATIENTS:Fifteen patients (mean age of 66.5 years; 53.5% woman)
   were included; 80% were primarily diagnosed with rectal cancer,
   predominantly stage II and III disease (46.6%). The mean tumor height
   from the anal verge was 4 cm.INTERVENTIONS:From a generative artificial
   intelligence chatbot, we generated management recommendations for each
   patient, which were subsequently compared to historical decisions from a
   multidisciplinary team to gauge concordance.MAIN OUTCOME
   MEASURES:Primary outcomes included a degree of concordance between
   generative artificial intelligence chatbot recommendations and the
   multidisciplinary team decisions, assessed on a scale from 1 (complete
   disagreement) to 5 (complete agreement), and justification was evaluated
   by 3 experienced colorectal surgeons.RESULTS:A generative artificial
   intelligence chatbot achieved a high concordance rate with
   multidisciplinary team decisions, with an average concordance rating of
   4.08. Multidisciplinary team treatment strategies included neoadjuvant
   therapy for 33.3% of patients, upfront surgery for 26.6%, and further
   diagnostic assessment for 20%. Interrater agreement on concordance was
   found to be moderate (kappa coefficient range, 0.333-0.577), whereas
   agreement on decision justification was slight (kappa coefficient range,
   0.047-0.094).LIMITATIONS:Retrospective study with small sample
   size.CONCLUSIONS:The findings indicate a high level of concordance
   between generative artificial intelligence chatbot recommendations and
   the decisions from a colorectal cancer multidisciplinary team,
   suggesting the potential of large language models to support clinical
   decision-making in the management of anal and colorectal cancers. See
   Video Abstract.COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL
   MODELO EXTENSO DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C &
   Aacute;NCER COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los
   c & aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.
   PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a & ntilde;os;
   53,5% mujeres); el 80% fueron diagnosticados principalmente de c &
   aacute;ncer de recto, con predominio de la enfermedad en estadio II-III
   (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.
   INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la concordancia.
   PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios incluyeron el
   grado de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones del equipo
   multidisciplinario, evaluadas en una escala de 1 (desacuerdo total) a 5
   (acuerdo total), y la justificaci & oacute;n evaluada por tres cirujanos
   colorrectales experimentados.RESULTADOS:Un chatbot de inteligencia
   artificial generativa logr & oacute; una alta tasa de concordancia con
   las decisiones del equipo multidisciplinario, con una calificaci &
   oacute;n media de concordancia de 4,08. Las estrategias de tratamiento
   del equipo multidisciplinario incluyeron terapia neoadyuvante para el
   33,3% de los pacientes, cirug & iacute;a inicial para el 26,6% y
   evaluaci & oacute;n diagn & oacute;stica adicional para el 20%. La
   concordancia entre los evaluadores fue moderada (rango del coeficiente
   kappa: 0,333 a 0,577), mientras que la concordancia en la justificaci &
   oacute;n de las decisiones fue leve (rango del coeficiente kappa: 0,047
   a 0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales experimentados.
   RESULTADOS:Un chatbot de inteligencia artificial generativa logr &
   oacute; una alta tasa de concordancia con las decisiones del equipo
   multidisciplinario, con una calificaci & oacute;n media de concordancia
   de 4,08. Las estrategias de tratamiento del equipo multidisciplinario
   incluyeron terapia neoadyuvante para el 33,3% de los pacientes, cirug &
   iacute;a inicial para el 26,6% y evaluaci & oacute;n diagn &
   oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%.
   La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.
   CONCLUSIONES:Los hallazgos indican un alto nivel de concordancia entre
   las recomendaciones de un chatbot de inteligencia artificial generativa
   y las decisiones de un equipo multidisciplinario de c & aacute;ncer
   colorrectal, lo que sugiere el potencial de los modelos extensos de
   lenguaje en apoyar la toma de decisiones cl & iacute;nicas en el manejo
   del c & aacute;ncer anal y colorrectal. (Traducci & oacute;n: Dr. Fidel
   Ruiz Healy).COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL
   MODELO EXTENSO DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C &
   Aacute;NCER COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los
   c & aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).
   COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO
   DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER
   COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los c &
   aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.
   OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n sugeridos por
   un chatbot de inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones
   LUGAR:Un & uacute;nico centro terciario de referencia.PACIENTES:Se
   incluyeron 15 pacientes (edad media de 66,5 a & ntilde;os; 53,5%
   mujeres); el 80% fueron diagnosticados principalmente de c & aacute;ncer
   de recto, con predominio de la enfermedad en estadio II-III (46,6%). La
   altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.
   INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la concordancia.
   PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios incluyeron el
   grado de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones del equipo
   multidisciplinario, evaluadas en una escala de 1 (desacuerdo total) a 5
   (acuerdo total), y la justificaci & oacute;n evaluada por tres cirujanos
   colorrectales experimentados.RESULTADOS:Un chatbot de inteligencia
   artificial generativa logr & oacute; una alta tasa de concordancia con
   las decisiones del equipo multidisciplinario, con una calificaci &
   oacute;n media de concordancia de 4,08. Las estrategias de tratamiento
   del equipo multidisciplinario incluyeron terapia neoadyuvante para el
   33,3% de los pacientes, cirug & iacute;a inicial para el 26,6% y
   evaluaci & oacute;n diagn & oacute;stica adicional para el 20%. La
   concordancia entre los evaluadores fue moderada (rango del coeficiente
   kappa: 0,333 a 0,577), mientras que la concordancia en la justificaci &
   oacute;n de las decisiones fue leve (rango del coeficiente kappa: 0,047
   a 0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales experimentados.
   RESULTADOS:Un chatbot de inteligencia artificial generativa logr &
   oacute; una alta tasa de concordancia con las decisiones del equipo
   multidisciplinario, con una calificaci & oacute;n media de concordancia
   de 4,08. Las estrategias de tratamiento del equipo multidisciplinario
   incluyeron terapia neoadyuvante para el 33,3% de los pacientes, cirug &
   iacute;a inicial para el 26,6% y evaluaci & oacute;n diagn &
   oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%.
   La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.
   CONCLUSIONES:Los hallazgos indican un alto nivel de concordancia entre
   las recomendaciones de un chatbot de inteligencia artificial generativa
   y las decisiones de un equipo multidisciplinario de c & aacute;ncer
   colorrectal, lo que sugiere el potencial de los modelos extensos de
   lenguaje en apoyar la toma de decisiones cl & iacute;nicas en el manejo
   del c & aacute;ncer anal y colorrectal. (Traducci & oacute;n: Dr. Fidel
   Ruiz Healy).COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL
   MODELO EXTENSO DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C &
   Aacute;NCER COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los
   c & aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).
   COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO
   DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER
   COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los c &
   aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.
   OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n sugeridos por
   un chatbot de inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones
   LUGAR:Un & uacute;nico centro terciario de referencia.PACIENTES:Se
   incluyeron 15 pacientes (edad media de 66,5 a & ntilde;os; 53,5%
   mujeres); el 80% fueron diagnosticados principalmente de c & aacute;ncer
   de recto, con predominio de la enfermedad en estadio II-III (46,6%). La
   altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.
   INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).
ZS 0
ZA 0
ZB 0
ZR 0
Z8 0
TC 0
Z9 0
DA 2025-01-05
UT WOS:001385242400023
PM 39679608
ER

EF