FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Ghaffar, Umar
   Tariq, Amara
   Choudry, Mouneeb
   Briggs, Logan
   Channar, Aneeta
   Banerjee, Imon
   Luo, Man
   Bin Riaz, Irbaz
   Abdul-Muhsin, Haidar
TI Domain-specific large language model for predicting prostate cancer
   treatment plan.
SO JOURNAL OF CLINICAL ONCOLOGY
VL 43
IS 5_SUPPL
AR 428
DI 10.1200/JCO.2025.43.5_suppl.428
SU 5_SUPPL
DT Meeting Abstract
PD FEB 10 2025
PY 2025
ZB 0
TC 0
ZR 0
Z8 0
ZA 0
ZS 0
Z9 0
DA 2025-04-11
UT WOS:001454809900018
ER

PT J
AU Umar, Ghaffar
   Amara, Tariq
   Mouneeb, M. Choudry
   Logan, G. Briggs
   Aneeta, Channar
   Imon, Banerjee
   Man, Luo
   Bin, Riaz Irbaz
   Haidar, M. Abdul-Muhsin
TI DOMAIN-SPECIFIC LARGE LANGUAGE MODEL FOR PREDICTING PROSTATE CANCER
   TREATMENT PLAN
SO UROLOGIC ONCOLOGY-SEMINARS AND ORIGINAL INVESTIGATIONS
VL 43
IS 3
MA 99
DI 10.1016/j.urolonc.2024.12.102
EA FEB 2025
SU S
DT Meeting Abstract
PD MAR 2025
PY 2025
TC 0
ZA 0
ZS 0
ZB 0
Z8 0
ZR 0
Z9 0
DA 2025-04-12
UT WOS:001460495000100
ER

PT J
AU Chen, Shan
   Kann, Benjamin H.
   Foote, Michael B.
   Aerts, Hugo J. W. L.
   Savova, Guergana K.
   Mak, Raymond H.
   Bitterman, Danielle S.
TI Use of Artificial Intelligence Chatbots for Cancer Treatment Information
SO JAMA ONCOLOGY
VL 9
IS 10
BP 1459
EP 1462
DI 10.1001/jamaoncol.2023.2954
EA AUG 2023
DT Letter
PD OCT 2023
PY 2023
Z8 2
ZB 20
ZS 0
ZR 0
ZA 0
TC 104
Z9 105
DA 2023-09-27
UT WOS:001063516700006
PM 37615976
ER

PT J
AU Santoki, Aditya
   Jones, Rachel
   Peter, Manas
   Mathew, Aju
TI Implementing large language model-based artificial intelligence (AI)
   technology in proposing effective treatment plans in patients with
   cancer
SO JOURNAL OF CLINICAL ONCOLOGY
VL 42
IS 16
MA e13660
SU S
DT Meeting Abstract
PD JUN 1 2024
PY 2024
CT Special Clinical Science Symposia
CY MAY 29-29, 2024
CL ELECTR NETWORK
ZS 0
TC 0
Z8 0
ZR 0
ZA 0
ZB 0
Z9 0
DA 2024-08-18
UT WOS:001275557403421
ER

PT J
AU Liu, Rozee Junhan
   Forsythe, Anna
   Rege, Jessicca Martin
   Kaufman, Peter
TI BIO25-024: Real-Time Clinical Trial Data Library in Non-Small Cell Lung
   (NSCLC), Prostate (PC), and Breast Cancer (BC) to Support Informed
   Treatment Decisions: Now a Reality With a Fine-Tuned Large Language
   Model (LLM).
SO Journal of the National Comprehensive Cancer Network : JNCCN
VL 23
IS 3.5
DI 10.6004/jnccn.2024.7156
DT Journal Article
PD 2025 Mar 28
PY 2025
Z8 0
ZA 0
ZS 0
ZR 0
TC 0
ZB 0
Z9 0
DA 2025-04-02
UT MEDLINE:40157350
PM 40157350
ER

PT J
AU Naik, Himani R.
   Prather, Andrew D.
   Gurda, Grzegorz T.
TI Synchronous Bilateral Breast Cancer: A Case Report Piloting and
   Evaluating the Implementation of the AI-Powered Large Language Model
   (LLM) ChatGPT
SO CUREUS JOURNAL OF MEDICAL SCIENCE
VL 15
IS 4
AR e37587
DI 10.7759/cureus.37587
DT Article
PD APR 14 2023
PY 2023
AB Primary breast carcinoma is the most common cancer type in women, and
   although bilateral synchronous breast cancers (s-BBC) remain quite rare,
   the reported incidence may increase with the adoption of more sensitive
   imaging modalities. Here, we present a case of histomorphological and
   clinically distinct s-BBC, together with a discussion of clinical
   management decisions, prognosis, and treatment standards and how these
   relate to outcomes vis-a-vis more established standards in unifocal
   breast carcinoma. The case report also constitutes a pilot and formal
   evaluation of a large language model (LLM) of ChatGPT as a tool to aid
   in generating a single patient case report.
ZB 4
Z8 1
ZR 0
ZA 0
TC 8
ZS 0
Z9 9
DA 2023-11-05
UT WOS:001082835600036
PM 37193434
ER

PT J
AU Yuan, Yue
   Zhang, Guolong
   Gu, Yuqi
   Hao, Sicheng
   Huang, Chen
   Xie, Hongxia
   Mi, Wei
   Zeng, Yingchun
TI Artificial intelligence-assisted machine learning models for predicting
   lung cancer survival
SO ASIA-PACIFIC JOURNAL OF ONCOLOGY NURSING
VL 12
AR 100680
DI 10.1016/j.apjon.2025.100680
EA MAR 2025
DT Article
PD DEC 2025
PY 2025
AB Objective: This study aimed to evaluate the feasibility of large
   language model-Advanced Data Analysis (ADA) in developing and
   implementing machine learning models to predict survival outcomes for
   lung cancer patients, with a focus on its implications for nursing
   practice. Methods: A retrospective study design was employed using a
   dataset of lung cancer patients. Data included sociodemographic,
   clinical, treatment-specific, and comorbidity variables. Large language
   model-ADA was used to build and evaluate three machine learning models.
   Model performance was validated, and results were presented using
   calibration plots. Results: Of 737 patients, the survival rate of this
   cohort was 73.3%, with a mean age of 59.32 years. Calibration plots
   indicated robust model reliability across all models. The Random Forest
   model demonstrated the highest predictive accuracy among the models.
   Most critical features identified were preoperative white blood cells
   (2.2%), preoperative lung function of Forced Expiratory Volume in one
   second (2.1%), preoperative arterial oxygen saturation (1.9%),
   preoperative partial pressure of oxygen (1.7%), preoperative albumin
   (1.6%), preoperative preparation time (1.5%), age at admission (1.5%),
   preoperative partial pressure of carbon dioxide (1.5%), preoperative
   hospital stay days (1.5%), and postoperative total days of thoracic tube
   drainage (1.4%). Conclusions: Large language model-ADA effectively
   facilitates the development of machine learning models for lung cancer
   survival prediction, enabling non-technical health care professionals to
   harness the power of advanced analytics. The findings underscore the
   importance of preoperative factors in predicting outcomes, while also
   highlighting the need for external validation across diverse settings.
ZS 0
ZR 0
ZA 0
Z8 0
ZB 0
TC 0
Z9 0
DA 2025-03-31
UT WOS:001451782200001
PM 40201531
ER

PT C
AU Marques, Adriell Gomes
   Candido de Figueiredo, Marcus Vinicius
   Da Costa Nascimento, Jose Jerovane
   de Souza, Cidcley Teixeira
   Jaborandy de Mattos Dourado, Carlos Mauricio, Jr.
   de Albuquerque, Victor Hugo C.
   de Freitas Souza, Luis Fabricio
BE Emmendorfer, LR
   Kieling, A
TI New approach Generative AI Melanoma Data Fusion for classification in
   dermoscopic images with Large Language Model
SO 2024 37TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES, SIBGRAPI
   2024
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
BP 157
EP 162
DI 10.1109/SIBGRAPI62404.2024.10716298
DT Proceedings Paper
PD 2024
PY 2024
AB Skin cancer is a disease that causes thousands of deaths each year.
   Early diagnosis and monitoring the progression of the disease are
   crucial factors for the treatment and health indicators of a society.
   This study presents an innovative approach for the detection,
   segmentation, and classification of melanomas in dermoscopic images
   using advanced Computer Vision and Artificial Intelligence (AI) methods.
   Specifically, it applies Large Language Model (LLM) solutions for
   pre-diagnosis results through generative AI. This work explores
   combinations of methods for melanoma detection and segmentation based on
   the YOLO and SAM architectures, achieving 99% accuracy, surpassing
   various studies in the literature. The classification phase is based on
   a pipeline integrating feature extraction and selecting the best
   combination for melanoma region classification, achieving an accuracy of
   86.0%, also outperforming different studies in the literature.
CT 37th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)
CY SEP 30-OCT 03, 2024
CL Manaus, BRAZIL
SP SIBGRAPI; Univ Estado Amazonas, Escola Super Tecnologia; Coordinat
   Improvement Higher Educ Personnel; SiDi Tecnologia lnformacao Servicos
   lnterdependencia; Inst Pesquisas Eldorado; Sidia; Tutiplast Ind &
   Comercio Ltda; LUDUS Lab Tecnologia, lnovavao & Economia Criativa;
   ThinkTED Lab Pesquisa, Desenvolvimento & lnovacao Tecnologias
   Emergentes; Brazilian Comp Soc; IEEE Brazil NE; Governo Estado Amazonas
ZB 0
ZR 0
ZS 0
Z8 0
ZA 0
TC 0
Z9 0
DA 2025-01-25
UT WOS:001345126400027
ER

PT C
AU Dodhia, Parth
   Meepagala, Shawn
   Moallem, Golanz
   Rubin, Daniel
   Bean, Gregory
   Rusu, Mirabela
BE Yoshida, H
   Wu, S
TI Assessing breast cancer chemotherapy response in radiology and pathology
   reports via a Large Language Model
SO IMAGING INFORMATICS FOR HEALTHCARE, RESEARCH, AND APPLICATIONS, MEDICAL
   IMAGING 2024
SE Proceedings of SPIE
VL 12931
AR 1293102
DI 10.1117/12.3006495
DT Proceedings Paper
PD 2024
PY 2024
AB A wealth of medical knowledge is used to make clinical decisions, yet
   treatment or disease outcomes are challenging to assess without clinical
   trials. However, clinical trials take time, are expensive, and are
   impossible to perform for every decision. One approach to systematically
   assess treatment outcomes involves the retrospective analysis of
   clinical notes, e.g., radiology and pathology reports. While such
   studies are often performed by clinicians who manually review the notes
   and other information, such retrospective analysis can benefit from the
   automated parsing of radiology and pathology reports to provide
   systematic framework to extract outcome information.
   In this study, we used a large language model, i.e., ChatGPT (GPT-3.5),
   to parse 267 radiology and pathology reports and extract information
   related to response to neoadjuvant chemotherapy in patients with breast
   cancer. Our study includes a heterogeneous group of 89 women who
   underwent neoadjuvant therapy and underwent two MRI exams, pre- and
   post-therapy, followed by surgery (lumpectomy or mastectomy). We
   assessed the treatment response based on clinical reports from the
   post-therapy surgical excision. From the reports, we extracted the
   number of lesions, their anatomic location, and size.
   Our study provides insight into neoadjuvant chemotherapy response,
   indicating that even cases with complete MRI response can still have
   residual invasive breast carcinoma (1/3 of subjects), and, on the other
   hand, even those with reduced MRI response (<30% reduction in tumor
   size) can have no residual tumor at surgery, indicating that when cancer
   responds to treatment, it may not be captured by the MRI. The large
   language model achieved sensitivities of 84-94% in extracting the
   information from radiology reports, but had lower performance in the
   pathology reports, 72-87%, where more information is provided in free
   format. While this study is preliminary and performed in a small cohort,
   it illustrates the complexity of outcome prediction using radiology
   images.
CT Conference on Medical Imaging - Imaging Informatics for Healthcare,
   Research, and Applications
CY FEB 19-21, 2024
CL San Diego, CA
SP SPIE; Amer Assoc Physicists Med; Radiol Soc N Amer; World Mol Imaging
   Soc; Soc Imaging Informat Med; Int Fdn Comp Assisted Radiol & Surg; Med
   Image Percept Soc
ZA 0
ZB 0
ZR 0
ZS 0
Z8 0
TC 0
Z9 0
DA 2024-05-31
UT WOS:001219280700001
ER

PT C
AU Kim, Mirae
   Hwang, Kyubum
   Oh, Hayoung
   Kim, Heejin
   Kim, Min Ah
GP ACM
TI Can a Chatbot be Useful in Childhood Cancer Survivorship? Development of
   a Chatbot for Survivors of Childhood Cancer
SO PROCEEDINGS OF THE 32ND ACM INTERNATIONAL CONFERENCE ON INFORMATION AND
   KNOWLEDGE MANAGEMENT, CIKM 2023
BP 4018
EP 4022
DI 10.1145/3583780.3615234
DT Proceedings Paper
PD 2023
PY 2023
AB This study introduces an informational and empathetic chatbot for
   childhood cancer survivors. As the survival rates for childhood cancer
   around the world have increased, survivors often face various
   psychosocial challenges during and after cancer treatment. However, they
   rarely seek support from psychosocial professionals due to the low
   availability of resources and stigma toward cancer survivors in
   countries like South Korea. This study aimed to develop a chatbot tailed
   to the unique characteristics of childhood cancer survivors in need of
   informational and emotional support. Given the limited availability of
   empirical data on childhood cancer survivors, quotes from survivors were
   gathered from academic articles and social media, then large language
   models were employed to generate appropriate responses. Furthermore, we
   incorporated domain learning techniques to ensure a more tailored and
   suitable model for addressing the needs of survivors.
CT 32nd ACM International Conference on Information and Knowledge
   Management (CIKM)
CY OCT 21-25, 2023
CL Birmingham, ENGLAND
SP Assoc Comp Machinery; ACM Special Interest Grp Informat Retrieval; ACM
   SIGWEB
TC 1
ZS 0
ZA 0
Z8 0
ZB 0
ZR 0
Z9 1
DA 2024-03-19
UT WOS:001161549504012
ER

PT C
AU Gubanov, Michael
   Pyayt, Anna
   Karolak, Aleksandra
GP ACM
TI CancerKG.ORG-A Web-scale, Interactive, Verifiable Knowledge Graph-LLM
   Hybrid for Assisting with Optimal Cancer Treatment and Care
SO PROCEEDINGS OF THE 33RD ACM INTERNATIONAL CONFERENCE ON INFORMATION AND
   KNOWLEDGE MANAGEMENT, CIKM 2024
BP 4497
EP 4505
DI 10.1145/3627673.3680094
DT Proceedings Paper
PD 2024
PY 2024
AB Here, we describe one of the first Web-scale hybrid Knowledge Graph
   (KG)-Large Language Model (LLM), populated with the latest peer-reviewed
   medical knowledge on colorectal Cancer. It is currently being evaluated
   to assist with both medical research and clinical information retrieval
   tasks at Moffitt Cancer Center, which is one of the top Cancer centers
   in the U.S. and in the world. Our hybrid is remarkable as it serves the
   user needs better than just an LLM, KG or a search-engine in isolation.
   LLMs as is are known to exhibit hallucinations and catastrophic
   forgetting as well as are trained on outdated corpora. The state of the
   art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require
   manual curation, hence are quickly getting stale. CancerKG is
   unsupervised and is capable of automatically ingesting and organizing
   the latest medical findings. To alleviate the LLMs shortcomings, the
   verified KG serves as a Retrieval Augmented Generation (RAG) guardrail.
   CancerKG exhibits 5 different advanced user interfaces, each tailored to
   serve different data modalities better and more convenient for the user.
   We evaluated CancerKG on real user queries and report a high NDCG score
   on a large-scale corpora of approximately 44K publications.
CT 33rd ACM International Conference on Information and Knowledge
   Management (CIKM)
CY OCT 21-25, 2024
CL Boise, ID
SP Assoc Comp Machinery; ACM SIGIR; ACM SIGWEB
TC 2
ZS 0
ZR 0
ZA 0
ZB 2
Z8 0
Z9 2
DA 2025-03-05
UT WOS:001349579604110
ER

PT J
AU Zhu, Libing
   Rong, Yi
   Mcgee, Lisa A.
   Rwigema, Jean-Claude M.
   Patel, Samir H.
TI Testing and Validation of a Custom Retrained Large Language Model for
   the Supportive Care of HN Patients with External Knowledge Base
SO CANCERS
VL 16
IS 13
AR 2311
DI 10.3390/cancers16132311
DT Article
PD JUL 2024
PY 2024
AB Simple Summary Cancer patients, especially long-distance patients, often
   struggle to receive timely and precise medical information and support
   for their symptom management and survivorship care. ChatGPT-4's
   responses to queries concerning head and neck (HN) cancer remain
   questionable. The purpose of this study was to develop and validate a
   retrained large language model (LLM) for HN cancer patients. In this
   cross-sectional study, the presented LLM was retrained with a
   high-quality user-defined knowledge base. The responses from the LLM to
   patients' questions were validated against human responses, and the
   model showed a superior performance, with average scores of 4.25 for
   accuracy, 4.35 for clarity, 4.22 for completeness, and 4.32 for
   relevance, on a 5-point scale. The confined-trained LLM with a
   high-quality user-defined knowledge base demonstrates high accuracy,
   clarity, completeness, and relevance in offering evidence-based
   information and guidance on the symptom management and survivorship care
   for head and neck cancer patients.Abstract Purpose: This study aimed to
   develop a retrained large language model (LLM) tailored to the needs of
   HN cancer patients treated with radiotherapy, with emphasis on symptom
   management and survivorship care. Methods: A comprehensive external
   database was curated for training ChatGPT-4, integrating
   expert-identified consensus guidelines on supportive care for HN
   patients and correspondences from physicians and nurses within our
   institution's electronic medical records for 90 HN patients. The
   performance of our model was evaluated using 20 patient post-treatment
   inquiries that were then assessed by three Board certified radiation
   oncologists (RadOncs). The rating of the model was assessed on a scale
   of 1 (strongly disagree) to 5 (strongly agree) based on accuracy,
   clarity of response, completeness s, and relevance. Results: The average
   scores for the 20 tested questions were 4.25 for accuracy, 4.35 for
   clarity, 4.22 for completeness, and 4.32 for relevance, on a 5-point
   scale. Overall, 91.67% (220 out of 240) of assessments received scores
   of 3 or higher, and 83.33% (200 out of 240) received scores of 4 or
   higher. Conclusion: The custom-trained model demonstrates high accuracy
   in providing support to HN patients offering evidence-based information
   and guidance on their symptom management and survivorship care.
ZA 0
ZS 0
Z8 0
ZR 0
TC 1
ZB 1
Z9 1
DA 2024-07-22
UT WOS:001269842700001
PM 39001375
ER

PT J
AU Gerstung, Moritz
   Liu, David
   Ghassemi, Marzyeh
   Zou, James
   Chowell, Diego
   Teuwen, Jonas
   Mahmood, Faisal
   Kather, Jakob Nikolas
TI Artificial intelligence
SO CANCER CELL
VL 42
IS 6
BP 915
EP 918
DT Editorial Material
PD JUN 10 2024
PY 2024
ZS 0
ZR 0
ZA 0
Z8 0
TC 2
ZB 1
Z9 2
DA 2025-02-12
UT WOS:001412853800001
PM 38861926
ER

PT J
AU Hooshangnejad, Hamed
   Huang, Gaofeng
   Kelly, Katelyn
   Feng, Xue
   Luo, Yi
   Zhang, Rui
   Xu, Ziyue
   Chen, Quan
   Ding, Kai
TI EXACT-Net: Framework for EHR-Guided Lung Tumor Auto-Segmentation for
   Non-Small Cell Lung Cancer Radiotherapy
SO CANCERS
VL 16
IS 23
AR 4097
DI 10.3390/cancers16234097
DT Article
PD DEC 2024
PY 2024
AB Background/Objectives: Lung cancer is a devastating disease with the
   highest mortality rate among cancer types. Over 60% of non-small cell
   lung cancer (NSCLC) patients, accounting for 87% of lung cancer
   diagnoses, require radiation therapy. Rapid treatment initiation
   significantly increases the patient's survival rate and reduces the
   mortality rate. Accurate tumor segmentation is a critical step in
   diagnosing and treating NSCLC. Manual segmentation is time- and
   labor-consuming and causes delays in treatment initiation. Although many
   lung nodule detection methods, including deep learning-based models,
   have been proposed. Most of these methods still have a long-standing
   problem of high false positives (FPs). Methods: Here, we developed an
   electronic health record (EHR)-guided lung tumor auto-segmentation
   called EXACT-Net (EHR-enhanced eXACtitude in Tumor segmentation), where
   the extracted information from EHRs using a pre-trained large language
   model (LLM) was used to remove the FPs and keep the TP nodules only.
   Results: The auto-segmentation model was trained on NSCLC patients'
   computed tomography (CT), and the pre-trained LLM was used with the
   zero-shot learning approach. Our approach resulted in a 250% boost in
   successful nodule detection using the data from ten NSCLC patients
   treated in our institution. Conclusions: We demonstrated that combining
   vision-language information in EXACT-Net multi-modal AI framework
   greatly enhances the performance of vision only models, paving the road
   to multimodal AI framework for medical image processing.
ZS 0
ZA 0
ZR 0
ZB 0
TC 0
Z8 0
Z9 0
DA 2024-12-19
UT WOS:001376131100001
PM 39682283
ER

PT J
AU Wang, P.
   Liu, Z.
   Li, Y.
   Holmes, J.
   Shu, P.
   Zhang, L.
   Li, X.
   Li, Q.
   Vora, S. A.
   Patel, S. H.
   Sio, T. T. W.
   Liu, T.
   Liu, W.
TI Fine-Tuning Large Language Models for Radiation Oncology, A Specialized
   Health Care Domain
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3452
BP E664
EP E664
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
Z8 0
ZB 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892302131
ER

PT J
AU Yasaka, Koichiro
   Kanzawa, Jun
   Kanemaru, Noriko
   Koshino, Saori
   Abe, Osamu
TI Fine-Tuned Large Language Model for Extracting Patients on Pretreatment
   for Lung Cancer from a Picture Archiving and Communication System Based
   on Radiological Reports
SO JOURNAL OF IMAGING INFORMATICS IN MEDICINE
VL 38
IS 1
BP 327
EP 334
DI 10.1007/s10278-024-01186-8
EA JUL 2024
DT Article
PD FEB 2025
PY 2025
AB This study aimed to investigate the performance of a fine-tuned large
   language model (LLM) in extracting patients on pretreatment for lung
   cancer from picture archiving and communication systems (PACS) and
   comparing it with that of radiologists. Patients whose radiological
   reports contained the term lung cancer (3111 for training, 124 for
   validation, and 288 for test) were included in this retrospective study.
   Based on clinical indication and diagnosis sections of the radiological
   report (used as input data), they were classified into four groups (used
   as reference data): group 0 (no lung cancer), group 1 (pretreatment lung
   cancer present), group 2 (after treatment for lung cancer), and group 3
   (planning radiation therapy). Using the training and validation
   datasets, fine-tuning of the pretrained LLM was conducted ten times. Due
   to group imbalance, group 2 data were undersampled in the training. The
   performance of the best-performing model in the validation dataset was
   assessed in the independent test dataset. For testing purposes, two
   other radiologists (readers 1 and 2) were also involved in classifying
   radiological reports. The overall accuracy of the fine-tuned LLM, reader
   1, and reader 2 was 0.983, 0.969, and 0.969, respectively. The
   sensitivity for differentiating group 0/1/2/3 by LLM, reader 1, and
   reader 2 was 1.000/0.948/0.991/1.000, 0.750/0.879/0.996/1.000, and
   1.000/0.931/0.978/1.000, respectively. The time required for
   classification by LLM, reader 1, and reader 2 was 46s/2539s/1538s,
   respectively. Fine-tuned LLM effectively extracted patients on
   pretreatment for lung cancer from PACS with comparable performance to
   radiologists in a shorter time.
ZA 0
ZB 0
TC 6
ZS 0
ZR 0
Z8 0
Z9 6
DA 2024-07-10
UT WOS:001261213000001
PM 38955964
ER

PT J
AU Rajendran, Praveenbalaji
   Yang, Yong
   Niedermayr, Thomas R
   Gensheimer, Michael
   Beadle, Beth
   Le, Quynh-Thu
   Xing, Lei
   Dai, Xianjin
TI Large Language Model-Augmented Auto-Delineation of Treatment Target
   Volume in Radiation Therapy.
SO ArXiv
DT Journal Article; Preprint
PD 2024 Jul 10
PY 2024
AB Radiation therapy (RT) is one of the most effective treatments for
   cancer, and its success relies on the accurate delineation of targets.
   However, target delineation is a comprehensive medical decision that
   currently relies purely on manual processes by human experts. Manual
   delineation is time-consuming, laborious, and subject to interobserver
   variations. Although the advancements in artificial intelligence (AI)
   techniques have significantly enhanced the auto-contouring of normal
   tissues, accurate delineation of RT target volumes remains a challenge.
   In this study, we propose a visual language model-based RT target volume
   auto-delineation network termed Radformer. The Radformer utilizes a
   hierarchical vision transformer as the backbone and incorporates large
   language models to extract text-rich features from clinical data. We
   introduce a visual language attention module (VLAM) for integrating
   visual and linguistic features for language-aware visual encoding
   (LAVE). The Radformer has been evaluated on a dataset comprising 2985
   patients with head-and-neck cancer who underwent RT. Metrics, including
   the Dice similarity coefficient (DSC), intersection over union (IOU),
   and 95th percentile Hausdorff distance (HD95), were used to evaluate the
   performance of the model quantitatively. Our results demonstrate that
   the Radformer has superior segmentation performance compared to other
   state-of-the-art models, validating its potential for adoption in RT
   practice.
TC 0
ZS 0
ZR 0
ZA 0
ZB 0
Z8 0
Z9 0
DA 2024-07-24
UT MEDLINE:39040646
PM 39040646
ER

PT J
AU Timilsina, Mohan
   Buosi, Samuele
   Torrente, Maria
   Provencio, Mariano
   Cobo, Manuel
   Abreu, Delvys Rodriguez
   Castro, Rafael Lopez
   Carcereny, Enric
   Curry, Edward
   Novacek, Vit
TI Large language model vs. traditional machine learning: Evaluating
   predictive models for early detection of tumor relapse
SO EXPERT SYSTEMS WITH APPLICATIONS
VL 283
AR 127641
DI 10.1016/j.eswa.2025.127641
EA MAY 2025
DT Article
PD JUL 15 2025
PY 2025
AB In this study, we evaluate the effectiveness of foundational artificial
   intelligence (AI) models, particularly large language models (LLMs), in
   comparison to traditional machine learning methods for predicting tumor
   relapse in patients with non-small-cell lung cancer (NSCLC). With a high
   recurrence risk in NSCLC, early and accurate prediction is essential for
   improving patient outcomes and guiding treatment decisions. Our analysis
   utilizes a dataset of 1,348 patients, examining the performance of
   traditional machine learning models such as Random Forest, alongside
   cutting-edge LLMs like Mistral-7B, LLaMA-7B, Falcon-7B, and GPT-based
   models. While the Random Forest model slightly outperforms Mistral-7B in
   precision-recall for relapse prediction, the comparable results suggest
   that both approaches offer valuable insights for early relapse
   detection. This study underscores the potential of integrating classical
   machine learning with foundational AI models to enhance predictive
   accuracy in cancer prognosis, providing pathways for more personalized
   medical interventions.
TC 0
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
Z9 0
DA 2025-05-18
UT WOS:001487959200001
ER

PT J
AU Han, B.
   Chen, Y.
   Buyyounouski, M. K.
   Gensheimer, M. F.
   Xing, L.
TI RadAlonc: Enhancing Decision-Making in Radiation Oncology with a
   GPT-4-Based Prompt-Driven Large Language Model
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 2297
BP E134
EP E134
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
ZB 0
TC 0
Z8 0
ZR 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892300029
ER

PT C
AU Shieh, Alexander
   Paolucci, Iwan
   Albuquerque, Jessica
   Brock, Kristy
   Odisio, Bruno
BE Yoshida, H
   Wu, S
TI Feasibility of Extracting Critical Diagnostic Imaging Report Findings
   Following Percutaneous Liver Ablation with a Large Language Model
SO IMAGING INFORMATICS FOR HEALTHCARE, RESEARCH, AND APPLICATIONS, MEDICAL
   IMAGING 2024
SE Proceedings of SPIE
VL 12931
AR 1293104
DI 10.1117/12.3008791
DT Proceedings Paper
PD 2024
PY 2024
AB Percutaneous liver ablation is a minimally invasive procedure to treat
   liver tumors. Postablation images are highly significant as they
   distinguish normal post-procedure changes from abnormalities, preventing
   unnecessary retreatment and confirming procedural quality. However, the
   cancer surveillance imaging reports after the procedure can be numerous
   and challenging to read. Moreover, annotated data is limited in this
   setting. In this study we used the cutting-edge large language model
   Llama 2 to automatically extract critical findings from real-world
   diagnostic imaging reports without the need of training a new
   information extraction model. This could potentially automate part of
   the outcome research and registry construction process, as well as
   decrease the number of studies needed to review for research purposes. A
   dataset of 87 full-text reports from 13 patients who underwent
   percutaneous thermal ablation for pancreatic liver metastases were used
   to benchmark the capability of Llama 2 for cancer progression finding
   extraction and classification. We asked Llama 2 to determine whether
   there is cancer progression within the given report and then classify
   progression findings into local tumor progression (LTP), intrahepatic
   progression (IHP) and extrahepatic progression (EHP). Llama 2 achieved
   decent performance for detecting progression at study level. The
   precision is 0.91 and recall is 0.96, with specificity 0.84. However,
   the classification of progression into LTP, IHP and EHP still needs to
   be improved.
CT Conference on Medical Imaging - Imaging Informatics for Healthcare,
   Research, and Applications
CY FEB 19-21, 2024
CL San Diego, CA
SP SPIE; Amer Assoc Physicists Med; Radiol Soc N Amer; World Mol Imaging
   Soc; Soc Imaging Informat Med; Int Fdn Comp Assisted Radiol & Surg; Med
   Image Percept Soc
ZR 0
ZA 0
ZB 0
Z8 0
ZS 0
TC 0
Z9 0
DA 2024-05-31
UT WOS:001219280700003
ER

PT C
AU Chang, Chia-Hsuan
   Lucas, Mary M.
   Lee, Yeawon
   Yang, Christopher C.
   Lu-Yao, Grace
BE Finkelstein, J
   Moskovitch, R
   Parimbelli, E
TI Beyond Self-consistency: Ensemble Reasoning Boosts Consistency and
   Accuracy of LLMs in Cancer Staging
SO ARTIFICIAL INTELLIGENCE IN MEDICINE, PT I, AIME 2024
SE Lecture Notes in Artificial Intelligence
VL 14844
BP 224
EP 228
DI 10.1007/978-3-031-66538-7_23
DT Proceedings Paper
PD 2024
PY 2024
AB Pathologic cancer stage, crucial for treatment decisions, is often
   buried in unstructured pathology reports. This study investigates using
   pre-trained clinical LLMs for stage extraction, leveraging prompting
   techniques like chain-of-thought to enhance model transparency. While
   self-consistency methods further improve LLM performance, they can
   introduce inconsistencies in reasoning paths and predictions. We propose
   an ensemble reasoning approach, aiming for reliable cancer stage
   extraction. Utilizing an open-source clinical LLM on real-world reports,
   we demonstrate that the ensemble approach improves consistency and
   boosts performance, paving the way for utilizing LLMs in healthcare
   settings where reliability and interpretability are paramount.
CT 22nd International Conference on Artificial Intelligence in Medicine
   (AIME)
CY JUL 09-12, 2024
CL Salt Lake City, UT
ZB 1
TC 1
ZR 0
ZA 0
Z8 0
ZS 0
Z9 1
DA 2024-09-29
UT WOS:001295129500023
ER

PT J
AU Liang, Shufan
   Zhang, Jiangjiang
   Liu, Xingting
   Huang, Yinkui
   Shao, Jun
   Liu, Xiaohong
   Li, Weimin
   Wang, Guangyu
   Wang, Chengdi
TI The potential of large language models to advance precision oncology
SO EBIOMEDICINE
VL 115
AR 105695
DI 10.1016/j.ebiom.2025.105695
EA APR 2025
DT Review
PD MAY 2025
PY 2025
AB With the rapid development of artificial intelligence (AI) within
   medicine, the emergence of large language models (LLMs) has gradually
   reached the forefront of clinical research. In oncology, by mining the
   underlying connection between a text or image input and the desired
   output, LLMs demonstrate great potential for managing tumours. In this
   review, we provide a brief description of the development of LLMs,
   followed by model construction strategies and general medical functions.
   We then elaborate on the role of LLMs in cancer screening and diagnosis,
   metastasis identification, tumour staging, treatment recommendation, and
   documentation processing tasks by decoding various types of clinical
   data. Moreover, the current barriers faced by LLMs, such as
   hallucinations, ethical problems, limited application, and so on, are
   outlined along with corresponding solutions, where the further purpose
   is to inspire improvement and innovation in this field with respect to
   harnessing LLMs for advancing precision oncology. Copyright (c) 2025 The
   Author(s). Published by Elsevier B.V. This is an open access article
   under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).
TC 0
Z8 0
ZR 0
ZB 0
ZS 0
ZA 0
Z9 0
DA 2025-05-15
UT WOS:001485098500001
PM 40305985
ER

PT J
AU Agrawal, Anjali
TI Fairness in AI-Driven Oncology: Investigating Racial and Gender Biases
   in Large Language Models
SO CUREUS JOURNAL OF MEDICAL SCIENCE
VL 16
IS 9
AR e69541
DI 10.7759/cureus.69541
DT Article
PD SEP 16 2024
PY 2024
AB Introduction: Large language model (LLM) chatbots have many applications
   in medical settings. However, these tools can potentially perpetuate
   racial and gender biases through their responses, worsening disparities
   in healthcare. With the ongoing discussion of LLM chatbots in oncology
   and the widespread goal of addressing cancer disparities, this study
   focuses on biases propagated by LLM chatbots in oncology. Methods: Chat
   Generative Pre-trained Transformer (Chat GPT; OpenAI, San Francisco, CA,
   USA) was asked to determine what occupation a generic description of
   "assesses cancer patients" would correspond to for different
   demographics. Chat GPT, Gemini (Alphabet Inc., Mountain View, CA, USA),
   and Bing Chat (Microsoft Corp., Redmond, WA, USA) were prompted to
   provide oncologist recommendations in the top U.S. cities and
   demographic information (race, gender) of recommendations was compared
   against national distributions. Chat GPT was also asked to generate a
   job description for oncologists with different demographic backgrounds.
   Finally, Chat GPT, Gemini, and Bing Chat were asked to generate
   hypothetical cancer patients with race, smoking, and drinking histories.
   Results: LLM chatbots are about two times more likely to predict Blacks
   and Native Americans as oncology nurses than oncologists, compared to
   Asians (p < 0.01 and < 0.001, respectively). Similarly, they are also
   significantly more likely to predict females than males as oncology
   nurses (p < 0.001). Chat GPT's real-world oncologist recommendations
   overrepresent Asians by almost double and underrepresent Blacks by
   double and Hispanics by seven times. Chatbots also generate different
   job descriptions based on demographics, including cultural competency
   and advocacy and excluding treatment administration for underrepresented
   backgrounds. AI-generated cancer cases are not fully representative of
   real-world demographic distributions and encode stereotypes on substance
   abuse, such as Hispanics having a greater proportion of smokers than
   Whites by about 20% in Chat GPT breast cancer cases. Conclusion: To our
   knowledge, this is the first study of its kind to investigate racial and
   gender biases of such a diverse set of AI chatbots, and that too, within
   oncology. The methodology presented in this study provides a framework
   for targeted bias evaluation of LLMs in various fields across medicine.
ZB 0
Z8 0
ZA 0
TC 0
ZR 0
ZS 0
Z9 0
DA 2024-09-29
UT WOS:001318056600011
PM 39416584
ER

PT J
AU Hasei, Joe
   Hanzawa, Mana
   Nagano, Akihito
   Maeda, Naoko
   Yoshida, Shinichirou
   Endo, Makoto
   Yokoyama, Nobuhiko
   Ochi, Motoharu
   Ishida, Hisashi
   Katayama, Hideki
   Fujiwara, Tomohiro
   Nakata, Eiji
   Nakahara, Ryuichi
   Kunisada, Toshiyuki
   Tsukahara, Hirokazu
   Ozaki, Toshifumi
TI Empowering pediatric, adolescent, and young adult patients with cancer
   utilizing generative AI chatbots to reduce psychological burden and
   enhance treatment engagement: a pilot study
SO FRONTIERS IN DIGITAL HEALTH
VL 7
AR 1543543
DI 10.3389/fdgth.2025.1543543
DT Article
PD FEB 25 2025
PY 2025
AB Background Pediatric and adolescent/young adult (AYA) cancer patients
   face profound psychological challenges, exacerbated by limited access to
   continuous mental health support. While conventional therapeutic
   interventions often follow structured protocols, the potential of
   generative artificial intelligence (AI) chatbots to provide continuous
   conversational support remains unexplored. This study evaluates the
   feasibility and impact of AI chatbots in alleviating psychological
   distress and enhancing treatment engagement in this vulnerable
   population.Methods Two age-appropriate AI chatbots, leveraging GPT-4,
   were developed to provide natural, empathetic conversations without
   structured therapeutic protocols. Five pediatric and AYA cancer patients
   participated in a two-week intervention, engaging with the chatbots via
   a messaging platform. Pre- and post-intervention anxiety and stress
   levels were self-reported, and usage patterns were analyzed to assess
   the chatbots' effectiveness.Results Four out of five participants
   reported significant reductions in anxiety and stress levels
   post-intervention. Participants engaged with the chatbot every 2-3 days,
   with sessions lasting approximately 10 min. All participants noted
   improved treatment motivation, with 80% disclosing personal concerns to
   the chatbot they had not shared with healthcare providers. The 24/7
   availability particularly benefited patients experiencing nighttime
   anxiety.Conclusions This pilot study demonstrates the potential of
   generative AI chatbots to complement traditional mental health services
   by addressing unmet psychological needs in pediatric and AYA cancer
   patients. The findings suggest these tools can serve as accessible,
   continuous support systems. Further large-scale studies are warranted to
   validate these promising results.
Z8 0
ZB 0
ZA 0
ZS 0
TC 0
ZR 0
Z9 0
DA 2025-03-16
UT WOS:001441098700001
PM 40070545
ER

PT J
AU Liu, Xiaona
   Wang, Qing
   Zhou, Minghao
   Wang, Yanfei
   Wang, Xuefeng
   Zhou, Xiaobo
   Song, Qianqian
TI DrugFormer: Graph-Enhanced Language Model to Predict Drug Sensitivity
SO ADVANCED SCIENCE
VL 11
IS 40
DI 10.1002/advs.202405861
EA AUG 2024
DT Article
PD OCT 2024
PY 2024
AB Drug resistance poses a crucial challenge in healthcare, with response
   rates to chemotherapy and targeted therapy remaining low. Individual
   patient's resistance is exacerbated by the intricate heterogeneity of
   tumor cells, presenting significant obstacles to effective treatment. To
   address this challenge, DrugFormer, a novel graph-augmented large
   language model designed to predict drug resistance at single-cell level
   is proposed. DrugFormer integrates both serialized gene tokens and
   gene-based knowledge graphs for the accurate predictions of drug
   response. After training on comprehensive single-cell data with drug
   response information, DrugFormer model presents outperformance, with
   higher F1, precision, and recall in predicting drug response. Based on
   the scRNA-seq data from refractory multiple myeloma (MM) and acute
   myeloid leukemia (AML) patients, DrugFormer demonstrates high efficacy
   in identifying resistant cells and uncovering underlying molecular
   mechanisms. Through pseudotime trajectory analysisunique drug-resistant
   cellular states associated with poor patient outcomes are revealed.
   Furthermore, DrugFormer identifies potential therapeutic targets, such
   as COX8A, for overcoming drug resistance across different cancer types.
   In conclusion, DrugFormer represents a significant advancement in the
   field of drug resistance prediction, offering a powerful tool for
   unraveling the heterogeneity of cellular response to drugs and guiding
   personalized treatment strategies.
   DrugFormer, a novel graph-augmented language model, addresses the
   critical challenge of drug resistance in cancer treatment. By
   integrating serialized gene tokens and a gene-based knowledge graph, it
   provides accurate drug response predictions at the single-cell level.
   DrugFormer demonstrates superior performance and identifies resistant
   cells with potential therapeutic targets, offering a promising solution
   for personalized cancer therapy. image
Z8 0
ZA 0
ZS 0
ZB 1
ZR 0
TC 9
Z9 9
DA 2024-09-02
UT WOS:001300001500001
PM 39206872
ER

PT C
AU Gao, Welting
   Gao, Xiangyu
   Chen, Wenjin
   Foran, David J.
   Chen, Yi
BE Yang, DN
   Xie, X
   Tseng, VS
   Pei, J
   Huang, JW
   Lin, JCW
TI BioReX: Biomarker Information Extraction Inspired by Aspect-Based
   Sentiment Analysis
SO ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PT IV, PAKDD 2024
SE Lecture Notes in Artificial Intelligence
VL 14648
BP 129
EP 141
DI 10.1007/978-981-97-2238-9_10
DT Proceedings Paper
PD 2024
PY 2024
AB Biomarkers are critical in cancer diagnosis, prognosis, and treatment
   planning. However, this information is often buried in unstructured text
   form. In this paper, we make an analogy between Biomarker Information
   Extraction and Aspect-Based Sentiment Analysis. We propose a system,
   Biomarker and Result Extraction Model (BioReX). BioReX employs BERT
   post-training methods to augment the BioBERT model with domain-specific
   and task-specific knowledge for biomarker extraction. It uses
   syntactic-based and semantic-based attention to associate results to
   corresponding biomarkers. Evaluation demonstrates the effectiveness of
   the proposed approach.
CT 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining
   (PAKDD)
CY MAY 07-10, 2024
CL Taipei, TAIWAN
ZS 0
ZA 0
Z8 0
ZB 0
TC 0
ZR 0
Z9 0
DA 2024-09-04
UT WOS:001275770400010
ER

PT J
AU Hermann, Catherine E.
   Patel, Jharna M.
   Boyd, Leslie
   Aviki, Emeline
   Stasenko, Marina
TI Let's chat about cervical cancer: Assessing the accuracy of ChatGPT
   responses to cervical cancer questions
SO GYNECOLOGIC ONCOLOGY
VL 179
BP 164
EP 168
DI 10.1016/j.ygyno.2023.11.008
EA NOV 2023
DT Article
PD DEC 2023
PY 2023
AB Objective. To quantify the accuracy of ChatGPT in answering commonly
   asked questions pertaining to cervical cancer prevention, diagnosis,
   treatment, and survivorship/quality-of-life (QOL). Methods. ChatGPT was
   queried with 64 questions adapted from professional society websites and
   the au-thors' clinical experiences. The answers were scored by two
   attending Gynecologic Oncologists according to the following scale: 1)
   correct and comprehensive, 2) correct but not comprehensive, 3) some
   correct, some in-correct, and 4) completely incorrect. Scoring
   discrepancies were resolved by additional reviewers as needed. The
   proportion of responses earning each score were calculated overall and
   within each question category.Results. ChatGPT provided correct and
   comprehensive answers to 34 (53.1%) questions, correct but not
   com-prehensive answers to 19 (29.7%) questions, partially incorrect
   answers to 10 (15.6%) questions, and completely incorrect answers to 1
   (1.6%) question. Prevention and survivorship/QOL had the highest
   proportion of "correct" scores (scores of 1 or 2) at 22/24 (91.7%) and
   15/16 (93.8%), respectively. ChatGPT performed less well in the
   treatment category, with 15/21 (71.4%) correct scores. It performed the
   worst in the diagnosis category with only 1/3 (33.3%) correct
   scores.Conclusion. ChatGPT accurately answers questions about cervical
   cancer prevention, survivorship, and QOL. It performs less accurately
   for cervical cancer diagnosis and treatment. Further development of this
   immensely popular large language model should include physician input
   before it can be utilized as a tool for Gynecologists or recommended as
   a patient resource for information on cervical cancer diagnosis and
   treatment.(c) 2023 Elsevier Inc. All rights reserved.
TC 21
ZR 0
ZA 0
Z8 0
ZB 6
ZS 0
Z9 21
DA 2023-12-23
UT WOS:001122497400001
PM 37988948
ER

PT J
AU Piao, Ying
   Chen, Hongtao
   Wu, Shihai
   Li, Xianming
   Li, Zihuang
   Yang, Dong
TI Assessing the performance of large language models (LLMs) in answering
   medical questions regarding breast cancer in the Chinese context
SO DIGITAL HEALTH
VL 10
AR 20552076241284771
DI 10.1177/20552076241284771
DT Article
PD 2024
PY 2024
AB Purpose Large language models (LLMs) are deep learning models designed
   to comprehend and generate meaningful responses, which have gained
   public attention in recent years. The purpose of this study is to
   evaluate and compare the performance of LLMs in answering questions
   regarding breast cancer in the Chinese context. Material and Methods
   ChatGPT, ERNIE Bot, and ChatGLM were chosen to answer 60 questions
   related to breast cancer posed by two oncologists. Responses were scored
   as comprehensive, correct but inadequate, mixed with correct and
   incorrect data, completely incorrect, or unanswered. The accuracy,
   length, and readability among answers from different models were
   evaluated using statistical software. Results ChatGPT answered 60
   questions, with 40 (66.7%) comprehensive answers and six (10.0%) correct
   but inadequate answers. ERNIE Bot answered 60 questions, with 34 (56.7%)
   comprehensive answers and seven (11.7%) correct but inadequate answers.
   ChatGLM generated 60 answers, with 35 (58.3%) comprehensive answers and
   six (10.0%) correct but inadequate answers. The differences for chosen
   accuracy metrics among the three LLMs did not reach statistical
   significance, but only ChatGPT demonstrated a sense of human compassion.
   The accuracy of the three models in answering questions regarding breast
   cancer treatment was the lowest, with an average of 44.4%. ERNIE Bot's
   responses were significantly shorter compared to ChatGPT and ChatGLM (p
   < .001 for both). The readability scores of the three models showed no
   statistical significance. Conclusions In the Chinese context, the
   capabilities of ChatGPT, ERNIE Bot, and ChatGLM are similar in answering
   breast cancer-related questions at present. These three LLMs may serve
   as adjunct informational tools for breast cancer patients in the Chinese
   context, offering guidance for general inquiries. However, for highly
   specialized issues, particularly in the realm of breast cancer
   treatment, LLMs cannot deliver reliable performance. It is necessary to
   utilize them under the supervision of healthcare professionals.
ZA 0
Z8 0
ZS 0
ZB 1
ZR 0
TC 4
Z9 4
DA 2024-10-18
UT WOS:001331508800001
PM 39386109
ER

PT J
AU Zhu, L.
   Anand, A.
   Gevorkyan, G.
   Mcgee, L. A.
   Rwigema, J. C.
   Rong, Y.
   Patel, S. H.
TI Testing and Validation of a Custom Trained Large Language Model for HN
   Patients with Guardrails
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 118
IS 5
MA 182
BP E52
EP E53
DT Meeting Abstract
PD APR 1 2024
PY 2024
CT Multidisciplinary Head and Neck Cancers Symposium
CY FEB 29-MAR 02, 2024
CL Phoenix, AZ
TC 1
ZS 0
Z8 0
ZA 0
ZR 0
ZB 0
Z9 1
DA 2024-10-18
UT WOS:001300212900102
ER

PT J
AU Geraci, Joseph
   Qorri, Bessi
   Tsay, Mike
   Cumbaa, Christian
   Leonchyk, Paul
   Alphs, Larry
   Pani, Luca
TI An AI approach to unraveling treatment response in pancreatic cancer:
   Insights from the COMPASS trial leveraging large language models (LLMs)
SO CANCER RESEARCH
VL 84
IS 17
MA B066
DI 10.1158/1538-7445.PANCREATIC24-B066
SU 2
DT Meeting Abstract
PD SEP 1 2024
PY 2024
CT AACR Special Conference in Cancer Research: Advances in Pancreatic
   Cancer Research
CY SEP 15-18, 2024
CL Boston, MA
SP Amer Assoc Cancer Res
ZB 0
TC 0
Z8 0
ZR 0
ZS 0
ZA 0
Z9 0
DA 2024-09-29
UT WOS:001317590000140
ER

PT C
AU Kim, Kyungwon
   Lee, Yongmoon
   Park, Doohyun
   Eo, Taejoon
   Youn, Daemyung
   Lee, Hyesang
   Hwang, Dosik
BE Feragen, A
   Giannarou, S
   Glocker, B
   Lekadir, K
   Schnabel, JA
   Linguraru, MG
   Dou, Q
TI LLM-Guided Multi-modal Multiple Instance Learning for 5-Year Overall
   Survival Prediction of Lung Cancer
SO MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI
   2024, PT III
SE Lecture Notes in Computer Science
VL 15003
BP 239
EP 249
DI 10.1007/978-3-031-72384-1_23
DT Proceedings Paper
PD 2024
PY 2024
AB Accurately predicting the 5-year prognosis of lung cancer patients is
   crucial for guiding treatment planning and providing optimal patient
   care. Traditional methods relying on CT image-based cancer stage
   assessment and morphological analysis of cancer cells in pathology
   images have encountered challenges in terms of reliability and accuracy
   due to the complexity and diversity of information within these images.
   Recent rapid advancements in deep learning have shown promising
   performance in prognosis prediction, however utilizing CT and pathology
   images independently is limited by their differing imaging
   characteristics and the unique prognostic information. To effectively
   address these challenges, this study proposes a novel framework that
   integrates prognostic capabilities of both CT and pathology images with
   clinical information, employing a multi-modal integration approach via
   multiple instance learning, leveraging large language models (LLMs) to
   analyze clinical notes and align them with image modalities. The
   proposed approach was rigorously validated using external datasets from
   different hospitals, demonstrating superior performance over models
   reliant on vision or clinical data alone. This highlights the
   adaptability and strength of LLMs in managing complex multi-modal
   medical datasets for lung cancer prognosis, marking a significant
   advance towards more accurate and comprehensive patient care strategies.
   The code is publicly available on
   https://github.com/KyleKWKim/LLM-guided-Multimodal-MIL.
CT 27th International Conference on Medical Image Computing and Computer
   Assisted Intervention (MICCAI)
CY OCT 06-10, 2024
CL Palmeraie Conf Ctr, Marrakesh, MOROCCO
HO Palmeraie Conf Ctr
SP GH Labs; Childrens Natl Hosp; Pierre Fabre; Comp Assisted Med Intervent
   Labex; Multidisciplinary Inst Artificial Intelligence Grenoble Alpes;
   Western Univ, Frugal Biomed Innovat Program; Int Soc Radiol; Medtronic;
   Pasqual Maragall Fdn; Delft Imaging; Univ Barcelona, Artificial
   Intelligence Med Lab; Cadi Ayyad Univ; Natl Ctr Sci & Tech Res
ZR 0
Z8 0
ZB 0
ZS 0
TC 1
ZA 0
Z9 1
DA 2024-11-28
UT WOS:001342227700023
ER

PT J
AU Mora, J.
   Chen, S.
   Mak, R. H.
   Bitterman, D. S.
TI Cancer Treatment Information Differences by Bilingual Prompting in Large
   Language Model Chatbots
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3415
BP E645
EP E646
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
ZB 0
TC 0
ZA 0
ZR 0
Z8 0
Z9 0
DA 2024-12-16
UT WOS:001325892302096
ER

PT J
AU Moulson, Ruth
   Law, Jennifer
   Sacher, Adrian
   Liu, Geoffrey
   Shepherd, Frances A.
   Bradbury, Penelope
   Eng, Lawson
   Iczkovitz, Sandra
   Abbie, Erica
   Elia-Pacitti, Julia
   Ewara, Emmanuel M.
   Mokriak, Viktoriia
   Weiss, Jessica
   Pettengell, Christopher
   Leighl, Natasha B.
TI Real-World Outcomes of Patients with Advanced Epidermal Growth Factor
   Receptor-Mutated Non-Small Cell Lung Cancer in Canada Using Data
   Extracted by Large Language Model-Based Artificial Intelligence
SO CURRENT ONCOLOGY
VL 31
IS 4
BP 1947
EP 1960
DI 10.3390/curroncol31040146
DT Article
PD APR 2024
PY 2024
AB Real-world evidence for patients with advanced EGFR-mutated non-small
   cell lung cancer (NSCLC) in Canada is limited. This study's objective
   was to use previously validated DARWENTM artificial intelligence (AI) to
   extract data from electronic heath records of patients with non-squamous
   NSCLC at University Health Network (UHN) to describe EGFR mutation
   prevalence, treatment patterns, and outcomes. Of 2154 patients with
   NSCLC, 613 had advanced disease. Of these, 136 (22%) had common
   sensitizing EGFR mutations (cEGFRm; ex19del, L858R), 8 (1%) had exon 20
   insertions (ex20ins), and 338 (55%) had EGFR wild type. One-year overall
   survival (OS) (95% CI) for patients with cEGFRm, ex20ins, and EGFR wild
   type tumours was 88% (83, 94), 100% (100, 100), and 59% (53, 65),
   respectively. In total, 38% patients with ex20ins received experimental
   ex20ins targeting treatment as their first-line therapy. A total of 57
   patients (36%) with cEGFRm received osimertinib as their first-line
   treatment, and 61 (39%) received it as their second-line treatment.
   One-year OS (95% CI) following the discontinuation of osimertinib was
   35% (17, 75) post-first-line and 20% (9, 44) post-second-line. In this
   real-world AI-generated dataset, survival post-osimertinib was poor in
   patients with cEGFR mutations. Patients with ex20ins in this cohort had
   improved outcomes, possibly due to ex20ins targeting treatment,
   highlighting the need for more effective treatments for patients with
   advanced EGFRm NSCLC.
ZS 0
ZR 0
ZA 0
ZB 0
Z8 0
TC 2
Z9 2
DA 2024-06-06
UT WOS:001236994300001
PM 38668049
ER

PT J
AU Marchi, Filippo
   Bellini, Elisa
   Iandelli, Andrea
   Sampieri, Claudio
   Peretti, Giorgio
TI Exploring the landscape of AI-assisted decision-making in head and neck
   cancer treatment: a comparative analysis of NCCN guidelines and ChatGPT
   responses
SO EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY
VL 281
IS 4
BP 2123
EP 2136
DI 10.1007/s00405-024-08525-z
EA FEB 2024
DT Article
PD APR 2024
PY 2024
AB PurposeRecent breakthroughs in natural language processing and machine
   learning, exemplified by ChatGPT, have spurred a paradigm shift in
   healthcare. Released by OpenAI in November 2022, ChatGPT rapidly gained
   global attention. Trained on massive text datasets, this large language
   model holds immense potential to revolutionize healthcare. However,
   existing literature often overlooks the need for rigorous validation and
   real-world applicability.MethodsThis head-to-head comparative study
   assesses ChatGPT's capabilities in providing therapeutic recommendations
   for head and neck cancers. Simulating every NCCN Guidelines scenarios.
   ChatGPT is queried on primary treatments, adjuvant treatment, and
   follow-up, with responses compared to the NCCN Guidelines. Performance
   metrics, including sensitivity, specificity, and F1 score, are employed
   for assessment.ResultsThe study includes 68 hypothetical cases and 204
   clinical scenarios. ChatGPT exhibits promising capabilities in
   addressing NCCN-related queries, achieving high sensitivity and overall
   accuracy across primary treatment, adjuvant treatment, and follow-up.
   The study's metrics showcase robustness in providing relevant
   suggestions. However, a few inaccuracies are noted, especially in
   primary treatment scenarios.ConclusionOur study highlights the
   proficiency of ChatGPT in providing treatment suggestions. The model's
   alignment with the NCCN Guidelines sets the stage for a nuanced
   exploration of AI's evolving role in oncological decision support.
   However, challenges related to the interpretability of AI in clinical
   decision-making and the importance of clinicians understanding the
   underlying principles of AI models remain unexplored. As AI continues to
   advance, collaborative efforts between models and medical experts are
   deemed essential for unlocking new frontiers in personalized cancer
   care.
ZB 4
ZA 0
TC 18
Z8 0
ZR 0
ZS 0
Z9 18
DA 2024-04-24
UT WOS:001172712200001
PM 38421392
ER

PT J
AU Liu, Hui
   Peng, Jialun
   Li, Lu
   Deng, Ao
   Huang, XiangXin
   Yin, Guobing
   Luo, Haojun
TI Large Language Models as a Consulting Hotline for Patients With Breast
   Cancer and Specialists in China: Cross-Sectional Questionnaire Study
SO JMIR MEDICAL INFORMATICS
VL 13
AR e66429
DI 10.2196/66429
DT Article
PD 2025
PY 2025
AB Background: The disease burden of breast cancer is increasing in China.
   Guiding people to obtain accurate information on breast cancer and
   improving the public's health literacy are crucial for the early
   detection and timely treatment of breast cancer. Large language model
   (LLM) is a currently popular source of health information. However, the
   accuracy and practicality of the breast cancer-related information
   provided by LLMs have not yet been evaluated. Objective: This study aims
   to evaluate and compare the accuracy, practicality, and
   generalization-specificity of responses to breast cancer-related
   questions from two LLMs, ChatGPT and ERNIE Bot (EB). Methods: The
   questions asked to the LLMs consisted of a patient questionnaire and an
   expert questionnaire, each containing 15 questions. ChatGPT was queried
   in both Chinese and English, recorded as ChatGPT-Chinese (ChatGPT-C) and
   ChatGPT-English (ChatGPT-E) respectively, while EB was queried in
   Chinese. The accuracy, practicality, and generalization-specificity of
   each inquiry's responses were rated by a breast cancer multidisciplinary
   treatment team using Likert scales. Results: Overall, for both the
   patient and expert questionnaire, the accuracy and practicality of
   responses from ChatGPT-E were significantly higher than those from
   ChatGPT-C and EB (all Ps<.001). However, the responses from all LLMs are
   relatively generalized, leading to lower accuracy and practicality for
   the expert questionnaire compared to the patient questionnaire.
   Additionally, there were issues such as the lack of supporting evidence
   and potential ethical risks in the responses of LLMs. Conclusions:
   Currently, compared to other LLMs, ChatGPT-E has demonstrated greater
   potential for application in educating Chinese patients with breast
   cancer, and may serve as an effective tool for them to obtain health
   information. However, for breast cancer specialists, these LLMs are not
   yet suitable for assisting in clinical diagnosis or treatment
   activities. Additionally, data security, ethical, and legal risks
   associated with using LLMs in clinical practice cannot be ignored. In
   the future, further research is needed to determine the true efficacy of
   LLMs in clinical scenarios related to breast cancer in China.
TC 0
Z8 0
ZB 0
ZA 0
ZR 0
ZS 0
Z9 0
DA 2025-06-14
UT WOS:001506204800004
PM 40424585
ER

PT J
AU Alkhnbashi, Omer S.
   Mohammad, Rasheed
   Hammoudeh, Mohammad
TI Aspect-Based Sentiment Analysis of Patient Feedback Using Large Language
   Models
SO BIG DATA AND COGNITIVE COMPUTING
VL 8
IS 12
AR 167
DI 10.3390/bdcc8120167
DT Article
PD DEC 2024
PY 2024
AB Online medical forums have emerged as vital platforms for patients to
   share their experiences and seek advice, providing a valuable,
   cost-effective source of feedback for medical service management. This
   feedback not only measures patient satisfaction and improves health
   service quality but also offers crucial insights into the effectiveness
   of medical treatments, pain management strategies, and alternative
   therapies. This study systematically identifies and categorizes key
   aspects of patient experiences, emphasizing both positive and negative
   sentiments expressed in their narratives. We collected a dataset of
   approximately 15,000 entries from various sections of the widely used
   medical forum, patient.info. Our innovative approach integrates content
   analysis with aspect-based sentiment analysis, deep learning techniques,
   and a large language model (LLM) to analyze these data. Our methodology
   is designed to uncover a wide range of aspect types reflected in patient
   feedback. The analysis revealed seven distinct aspect types prevalent in
   the feedback, demonstrating that deep learning models can effectively
   predict these aspect types and their corresponding sentiment values.
   Notably, the LLM with few-shot learning outperformed other models. Our
   findings enhance the understanding of patient experiences in online
   forums and underscore the utility of advanced analytical techniques in
   extracting meaningful insights from unstructured patient feedback,
   offering valuable implications for healthcare providers and medical
   service management.
ZR 0
ZB 0
TC 1
ZA 0
Z8 0
ZS 0
Z9 1
DA 2025-01-09
UT WOS:001389594900001
ER

PT J
AU Hao, Yuexing
   Holmes, Jason
   Hobson, Jared
   Bennett, Alexandra
   McKone, Elizabeth L
   Ebner, Daniel K
   Routman, David M
   Shiraishi, Satomi
   Patel, Samir H
   Yu, Nathan Y
   Hallemeier, Chris L
   Ball, Brooke E
   Waddle, Mark
   Liu, Wei
TI Retrospective Comparative Analysis of Prostate Cancer In-Basket
   Messages: Responses From Closed-Domain Large Language Models Versus
   Clinical Teams.
SO Mayo Clinic proceedings. Digital health
VL 3
IS 1
DI 10.1016/j.mcpdig.2025.100198
DT Journal Article
PD 2025-Mar
PY 2025
AB Objective: To evaluate the effectiveness of RadOnc-generative pretrained
   transformer (GPT), a GPT-4 based large language model, in assisting with
   in-basket message response generation for prostate cancer treatment,
   with the goal of reducing the workload and time on clinical care teams
   while maintaining response quality.
   Patients and Methods: RadOnc-GPT was integrated with electronic health
   records from both Mayo Clinic-wide databases and a
   radiation-oncology-specific database. The model was evaluated on 158
   previously recorded in-basket message interactions, selected from 90
   patients with nonmetastatic prostate cancer from the Mayo Clinic
   Department of Radiation Oncology in-basket message database in the
   calendar years 2022-2024. Quantitative natural language processing
   analysis and 2 grading studies, conducted by 5 clinicians and 4 nurses,
   were used to assess RadOnc-GPT's responses. Three primary clinicians
   independently graded all messages, whereas a fourth senior clinician
   reviewed 41 responses with relevant discrepancies, and a fifth senior
   clinician evaluated 2 additional responses. The grading focused on 5 key
   areas: completeness, correctness, clarity, empathy, and editing time.
   The grading study was performed from July 20, 2024 to December 15, 2024.
   Results: The RadOnc-GPT slightly outperformed the clinical care team in
   empathy, whereas achieving comparable scores with the clinical care team
   in completeness, correctness, and clarity. Five clinician graders
   identified key limitations in RadOnc-GPT's responses, such as lack of
   context, insufficient domain-specific knowledge, inability to perform
   essential meta-tasks, and hallucination. It was estimated that
   RadOnc-GPT could save an average of 5.2 minutes per message for nurses
   and 2.4 minutes for clinicians, from reading the inquiry to sending the
   response.
   Conclusion: RadOnc-GPT has the potential to considerably reduce the
   workload of clinical care teams by generating high-quality, timely
   responses for in-basket message interactions. This could lead to
   improved efficiency in health care workflows and reduced costs while
   maintaining or enhancing the quality of communication between patients
   and health care providers.Abbreviations and AcronymsAI; artificial
   intelligence; LLM; large language model; NLP; natural language
   processing; RadOnc-GPT; radiation oncology generative pretrained
   transformer.
ZA 0
ZR 0
Z8 0
ZB 0
TC 0
ZS 0
Z9 0
DA 2025-03-28
UT MEDLINE:40130001
PM 40130001
ER

PT J
AU Griewing, Sebastian
   Lechner, Fabian
   Gremke, Niklas
   Lukac, Stefan
   Janni, Wolfgang
   Wallwiener, Markus
   Wagner, Uwe
   Hirsch, Martin
   Kuhn, Sebastian
TI Proof-of-concept study of a small language model chatbot for breast
   cancer decision support - a transparent, source-controlled, explainable
   and data-secure approach
SO JOURNAL OF CANCER RESEARCH AND CLINICAL ONCOLOGY
VL 150
IS 10
AR 451
DI 10.1007/s00432-024-05964-3
DT Article
PD OCT 9 2024
PY 2024
AB Purpose Large language models (LLM) show potential for decision support
   in breast cancer care. Their use in clinical care is currently
   prohibited by lack of control over sources used for decision-making,
   explainability of the decision-making process and health data security
   issues. Recent development of Small Language Models (SLM) is discussed
   to address these challenges. This preclinical proof-of-concept study
   tailors an open-source SLM to the German breast cancer guideline
   (BC-SLM) to evaluate initial clinical accuracy and technical
   functionality in a preclinical simulation. Methods A multidisciplinary
   tumor board (MTB) is used as the gold-standard to assess the initial
   clinical accuracy in terms of concordance of the BC-SLM with MTB and
   comparing it to two publicly available LLM, ChatGPT3.5 and 4. The study
   includes 20 fictional patient profiles and recommendations for 5
   treatment modalities, resulting in 100 binary treatment recommendations
   (recommended or not recommended). Statistical evaluation includes
   concordance with MTB in % including Cohen's Kappa statistic (kappa).
   Technical functionality is assessed qualitatively in terms of local
   hosting, adherence to the guideline and information retrieval. Results
   The overall concordance amounts to 86% for BC-SLM (kappa = 0.721, p <
   0.001), 90% for ChatGPT4 (kappa = 0.820, p < 0.001) and 83% for
   ChatGPT3.5 (kappa = 0.661, p < 0.001). Specific concordance for each
   treatment modality ranges from 65 to 100% for BC-SLM, 85-100% for
   ChatGPT4, and 55-95% for ChatGPT3.5. The BC-SLM is locally functional,
   adheres to the standards of the German breast cancer guideline and
   provides referenced sections for its decision-making. Conclusion The
   tailored BC-SLM shows initial clinical accuracy and technical
   functionality, with concordance to the MTB that is comparable to
   publicly-available LLMs like ChatGPT4 and 3.5. This serves as a
   proof-of-concept for adapting a SLM to an oncological disease and its
   guideline to address prevailing issues with LLM by ensuring decision
   transparency, explainability, source control, and data security, which
   represents a necessary step towards clinical validation and safe use of
   language models in clinical oncology.
ZR 0
ZA 0
Z8 0
TC 1
ZB 1
ZS 0
Z9 1
DA 2024-10-24
UT WOS:001335902900001
PM 39382778
ER

PT J
AU Liu, Jialin
   Wang, Changyu
   Liu, Siru
TI Utility of ChatGPT in Clinical Practice
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 25
AR e48568
DI 10.2196/48568
DT Article
PD JUN 28 2023
PY 2023
AB ChatGPT is receiving increasing attention and has a variety of
   application scenarios in clinical practice. In clinical decision
   support, ChatGPT has been used to generate accurate differential
   diagnosis lists, support clinical decision-making, optimize clinical
   decision support, and provide insights for cancer screening decisions.
   In addition, ChatGPT has been used for intelligent question-answering to
   provide reliable information about diseases and medical queries. In
   terms of medical documentation, ChatGPT has proven effective in
   generating patient clinical letters, radiology reports, medical notes,
   and discharge summaries, improving efficiency and accuracy for health
   care providers. Future research directions include real-time monitoring
   and predictive analytics, precision medicine and personalized treatment,
   the role of ChatGPT in telemedicine and remote health care, and
   integration with existing health care systems. Overall, ChatGPT is a
   valuable tool that complements the expertise of health care providers
   and improves clinical decision-making and patient care. However, ChatGPT
   is a double-edged sword. We need to carefully consider and study the
   benefits and potential dangers of ChatGPT. In this viewpoint, we discuss
   recent advances in ChatGPT research in clinical practice and suggest
   possible risks and challenges of using ChatGPT in clinical practice. It
   will help guide and support future artificial intelligence research
   similar to ChatGPT in health.
ZS 1
ZR 0
Z8 5
TC 230
ZB 25
ZA 0
Z9 234
DA 2023-08-24
UT WOS:001045687800005
PM 37379067
ER

PT J
AU Liu, Sheng
   Pastor-Serrano, Oscar
   Chen, Yizheng
   Gopaulchan, Matthew
   Liang, Weixing
   Buyyounouski, Mark
   Pollom, Erqi
   Le, Quynh-Thu
   Gensheimer, Michael
   Dong, Peng
   Yang, Yong
   Zou, James
   Xing, Lei
TI Automated radiotherapy treatment planning guided by GPT-4Vision.
SO ArXiv
DT Journal Article; Preprint
PD 2024 Jul 01
PY 2024
AB Radiotherapy treatment planning is a time-consuming and potentially
   subjective process that requires the iterative adjustment of model
   parameters to balance multiple conflicting objectives. Recent
   advancements in large foundation models offer promising avenues for
   addressing the challenges in planning and clinical decision-making. This
   study introduces GPT-RadPlan, a fully automated treatment planning
   framework that harnesses prior radiation oncology knowledge encoded in
   multi-modal large language models, such as GPT-4Vision (GPT-4V) from
   OpenAI. GPT-RadPlan is made aware of planning protocols as context and
   acts as an expert human planner, capable of guiding a treatment planning
   process. Via in-context learning, we incorporate clinical protocols for
   various disease sites as prompts to enable GPT-4V to acquire treatment
   planning domain knowledge. The resulting GPT-RadPlan agent is integrated
   into our in-house inverse treatment planning system through an API. The
   efficacy of the automated planning system is showcased using multiple
   prostate and head & neck cancer cases, where we compared GPT-RadPlan
   results to clinical plans. In all cases, GPT-RadPlan either outperformed
   or matched the clinical plans, demonstrating superior target coverage
   and organ-at-risk sparing. Consistently satisfying the dosimetric
   objectives in the clinical protocol, GPT-RadPlan represents the first
   multimodal large language model agent that mimics the behaviors of human
   planners in radiation oncology clinics, achieving remarkable results in
   automating the treatment planning process without the need for
   additional training.
TC 0
ZR 0
ZS 0
ZA 0
Z8 0
ZB 0
Z9 0
DA 2024-07-18
UT MEDLINE:39010876
PM 39010876
ER

PT J
AU Ghorbian, Mohsen
   Ghobaei-Arani, Mostafa
   Ghorbian, Saied
TI Transforming breast cancer diagnosis and treatment with large language
   Models: A comprehensive survey
SO METHODS
VL 239
BP 85
EP 110
DI 10.1016/j.ymeth.2025.04.001
EA APR 2025
DT Article
PD JUL 2025
PY 2025
AB Breast cancer (BrCa), being one of the most prevalent forms of cancer in
   women, poses many challenges in the field of treatment and diagnosis due
   to its complex biological mechanisms. Early and accurate diagnosis plays
   a fundamental role in improving survival rates, but the limitations of
   existing imaging methods and clinical data interpretation often prevent
   optimal results. Large Language Models (LLMs), which are developed based
   on advanced architectures such as transformers, have brought about a
   significant revolution in data processing and medical decision-making.
   By analyzing a large volume of medical and clinical data, these models
   enable early diagnosis by identifying patterns in images and medical
   records and provide personalized treatment strategies by integrating
   genetic markers and clinical guidelines. Despite the transformative
   potential of these models, their use in BrCa management faces challenges
   such as data sensitivity, algorithm transparency, ethical
   considerations, and model compatibility with the details of medical
   applications that need to be addressed to achieve reliable results. This
   review systematically reviews the impact of LLMs on BrCa treatment and
   diagnosis. This study's objectives include analyzing the role of LLM
   technology in diagnosing and treating this disease. The findings
   indicate that the application of LLMs has resulted in significant
   improvements in various aspects of BrCa management, such as a 35%
   increase in the Efficiency of Diagnosis and BrCa Treatment (EDBC), a 30%
   enhancement in the System's Clinical Trust and Reliability (SCTR), and a
   20% improvement in the quality of patient education and information
   (IPEI). Ultimately, this study demonstrates the importance of LLMs in
   advancing precision medicine for BrCa and paves the way for effective
   patient-centered care solutions.
ZS 0
ZR 0
TC 0
ZB 0
ZA 0
Z8 0
Z9 0
DA 2025-04-20
UT WOS:001466448900001
PM 40199412
ER

PT J
AU Anonymous
TI Meeting of the Anaesthetic-Research-Society, London, UK, May 16 -17,
   2024 
SO British Journal of Anaesthesia
VL 133
IS 2
BP 458
EP 472
DT Meeting
PD AUG 2024
PY 2024
AB This "Abstracts from Anesthetic Research Society Meeting", which focuses
   on different anesthesia treatments to patient during various treatment
   interventions like surgery or other diagnostic or therapeutic
   procedures, contains approximately 23 abstract presentations, written in
   English. Topics include local anaesthetic treatment, cancer surgery,
   perioperative management, cell apoptosis, cell proliferation, general
   anaesthesia, colorectal cancer, quality-of-life, length of hospital
   stay, patient-reported ethnicity, postpartum hemorrhage. Other topics
   include large language model, hallucination, questionnaire,
   perioperative medication advice, proteomic analysis, lung resection,
   cardiac magnetic resonance imaging, extracellular volume, plasma
   protein, lung protective ventilation, conventional ventilation,
   postoperative pulmonary complication, major noncardiac surgery:,
   myocardial inflammation.
CT Meeting of the Anaesthetic-Research-Society
CY May 16 -17, 2024
CL London, UK
HO London, UK
SP Anaesthet Res Soc
Z8 0
TC 0
ZA 0
ZB 0
ZR 0
ZS 0
Z9 0
DA 2024-08-30
UT BCI:BCI202400741698
ER

PT J
AU Wang, Zhixiang
   Zhang, Zhen
   Traverso, Alberto
   Dekker, Andre
   Qian, Linxue
   Sun, Pengfei
TI Assessing the role of GPT-4 in thyroid ultrasound diagnosis and
   treatment recommendations: enhancing interpretability with a chain of
   thought approach
SO QUANTITATIVE IMAGING IN MEDICINE AND SURGERY
VL 14
IS 2
DI 10.21037/qims-23-1180
EA JAN 2024
DT Article
PD FEB 2024
PY 2024
AB Background: As artificial intelligence (AI) becomes increasingly
   prevalent in the medical field, the effectiveness of AI-generated
   medical reports in disease diagnosis remains to be evaluated. ChatGPT is
   a large language model developed by open AI with a notable capacity for
   text abstraction and comprehension. This study aimed to explore the
   capabilities, limitations, and potential of Generative Pre-trained
   Transformer (GPT)-4 in analyzing thyroid cancer ultrasound reports,
   providing diagnoses, and recommending treatment plans. Methods: Using
   109 diverse thyroid cancer cases, we evaluated GPT-4's performance by
   comparing its generated reports to those from doctors with various
   levels of experience. We also conducted a Turing Test and a consistency
   analysis. To enhance the interpretability of the model, we applied the
   Chain of Thought (CoT) method to deconstruct the decision-making chain
   of the GPT model. Results: GPT-4 demonstrated proficiency in report
   structuring, professional terminology, and clarity of expression, but
   showed limitations in diagnostic accuracy. In addition, our consistency
   analysis highlighted certain discrepancies in the AI's performance. The
   CoT method effectively enhanced the interpretability of the AI's
   decision-making process. Conclusions: GPT-4 exhibits potential as a
   supplementary tool in healthcare, especially for generating thyroid
   gland diagnostic reports. Our proposed online platform, "ThyroAIGuide",
   alongside the CoT method, underscores the potential of AI to augment
   diagnostic processes, elevate healthcare accessibility, and advance
   patient education. However, the journey towards fully integrating AI
   into healthcare is ongoing, requiring continuous research, development,
   and careful monitoring by medical professionals to ensure patient safety
   and quality of care.
Z8 2
TC 15
ZA 0
ZB 0
ZR 0
ZS 0
Z9 17
DA 2024-02-02
UT WOS:001146755700001
PM 38415150
ER

PT J
AU Rajendran, Praveenbalaji
   Chen, Yizheng
   Qiu, Liang
   Niedermayr, Thomas
   Liu, Wu
   Buyyounouski, Mark
   Bagshaw, Hilary
   Han, Bin
   Yang, Yong
   Kovalchuk, Nataliya
   Gu, Xuejun
   Hancock, Steven
   Xing, Lei
   Dai, Xianjin
TI Autodelineation of Treatment Target Volume for Radiation Therapy Using
   Large Language Model-Aided Multimodal Learning
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 121
IS 1
BP 230
EP 240
DI 10.1016/j.ijrobp.2024.07.2149
EA DEC 2024
DT Article
PD JAN 1 2025
PY 2025
AB Purpose: Artificial intelligence-aided methods have made significant
   progress in the auto-delineation of normal tissues. However, these
   approaches struggle with the auto-contouring of radiation therapy target
   volume. Our goal was to model the delineation of target volume as a
   clinical decision-making problem, resolved by leveraging large language
   model-aided multimodal learning approaches. Methods and Materials: A
   vision-language model, termed Medformer, has been developed, employing
   the hierarchical vision transformer as its backbone and incorporating
   large language models to extract text-rich features. The contextually
   embedded linguistic features are seamlessly integrated into visual
   features for language-aware visual encoding through the visual language
   attention module. Metrics, including Dice similarity coefficient (DSC),
   intersection over union (IOU), and 95th percentile Hausdorff distance
   (HD95), were used to quantitatively evaluate the performance of our
   model. The evaluation was conducted on an in-house prostate cancer data
   set and a public oropharyngeal carcinoma data set, totaling 668
   subjects. Results: Our Medformer achieved a DSC of 0.81 f 0.10 versus
   0.72 f 0.10, IOU of 0.73 f 0.12 versus 0.65 f 0.09, and HD95 of 9.86 f
   9.77 mm versus 19.13 f 12.96 mm for delineation of gross tumor volume on
   the prostate cancer dataset. Similarly, on the oropharyngeal carcinoma
   dataset, it achieved a DSC of 0.77 f 0.11 versus 0.72 f 0.09, IOU of
   0.70 f 0.09 versus 0.65 f 0.07, and HD95 of 7.52 f 4.8 mm versus 13.63 f
   7.13 mm, representing significant improvements (P <0.05). For
   delineating the clinical target volume, Medformer achieved a DSC of 0.91
   f 0.04, IOU of 0.85 f 0.05, and HD95 of 2.98 f 1.60 mm, comparable with
   other state-of-the-art algorithms. Conclusions: Auto-delineation of the
   treatment target based on multimodal learning outperforms conventional
   approaches that rely purely on visual features. Our method could be
   adopted into routine practice to rapidly contour clinical target
   volume/gross tumor volume. (c) 2024 Elsevier Inc. All rights are
   reserved, including those for text and data mining, AI training, and
   similar technologies.
ZR 0
TC 3
ZA 0
ZB 0
ZS 0
Z8 0
Z9 3
DA 2025-02-10
UT WOS:001413606000001
PM 39117164
ER

PT J
AU Wei, Shuoyang
   Hu, Ankang
   Liang, Yongguang
   Yang, Jingru
   Yu, Lang
   Li, Wenbo
   Yang, Bo
   Qiu, Jie
TI Feasibility study of automatic radiotherapy treatment planning for
   cervical cancer using a large language model
SO RADIATION ONCOLOGY
VL 20
IS 1
AR 77
DI 10.1186/s13014-025-02660-5
DT Article
PD MAY 15 2025
PY 2025
AB BackgroundRadiotherapy treatment planning traditionally involves complex
   and time-consuming processes, often relying on trial-and-error methods.
   The emergence of artificial intelligence, particularly Large Language
   Models (LLMs), surpassing human capabilities and existing algorithms in
   various domains, presents an opportunity to automate and enhance this
   optimization process.PurposeThis study seeks to evaluate the capacity of
   LLMs to generate radiotherapy treatment plans comparable to those
   crafted by human medical physicists, focusing on target volume
   conformity and organs-at-risk (OARs) dose sparing. The goal is to
   automate the optimization process of radiotherapy treatment plans
   through the utilization of LLMs.MethodsMultiple LLMs were employed to
   adjust optimization parameters for radiotherapy treatment plans, using a
   dataset comprising 35 cervical cancer patients treated with volumetric
   modulated arc therapy (VMAT). Customized prompts were applied to 5
   patients to tailor the LLMs, which were subsequently tested on 30
   patients. Evaluation metrics included target volume conformity, dose
   homogeneity, monitor units (MU) value, and OARs dose sparing, comparing
   plans generated by various LLMs to manual plans.ResultsWith the
   exception of Gemini-1.5-flash, which faced challenges due to
   hallucinations, Qwen-2.5-max and Llama-3.2 produced acceptable VMAT
   plans in 16.3 +/- 5.0 and 9.8 +/- 2.1 min, respectively, outperforming
   an experienced human physicist's time cost of about 20 min. The average
   conformity index (CI) for Qwen-2.5-max plans, Llama-3.2 plans, and
   manual plans on the test set were 0.929 +/- 0.007, 0.928 +/- 0.007, and
   0.926 +/- 0.007, respectively. The average homogeneity index (HI) was
   0.058 +/- 0.006, 0.059 +/- 0.005, and 0.065 +/- 0.006, respectively.
   While there was a significant difference in target volume conformity
   between LLM plans and manual plans, OARs dose sparing showed no
   significant variations. In lateral comparisons among different LLMs, no
   statistically significant differences were observed in the PTV dose,
   OARs dose sparing, and target volume conformity between Qwen-2.5-max and
   Llama-3.2 plans.ConclusionsThrough an assessment of LLM-generated plans
   and clinical plans in terms of target volume conformity and OARs dose
   sparing, this study provides preliminary evidence supporting the
   viability of LLMs for optimizing radiotherapy treatment plans. The
   implementation of LLMs demonstrates the potential for enhancing clinical
   workflows and reducing the workload associated with treatment planning.
ZA 0
ZR 0
ZS 0
ZB 0
Z8 0
TC 0
Z9 0
DA 2025-05-21
UT WOS:001489386700002
PM 40375332
ER

PT C
AU Solinsky, Jacob
   Finzel, Raymond
   Michalowski, Martin
   Pakhomov, Serguei
GP Int Speech Commun Assoc
TI Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for
   Cognitive Monitoring
SO INTERSPEECH 2023
SE Interspeech
BP 684
EP 685
DT Proceedings Paper
PD 2023
PY 2023
AB ANNA is a telephony-based cognitive assessment tool designed to aid
   nurses in caring for patients who require close monitoring for the
   development of confusion or neurological impairment. Of particular
   concern is the treatment of Immune Effector Cell-Associated
   Neurotoxicity Syndrome (ICANS), a condition which occurs quite
   frequently as an adverse outcome of Chimeric Antigen Receptor-T (CAR-T)
   cancer immunotherapy. ANNA employs both traditional verbal tests for
   cognitive impairment and novel linguistic methods which identify
   abnormalities in the patient's speech during ordinary conversation. To
   collect ordinary speech it uses a lightweight instance of the Facebook's
   Large Language Model BlenderBot to engage the patient in a partially
   unscripted conversation. ANNA is designed with easy employment by
   healthcare providers in mind, being sufficiently lightweight to run on
   consumer-grade hardware and needing access only to a patient's phone
   number to interact with them.
CT Interspeech Conference
CY AUG 20-24, 2023
CL Dublin, IRELAND
ZS 0
TC 0
ZB 0
ZA 0
Z8 0
ZR 0
Z9 0
DA 2024-07-04
UT WOS:001186650300144
ER

PT J
AU Xiong, Yichun
   Li, Jiaqi
   Jin, Wang
   Sheng, Xiaoran
   Peng, Hui
   Wang, Zhiyi
   Jia, Caifeng
   Zhuo, Lili
   Zhang, Yibo
   Huang, Jingzhe
   Zhai, Modi
   Lyu, Beibei
   Sun, Jie
   Zhou, Meng
TI PCMR: a comprehensive precancerous molecular resource
SO SCIENTIFIC DATA
VL 12
IS 1
AR 551
DI 10.1038/s41597-025-04899-9
DT Article
PD APR 1 2025
PY 2025
AB Early detection and intervention of precancerous lesions are crucial in
   reducing cancer morbidity and mortality. Comprehensive analysis of
   genomic, transcriptomic, proteomic and epigenomic alterations can
   provide insights into the early stages of carcinogenesis. However, the
   lacke of an integrated, well-curated data resource of molecular
   signatures limits our understanding of precancerous processes. Here, we
   introduce a comprehensive PreCancerous Molecular Resource (PCMR), which
   compiles 25,828 molecular profiles of precancerous samples paired with
   normal or malignant counterparts. These profiles cover precancerous
   lesions of 35 cancer types across 20 organs and tissues, derived from
   tissue samples, liquid biopsies, cell lines and organoids, with data
   from transcriptomics, proteomics and epigenomics. PCMR includes 62,566
   precancer-gene associations derived from differential analysis and
   text-mining using the ChatGPT large language model. We examined PCMR
   dataset reliability and significance by the authoritative precancerous
   molecular signature, along with its biological and clinical relevance.
   Overall, PCMR will serve as a valuable resource for advancing precancer
   research and ultimately improving patient outcomes.
ZR 0
ZB 0
ZS 0
ZA 0
Z8 0
TC 0
Z9 0
DA 2025-04-11
UT WOS:001459759400009
PM 40169679
ER

PT J
AU Chatziisaak, Dimitrios
   Burri, Pascal
   Sparn, Moritz
   Hahnloser, Dieter
   Steffen, Thomas
   Bischofberger, Stephan
TI Concordance of ChatGPT artificial intelligence decision-making in
   colorectal cancer multidisciplinary meetings: retrospective study
SO BJS OPEN
VL 9
IS 3
AR zraf040
DI 10.1093/bjsopen/zraf040
DT Article
PD JUN 2025
PY 2025
AB Background The objective of this study was to evaluate the concordance
   between therapeutic recommendations proposed by a multidisciplinary team
   meeting and those generated by a large language model (ChatGPT) for
   colorectal cancer. Although multidisciplinary teams represent the
   'standard' for decision-making in cancer treatment, they require
   significant resources and may be susceptible to human bias. Artificial
   intelligence, particularly large language models such as ChatGPT, has
   the potential to enhance or optimize the decision-making processes. The
   present study examines the potential for integrating artificial
   intelligence into clinical practice by comparing multidisciplinary team
   decisions with those generated by ChatGPT.Methods A retrospective,
   single-centre study was conducted involving consecutive patients with
   newly diagnosed colorectal cancer discussed at our multidisciplinary
   team meeting. The pre- and post-therapeutic multidisciplinary team
   meeting recommendations were assessed for concordance compared with
   ChatGPT-4.Results One hundred consecutive patients with newly diagnosed
   colorectal cancer of all stages were included. In the pretherapeutic
   discussions, complete concordance was observed in 72.5%, with partial
   concordance in 10.2% and discordance in 17.3%. For post-therapeutic
   discussions, the concordance increased to 82.8%; 11.8% of decisions
   displayed partial concordance and 5.4% demonstrated discordance.
   Discordance was more frequent in patients older than 77 years and with
   an American Society of Anesthesiologists classification >=
   III.Conclusion There is substantial concordance between the
   recommendations generated by ChatGPT and those provided by traditional
   multidisciplinary team meetings, indicating the potential utility of
   artificial intelligence in supporting clinical decision-making for
   colorectal cancer management.
   This study assessed the concordance between recommendations by a
   multidisciplinary tumour board and ChatGPT-4 for colorectal cancer
   management. A retrospective analysis of 100 cases showed substantial
   agreement, with concordance rates of 72.5% pretherapeutically and 82.8%
   posttherapeutically. These findings highlight the potential role of
   artificial intelligence in supporting and complementing traditional
   multidisciplinary team decision-making processes.
ZB 0
TC 0
ZR 0
ZS 0
ZA 0
Z8 0
Z9 0
DA 2025-05-13
UT WOS:001484582700001
PM 40331891
ER

PT J
AU Rajendran, Praveenbalaji
   Yang, Yong
   Niedermayr, Thomas R.
   Gensheimer, Michael
   Beadle, Beth
   Le, Quynh-Thu
   Xing, Lei
   Dai, Xianjin
TI Large language model-augmented learning for auto-delineation of
   treatment targets in head-and-neck cancer radiotherapy
SO RADIOTHERAPY AND ONCOLOGY
VL 205
AR 110740
DI 10.1016/j.radonc.2025.110740
EA JAN 2025
DT Article
PD APR 2025
PY 2025
AB Background and Purpose: Radiation therapy (RT) is highly effective, but
   its success depends on accurate, manual target delineation, which is
   time-consuming, labor-intensive, and prone to variability. Despite AI
   advancements in auto-contouring normal tissues, accurate RT target
   volume delineation remains challenging. This study presents Radformer, a
   novel visual language model that integrates text-rich clinical data with
   medical imaging for accurate automated RT target volume delineation.
   Materials and Methods: We developed Radformer, an innovative network
   that utilizes a hierarchical vision transformer as its backbone and
   integrates large language models (LLMs) to extract and embed clinical
   data in text-rich form. The model features a novel visual language
   attention module (VLAM) to combine visual and linguistic features,
   enabling language-aware visual encoding (LAVE). The Radformer was
   evaluated on a dataset of 2985 patients with head-and-neck cancer who
   underwent RT. Quantitative evaluations were performed utilizing metrics
   such as the Dice similarity coefficient (DSC), intersection over union
   (IOU), and 95th percentile Hausdorff distance (HD95). Results: The
   Radformer demonstrated superior performance in segmenting RT target
   volumes compared to stateof-the-art models. On the head-and-neck cancer
   dataset, Radformer achieved a mean DSC of 0.76 f 0.09 versus 0.66 f
   0.09, a mean IOU of 0.69 f 0.08 versus 0.59 f 0.07, and a mean HD95 of
   7.82 f 6.87 mm versus 14.28 f 6.85 mm for gross tumor volume
   delineation, compared to the baseline 3D-UNETR. Conclusions: The
   Radformer model offers a clinically optimal means of RT target
   auto-delineation by integrating both imaging and clinical data through a
   visual language model. This approach improves the accuracy of RT target
   volume delineation, facilitating broader AI-assisted automation in RT
   treatment planning.
TC 1
ZR 0
ZA 0
Z8 0
ZS 0
ZB 0
Z9 1
DA 2025-03-06
UT WOS:001433650900001
PM 39855601
ER

PT J
AU Meyer, Bastian
   Kfuri-Rubens, Raphael
   Schmidt, Georg
   Tariq, Maliha
   Riedel, Caroline
   Recker, Florian
   Riedel, Fabian
   Kiechle, Marion
   Riedel, Maximilian
TI Exploring the potential of AI-powered applications for clinical
   decision-making in gynecologic oncology.
SO International journal of gynaecology and obstetrics: the official organ
   of the International Federation of Gynaecology and Obstetrics
DI 10.1002/ijgo.70251
DT Journal Article
PD 2025-Jun-13
PY 2025
AB OBJECTIVE: The rise of artificial intelligence (AI) and large language
   models like Llama, Gemini, or Generative Pretraining Transformer (GPT)
   signals a promising new era in natural language processing and has
   significant potential for application in medical care. This study seeks
   to investigate the potential of GPT-4 for automated therapy
   recommendations by examining individual patient health record data with
   a focus on gynecologic malignancies and breast cancer.
   METHODS: We tasked GPT-4 with generating independent treatment proposals
   for 60 randomly selected patient cases presented at gynecologic and
   senologic multidisciplinary tumor boards (MDTs). The treatment
   recommendations by GPT-4 were compared with those of the MDTs using a
   novel clinical concordance score and were reviewed both qualitatively
   and quantitatively by experienced gynecologic oncologists.
   RESULTS: GPT-4 generated coherent therapeutic recommendations for all
   clinical cases. Overall, these recommendations were assessed by clinical
   experts as moderately sufficient for real-word clinical application.
   Deficiencies in both accuracy and completeness were especially noted.
   Using a quantitative clinical concordance score, GPT-4 consistently
   demonstrated superior performance in managing the senologic cases
   compared with the gynecologic cases. Iterative prompting substantially
   enhanced treatment recommendations in both categories, increasing
   concordance with MDT decisions to up to 84% in senologic cases.
   CONCLUSION: GPT-4 is capable of processing complex patient cases and
   generates detailed treatment recommendations; however, differences
   persist in surgical approaches and the use of systemic therapies, and
   there is a tendency toward recommending excessive genetic testing. As
   AI-powered solutions continue to be integrated into medicine, we
   envision the potential for automated therapy recommendations to play a
   supportive role in human clinical decision-making in the future.
Z8 0
TC 0
ZR 0
ZA 0
ZB 0
ZS 0
Z9 0
DA 2025-06-15
UT MEDLINE:40512143
PM 40512143
ER

PT J
AU Schulte, Brian
TI Capacity of ChatGPT to Identify Guideline-Based Treatments for Advanced
   Solid Tumors
SO CUREUS JOURNAL OF MEDICAL SCIENCE
VL 15
IS 4
AR e37938
DI 10.7759/cureus.37938
DT Article
PD APR 21 2023
PY 2023
AB Background: ChatGPT, created by OpenAI, is a large language model which
   has become the fastest growing consumer application in history,
   recognized for its expansive knowledge of varied subjects. The field of
   oncology is highly specialized and requires nuanced understanding of
   medications and conditions. Herein, we sought to better qualify the
   ability of ChatGPT to name applicable treatments for patients with
   advanced solid cancers.Methods: This observational study was conducted
   utilizing ChatGPT. The capacity of ChatGPT to tabulate appropriate
   systemic therapies for new diagnoses of advanced solid malignancies was
   ascertained through standardized prompts. A ratio of those medications
   listed by ChatGPT to those suggested in the National Comprehensive
   Cancer Network (NCCN) guidelines was produced and called the valid
   therapy quotient (VTQ). Additional descriptive analyses of the VTQ and
   its association with incidence and type of treatment were
   performed.Results: Some 51 distinct diagnoses were utilized within this
   experiment. ChatGPT was able to identify 91 distinct medications in
   response to prompts related to advanced solid tumors. The overall VTQ is
   0.77. In all cases, ChatGPT was able to provide at least one example of
   systemic therapy suggested by the NCCN. There was a weak association
   between incidence of each malignancy and the VTQ.Conclusion: The
   capacity of ChatGPT to identify medications used to treat advanced solid
   tumors indicates a level of concordance with the NCCN guidelines. As it
   stands, the role of ChatGPT to assist oncologists and patients in
   treatment decision making remains unknown. Nonetheless, in future
   iterations, it may be anticipated that accuracy and consistency in this
   domain will improve, and further studies will be needed to better
   quantify its capabilities.
ZB 11
ZS 1
TC 32
ZR 0
ZA 0
Z8 0
Z9 32
DA 2023-06-25
UT WOS:000995922600030
PM 37220429
ER

PT J
AU Khanmohammadi, R.
   Ghanem, A. I.
   Verdecchia, K.
   Hall, R.
   Elshaikh, M. A.
   Movsas, B.
   Bagher-Ebadian, H.
   Chetty, I. J.
   Ghassemi, M. M.
   Thind, K.
TI A Novel Localized Student-Teacher LLM for Enhanced Toxicity Extraction
   in Radiation Oncology
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3388
BP E632
EP E633
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
Z8 0
TC 0
ZB 0
ZR 0
ZA 0
ZS 0
Z9 0
DA 2024-12-16
UT WOS:001325892302069
ER

PT J
AU Yang, Xiongwen
   Zhang, Yun
   Jiang, Jinyan
   Chen, Zhijun
   Bai, Rinasu
   Yuan, Zihao
   Dong, Longyan
   Xiao, Yi
   Liu, Di
   Deng, Huiyin
   Huang, Jian
   Shi, Huiyou
   Liu, Dan
   Liang, Maoli
   Tang, Weijuan
   Xu, Chuan
TI Harnessing GPT-4 for automated error detection in pathology reports:
   Implications for oncology diagnostics
SO DIGITAL HEALTH
VL 11
AR 20552076251346703
DI 10.1177/20552076251346703
DT Article
PD 2025
PY 2025
AB Objective Accurate pathology reports are crucial for the diagnosis and
   treatment planning of cancer patients. However, these reports are prone
   to errors due to time pressures, subjective interpretation, and
   inconsistencies among professionals. Addressing these errors is vital
   for improving oncology care outcomes. Artificial intelligence (AI)
   systems, such as GPT-4, offer the potential to enhance diagnostic
   accuracy and efficiency.Methods A total of 700 malignant tumor pathology
   reports were collected from four hospitals. Of these, 350 reports had
   deliberate errors introduced by a senior pathologist, mimicking
   real-world reporting challenges. Error detection performance was
   evaluated by comparing GPT-4 to six human pathologists (two seniors, two
   attending pathologists, and two residents). Key metrics included error
   detection rates with Wilson confidence intervals and processing time per
   report.Results GPT-4 detected 88% of errors (350/400; 95% CI: [84, 91]),
   compared to a 95% detection rate by the top senior pathologist (382/400;
   95% CI: [93, 97]). GPT-4 significantly reduced the average processing
   time to 4.03 seconds per report, compared to 65.64 seconds for the
   fastest human pathologist. However, GPT-4 exhibited a higher rate of
   false positives (2.3%; 95% CI: [1.52, 3.01]) compared to the
   best-performing senior pathologist (0.3%; 95% CI: [0.01,
   0.91]).Conclusions GPT-4 demonstrates substantial potential in improving
   the efficiency and accuracy of pathology error detection, which could
   accelerate clinical workflows and enhance cancer diagnostics. However,
   its higher false-positive rate emphasizes the need for human oversight
   to ensure safe implementation in clinical practice.
TC 0
Z8 0
ZB 0
ZS 0
ZA 0
ZR 0
Z9 0
DA 2025-06-03
UT WOS:001498668600001
PM 40453047
ER

PT J
AU Yuan, Lun-Hsiang
   Huang, Shi -Wei
   Chou, Dean
   Tsai, Chung-You
TI The In-depth Comparative Analysis of Four Large Language AI Models for
   Risk Assessment and Information Retrieval from Multi-Modality Prostate
   Cancer Work-up Reports
SO WORLD JOURNAL OF MENS HEALTH
DI 10.5534/wjmh.240173
EA DEC 2024
DT Article; Early Access
PY 2024
AB Purpose: Information retrieval (IR) and risk assessment (RA) from
   multi-modality imaging and pathology reports are critical to prostate
   cancer (PC) treatment. This study aims to evaluate the performance of
   four general-purpose large language model (LLMs) in IR and RA tasks.
   Materials and Methods: We conducted a study using simulated text reports
   from computed tomography, magnetic resonance imaging, bone scans, and
   biopsy pathology on stage IV PC patients. We assessed four LLMs
   (ChatGPT-4-turbo, Claude-3opus, Gemini-Pro-1.0, ChatGPT-3.5-turbo) on
   three RA tasks (LATITUDE, CHAARTED, TwNHI) and seven IR tasks. It
   included TNM staging, and the detection and quantification of bone and
   visceral metastases, providing a broad evaluation of their capabilities
   in handling diverse clinical data. We queried LLMs with multi-modality
   reports using zero-shot chain-of-thought prompting via application
   programming interface. With three adjudicators' consensus as the gold
   standard, these models' performances were assessed through repeated
   single-round queries and ensemble voting methods, using 6 outcome
   metrics. Results: Among 350 stage IV PC patients with simulated reports,
   115 (32.9%), 128 (36.6%), and 94 (26.9%) belonged to LATITUDE, CHAARTED,
   and TwNHI high-risk, respectively. Ensemble voting, based on three
   repeated single-round queries, consistently enhances accuracy with a
   higher likelihood of achieving non-inferior results compared to a single
   query. Four models showed minimal differences in IR tasks with high
   accuracy (87.4%-94.2%) and consistency (ICC>0.8) in TNM staging.
   However, there were significant differences in RA performance, with the
   ranking as follows: ChatGPT-4-turbo, Claude3-opus, Gemini-Pro-1.0, and
   ChatGPT-3.5-turbo, respectively. ChatGPT-4-turbo achieved the highest
   accuracy (90.1%, 90.7%,91.6%), and consistency (ICC 0.86, 0.93, 0.76)
   across 3 RA tasks. Conclusions: ChatGPT-4-turbo demonstrated
   satisfactory accuracy and outcomes in RA and IR for stage IV PC,
   suggesting its potential for clinical decision support. However, the
   risks of misinterpretation impacting decision-making cannot be over
   looked. Further research is necessary to validate these findings in
   other cancers.
ZS 0
ZA 0
TC 0
ZB 0
Z8 0
ZR 0
Z9 0
DA 2025-01-01
UT WOS:001384880900001
PM 39743220
ER

PT J
AU Sun, Di
   Hadjiiski, Lubomir
   Gormley, John
   Chan, Heang-Ping
   Caoili, Elaine
   Cohan, Richard
   Alva, Ajjai
   Bruno, Grace
   Mihalcea, Rada
   Zhou, Chuan
   Gulani, Vikas
TI Outcome Prediction Using Multi-Modal Information: Integrating Large
   Language Model-Extracted Clinical Information and Image Analysis
SO CANCERS
VL 16
IS 13
AR 2402
DI 10.3390/cancers16132402
DT Article
PD JUL 2024
PY 2024
AB Simple Summary: Predicting the survival of bladder cancer patients
   following cystectomy can offer valuable information for treatment
   planning, decision-making, patient counseling, and resource allocation.
   Our aim was to develop large language model (LLM)-aided multi-modal
   predictive models, based on clinical information and CT images. These
   models achieved performances comparable to those of multi-modal
   predictive models that rely on manually extracted clinical information.
   This study demonstrates the potential of employing LLMs to process
   medical data, and of integrating LLM-processed data into modeling for
   prognosis.
   Survival prediction post-cystectomy is essential for the follow-up care
   of bladder cancer patients. This study aimed to evaluate artificial
   intelligence (AI)-large language models (LLMs) for extracting clinical
   information and improving image analysis, with an initial application
   involving predicting five-year survival rates of patients after radical
   cystectomy for bladder cancer. Data were retrospectively collected from
   medical records and CT urograms (CTUs) of bladder cancer patients
   between 2001 and 2020. Of 781 patients, 163 underwent chemotherapy, had
   pre- and post-chemotherapy CTUs, underwent radical cystectomy, and had
   an available post-surgery five-year survival follow-up. Five AI-LLMs
   (Dolly-v2, Vicuna-13b, Llama-2.0-13b, GPT-3.5, and GPT-4.0) were used to
   extract clinical descriptors from each patient's medical records. As a
   reference standard, clinical descriptors were also extracted manually.
   Radiomics and deep learning descriptors were extracted from CTU images.
   The developed multi-modal predictive model, CRD, was based on the
   clinical (C), radiomics (R), and deep learning (D) descriptors. The LLM
   retrieval accuracy was assessed. The performances of the survival
   predictive models were evaluated using AUC and Kaplan-Meier analysis.
   For the 163 patients (mean age 64 +/- 9 years; M:F 131:32), the LLMs
   achieved extraction accuracies of 74%similar to 87% (Dolly), 76%similar
   to 83% (Vicuna), 82%similar to 93% (Llama), 85%similar to 91% (GPT-3.5),
   and 94%similar to 97% (GPT-4.0). For a test dataset of 64 patients, the
   CRD model achieved AUCs of 0.89 +/- 0.04 (manually extracted
   information), 0.87 +/- 0.05 (Dolly), 0.83 +/- 0.06 similar to 0.84 +/-
   0.05 (Vicuna), 0.81 +/- 0.06 similar to 0.86 +/- 0.05 (Llama), 0.85 +/-
   0.05 similar to 0.88 +/- 0.05 (GPT-3.5), and 0.87 +/- 0.05 similar to
   0.88 +/- 0.05 (GPT-4.0). This study demonstrates the use of LLM
   model-extracted clinical information, in conjunction with imaging
   analysis, to improve the prediction of clinical outcomes, with bladder
   cancer as an initial example.
ZB 0
Z8 0
ZS 0
ZR 0
ZA 0
TC 4
Z9 4
DA 2024-07-24
UT WOS:001270395100001
PM 39001463
ER

PT J
AU Rosich, A.
   Ferrer, J. C.
   Guerreros, S. M.
   Rivero, E.
   Torres, M.
   Roldan, S.
   Giordano, Sr M.
   Paolini, G.
   Ochandorena, K.
   Ricagni, L.
   Lorenzo, F.
TI Artificial Intelligence in Oncologic Radiotherapy: A New Tool for
   Treatment Selection in Patients with Early-Stage Breast Cancer
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3432
BP E654
EP E654
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZB 0
Z8 0
ZS 0
ZR 0
TC 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892302113
ER

PT J
AU Gungor, Nur Dokuzeylul
   Esen, Fatih Sinan
   Tasci, Tolga
   Gungor, Kagan
   Cil, Kaan
TI Navigating Gynecological Oncology with Different Versions of ChatGPT: A
   Transformative Breakthrough or the Next Black Box Challenge?
SO ONCOLOGY RESEARCH AND TREATMENT
DI 10.1159/000543173
EA DEC 2024
DT Article; Early Access
PY 2024
AB Introduction: The study evaluates the performance of large language
   model versions of ChatGPT - ChatGPT-3.5, ChatGPT-4, and ChatGPT-Omni -
   in addressing inquiries related to the diagnosis and treatment of
   gynecological cancers, including ovarian, endometrial, and cervical
   cancers. Methods: A total of 804 questions were equally distributed
   across four categories: true/false, multiple-choice, open-ended, and
   case-scenario, with each question type representing varying levels of
   complexity. Performance was assessed using a six-point Likert scale,
   focusing on accuracy, completeness, and alignment with established
   clinical guidelines. Results: For true/false queries, ChatGPT-Omni
   achieved accuracy rates of 100% for easy, 98% for medium, and 97% for
   complicated questions, higher than ChatGPT-4 (94%, 90%, 85%) and
   ChatGPT-3.5 (90%, 85%, 80%) (p = 0.041, 0.023, 0.014, respectively). In
   multiple-choice, ChatGPT-Omni maintained superior accuracy with 100% for
   easy, 98% for medium, and 93% for complicated queries, compared to
   ChatGPT-4 (92%, 88%, 80%) and ChatGPT-3.5 (85%, 80%, 70%) (p = 0.035,
   0.028, 0.011). For open-ended questions, ChatGPT-Omni had mean Likert
   scores of 5.8 for easy, 5.5 for medium, and 5.2 for complex levels,
   outperforming ChatGPT-4 (5.4, 5.0, 4.5) and ChatGPT-3.5 (5.0, 4.5, 4.0)
   (p = 0.037, 0.026, 0.015). Similar trends were observed in case-scenario
   questions, where ChatGPT-Omni achieved scores of 5.6, 5.3, and 4.9 for
   easy, medium, and hard levels, respectively (p = 0.017, 0.008, 0.012).
   Conclusions: ChatGPT-Omni exhibited superior performance in responding
   to clinical queries related to gynecological cancers, underscoring its
   potential utility as a decision support tool and an educational resource
   in clinical practice.
Z8 0
ZB 0
ZS 0
TC 0
ZA 0
ZR 0
Z9 0
DA 2025-02-05
UT WOS:001404655900001
PM 39689699
ER

PT J
AU Odabashian, Roupen
   Bastin, Donald
   Jones, Georden
   Manzoor, Maria
   Tangestaniapour, Sina
   Assad, Malke
   Lakhani, Sunita
   Odabashian, Maritsa
   Mcgee, Sharon
TI Assessment of ChatGPT-3.5's Knowledge in Oncology: Comparative Study
   with ASCO-SEP Benchmarks
SO JMIR AI
VL 3
AR e50442
DI 10.2196/50442
DT Article
PD 2024
PY 2024
AB Background: ChatGPT (Open AI) is a state-of-the-art large language model
   that uses artificial intelligence (AI) to address questions across
   diversetopics. TheAmerican Society of Clinical Oncology Self-Evaluation
   Program (ASCO-SEP) created a comprehensive educational program to help
   physicians keep up to date with the many rapid advances in the field.
   The question bank consists of multiple choice questions addressing the
   many facets of cancer care, including diagnosis, treatment, and
   supportive care. As ChatGPT applications rapidly expand, it becomesvital
   to ascertain if the knowledge of ChatGPT-3.5 matches the established
   standards that oncologists are recommended to follow. Objective: This
   studyaimstoevaluatewhetherChatGPT-3.5'sknowledgealigns with
   theestablished benchmarksthat oncologists are expected to adhere to.
   This will furnish us with a deeper understanding of the potential
   applications of this tool as a support forclinical decision-making.
   Methods: We conducted a systematic assessment of the performance of
   ChatGPT-3.5 on theASCO-SEP, the leading educational and assessment tool
   for medical oncologists in training and practice. Over 1000 multiple
   choice questions covering the spectrum of cancer care were extracted.
   Questions were categorized by cancer type or discipline, with
   subcategorization as treatment, diagnosis, or other. Answers were scored
   as correct if ChatGPT-3.5 selected the answer as defined by ASCO-SEP.
   Results: Overall, ChatGPT-3.5 achieved a score of 56.1% (583/1040) for
   the correct answers provided. The program demonstrated varying levels of
   accuracy across cancer types or disciplines. The highest accuracy was
   observed in questions related to developmental therapeutics (8/10; 80%
   correct), while the lowest accuracy was observed in questions related to
   gastrointestinal cancer (102/209; 48.8% correct). There was no
   significant difference in the program's performance across the
   predefined subcategoriesof diagnosis, treatment, and other (P=.16, which
   isgreaterthan .05). Conclusions:This study evaluated ChatGPT-3.5's
   oncology knowledge using the ASCO-SEP, aiming to address uncertainties
   regarding AI tools like ChatGPT in clinical decision-making. Our
   findings suggest that while ChatGPT-3.5 offers a hopeful outlook for AI
   in oncology, its present performance in ASCO-SEP tests necessitates
   further refinement to reach the requisite competency levels. Future
   assessments could explore ChatGPT's clinical decision support
   capabilities with real-world clinical scenarios, its ease of integration
   into medical workflows, and its potentialto foster interdisciplinary
   collaboration and patient engagement in health care settings.
ZB 0
ZA 0
TC 3
Z8 0
ZS 0
ZR 0
Z9 3
DA 2024-12-21
UT WOS:001374817300001
PM 38875575
ER

PT J
AU Kaiser, Philippe
   Yang, Shan
   Bach, Michael
   Breit, Christian
   Mertz, Kirsten
   Stieltjes, Bram
   Ebbing, Jan
   Wetterauer, Christian
   Henkel, Maurice
TI The interaction of structured data using openEHR and large Language
   models for clinical decision support in prostate cancer
SO WORLD JOURNAL OF UROLOGY
VL 43
IS 1
AR 67
DI 10.1007/s00345-024-05423-1
DT Article
PD JAN 13 2025
PY 2025
AB BackgroundMultidisciplinary teams (MDTs) are essential for cancer care
   but are resource-intensive. Decision-making processes within MDTs, while
   critical, contribute to increased healthcare costs due to the need for
   specialist time and coordination. The recent emergence of large language
   models (LLMs) offers the potential to improve the efficiency and
   accuracy of clinical decision-making processes, potentially reducing
   costs associated with traditional MDT models.MethodsWe conducted a
   retrospective study of 171 consecutively treated patients with newly
   diagnosed prostate cancer. Relevant structured clinical data and the
   European Association of Urology (EAU) pocket guidelines were provided to
   two LLMs (chatGPT-4, Claude-3-Opus). LLM treatment recommendations were
   compared to actual treatment recommendations of the MDT meeting
   (MDM).ResultsBoth LLMs demonstrated an overall adherence of 93% with the
   MDT treatment recommendations. Discrepancies between LLM and MDT
   recommendations were observed in 15 cases (9%), primarily due to lack of
   clinical information that could be provided to the LLMs. In 5 cases
   (3%), the LLM recommendations were not in line with EAU guidelines
   despite having access to all relevant information.ConclusionsOur
   findings provide evidence that LLMs can provide accurate treatment
   recommendations for newly diagnosed prostate cancer patients. LLMs have
   the potential to streamline MDT workflows, enabling specialists to focus
   on complex cases and patient-centered discussions.Patient SummaryIn this
   study, we explored the potential of artificial intelligence models
   called large language models (LLMs) to assist in treatment
   decision-making for prostate cancer patients. We found that LLMs, when
   provided with patient information and clinical guidelines, can recommend
   treatments that closely match those made by a team of cancer
   specialists, suggesting that LLMs could help streamline the
   decision-making process and potentially reduce healthcare costs.
ZR 0
Z8 0
TC 0
ZB 0
ZS 0
ZA 0
Z9 0
DA 2025-01-20
UT WOS:001397199400002
PM 39804478
ER

PT J
AU Dougherty, Robert F.
   Clarke, Patrick
   Atli, Merve
   Kuc, Joanna
   Schlosser, Danielle
   Dunlop, Boadie W.
   Hellerstein, David J.
   Aaronson, Scott T.
   Zisook, Sidney
   Young, Allan H.
   Carhart-Harris, Robin
   Goodwin, Guy M.
   Ryslik, Gregory A.
TI Psilocybin therapy for treatment resistant depression: prediction of
   clinical outcome by natural language processing
SO PSYCHOPHARMACOLOGY
DI 10.1007/s00213-023-06432-5
EA AUG 2023
DT Article; Early Access
PY 2023
AB Rationale Therapeutic administration of psychedelics has shown
   significant potential in historical accounts and recent clinical trials
   in the treatment of depression and other mood disorders. A recent
   randomized double-blind phase-IIb study demonstrated the safety and
   efficacy of COMP360, COMPASS Pathways' proprietary synthetic formulation
   of psilocybin, in participants with treatment-resistant depression.
   Objective While the phase-IIb results are promising, the treatment works
   for a portion of the population and early prediction of outcome is a key
   objective as it would allow early identification of those likely to
   require alternative treatment.
   Methods Transcripts were made from audio recordings of the psychological
   support session between participant and therapist 1 day post COMP360
   administration. A zero-shot machine learning classifier based on the
   BART large language model was used to compute two-dimensional sentiment
   (valence and arousal) for the participant and therapist from the
   transcript. These scores, combined with the Emotional Breakthrough Index
   (EBI) and treatment arm were used to predict treatment outcome as
   measured by MADRS scores. (Code and data are available at
   https://github.com/compasspathways/Sentiment2D.)
   Results Two multinomial logistic regression models were fit to predict
   responder status at week 3 and through week 12. Cross-validation of
   these models resulted in 85% and 88% accuracy and AUC values of 88% and
   85%.
   Conclusions A machine learning algorithm using NLP and EBI accurately
   predicts long-term patient response, allowing rapid prognostication of
   personalized response to psilocybin treatment and insight into
   therapeutic model optimization. Further research is required to
   understand if language data from earlier stages in the therapeutic
   process hold similar predictive power.
ZA 0
TC 9
ZS 0
ZR 0
ZB 4
Z8 0
Z9 9
DA 2023-09-03
UT WOS:001052834600002
PM 37606733
ER

PT J
AU Huang, Hanyao
   Zheng, Ou
   Wang, Dongdong
   Yin, Jiayi
   Wang, Zijin
   Ding, Shengxuan
   Yin, Heng
   Xu, Chuan
   Yang, Renjie
   Zheng, Qian
   Shi, Bing
TI ChatGPT for shaping the future of dentistry: the potential of
   multi-modal large language model
SO INTERNATIONAL JOURNAL OF ORAL SCIENCE
VL 15
IS 1
AR 29
DI 10.1038/s41368-023-00239-y
DT Review
PD JUL 28 2023
PY 2023
AB The ChatGPT, a lite and conversational variant of Generative Pretrained
   Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large
   Language Models (LLMs) with billions of parameters. LLMs have stirred up
   much interest among researchers and practitioners in their impressive
   skills in natural language processing tasks, which profoundly impact
   various fields. This paper mainly discusses the future applications of
   LLMs in dentistry. We introduce two primary LLM deployment methods in
   dentistry, including automated dental diagnosis and cross-modal dental
   diagnosis, and examine their potential applications. Especially,
   equipped with a cross-modal encoder, a single LLM can manage
   multi-source data and conduct advanced natural language reasoning to
   perform complex clinical operations. We also present cases to
   demonstrate the potential of a fully automatic Multi-Modal LLM AI system
   for dentistry clinical application. While LLMs offer significant
   potential benefits, the challenges, such as data privacy, data quality,
   and model bias, need further study. Overall, LLMs have the potential to
   revolutionize dental diagnosis and treatment, which indicates a
   promising avenue for clinical application and research in dentistry.
ZR 0
ZA 0
TC 112
ZS 0
ZB 10
Z8 4
Z9 115
DA 2023-08-15
UT WOS:001039606900001
PM 37507396
ER

PT J
AU Hou, Yihao
   Bert, Christoph
   Gomaa, Ahmed
   Lahmer, Godehard
   Hoefler, Daniel
   Weissmann, Thomas
   Voigt, Raphaela
   Schubert, Philipp
   Schmitter, Charlotte
   Depardon, Alina
   Semrau, Sabine
   Maier, Andreas
   Fietkau, Rainer
   Huang, Yixing
   Putz, Florian
TI Fine-tuning a local LLaMA-3 large language model for automated
   privacy-preserving physician letter generation in radiation oncology
SO FRONTIERS IN ARTIFICIAL INTELLIGENCE
VL 7
AR 1493716
DI 10.3389/frai.2024.1493716
DT Article
PD JAN 14 2025
PY 2025
AB Introduction Generating physician letters is a time-consuming task in
   daily clinical practice.Methods This study investigates local
   fine-tuning of large language models (LLMs), specifically LLaMA models,
   for physician letter generation in a privacy-preserving manner within
   the field of radiation oncology.Results Our findings demonstrate that
   base LLaMA models, without fine-tuning, are inadequate for effectively
   generating physician letters. The QLoRA algorithm provides an efficient
   method for local intra-institutional fine-tuning of LLMs with limited
   computational resources (i.e., a single 48 GB GPU workstation within the
   hospital). The fine-tuned LLM successfully learns radiation
   oncology-specific information and generates physician letters in an
   institution-specific style. ROUGE scores of the generated summary
   reports highlight the superiority of the 8B LLaMA-3 model over the 13B
   LLaMA-2 model. Further multidimensional physician evaluations of 10
   cases reveal that, although the fine-tuned LLaMA-3 model has limited
   capacity to generate content beyond the provided input data, it
   successfully generates salutations, diagnoses and treatment histories,
   recommendations for further treatment, and planned schedules. Overall,
   clinical benefit was rated highly by the clinical experts (average score
   of 3.4 on a 4-point scale).Discussion With careful physician review and
   correction, automated LLM-based physician letter generation has
   significant practical value.
Z8 0
ZR 0
TC 0
ZB 0
ZA 0
ZS 0
Z9 0
DA 2025-02-01
UT WOS:001406781800001
PM 39877751
ER

PT J
AU Yang, Sen
   Xu, Piao
TI LLM4THP: a computing tool to identify tumor homing peptides by molecular
   and sequence representation of large language model based on two-layer
   ensemble model strategy
SO AMINO ACIDS
VL 56
IS 1
AR 62
DI 10.1007/s00726-024-03422-5
DT Article
PD OCT 15 2024
PY 2024
AB Tumor homing peptides (THPs) have a distinctive capacity to specifically
   attach to tumor cells, providing a promising approach for targeted
   cancer treatment and detection. Although THPs have the potential for
   significant impact, their detection by conventional methods is both
   time-consuming and expensive. To tackle this issue, we provide LLM4THP,
   an innovative computational approach that utilizes large language models
   (LLMs) to quickly and effectively detect THPs. LLM4THP utilizes two
   protein LLMs, ESM2 and Prot_T5_XL_UniRef50, to encode peptide sequences.
   This allows for the capture of complex patterns and relationships within
   the peptide data. In addition, we utilize inherent sequence
   characteristics such as Amino Acid Composition (AAC), Pseudo Amino Acid
   Composition (PAAC), Amphiphilic Pseudo Amino Acid Composition (APAAC),
   and Composition, Transition, and Distribution (CTD) to improve the
   representation of peptides. The RDKitDescriptors feature representation
   approach transforms peptide sequences into molecular objects and
   computes chemical characteristics, resulting in enhanced THP
   identification. The LLM4THP ensemble strategy incorporates various
   features into a two-layer learning architecture. The first layer
   consists of LightGBM, XGBoost, Random Forest, and Extremely Randomized
   Trees, which generate a set of meta results. The second layer utilizes
   Logistic Regression to further refine the identification of sequences as
   either THP or non-THP. LLM4THP exhibits exceptional performance compared
   to the most advanced methods, showcasing enhancements in accuracy,
   Matthew's correlation coefficient, F1 score, area under the curve, and
   average precision. The source code and dataset can be accessed at the
   following URL: https://github.com/abcair/LLM4THP.
ZA 0
ZS 0
ZR 0
ZB 0
Z8 0
TC 0
Z9 0
DA 2024-10-19
UT WOS:001332078600001
PM 39404804
ER

PT J
AU Kuerbanjiang, Warisijiang
   Peng, Shengzhe
   Jiamaliding, Yiershatijiang
   Yi, Yuexiong
TI Performance Evaluation of Large Language Models in Cervical Cancer
   Management Based on a Standardized Questionnaire: Comparative Study
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 27
AR e63626
DI 10.2196/63626
DT Article
PD FEB 5 2025
PY 2025
AB Background: Cervical cancer remains the fourth leading cause of death
   among women globally, with a particularly severe burden in low-resource
   settings. A comprehensive approach-from screening to diagnosis and
   treatment-is essential for effective prevention and management. Large
   language models (LLMs) have emerged as potential tools to support health
   care, though their specific role in cervical cancer management remains
   underexplored. Objective: This study aims to systematically evaluate the
   performance and interpretability of LLMs in cervical cancer management.
   Methods: Models were selected from the AlpacaEval leaderboard version
   2.0 and based on the capabilities of our computer. The questions
   inputted into the models cover aspects of general knowledge, screening,
   diagnosis, and treatment, according to guidelines. The prompt was
   developed using the Context, Objective, Style, Tone, Audience, and
   Response (CO-STAR) framework. Responses were evaluated for accuracy,
   guideline compliance, clarity, and practicality, graded as A, B, C, and
   D with corresponding scores of 3, 2, 1, and 0. The effective rate was
   calculated as the ratio of A and B responses to the total number of
   designed questions. Local Interpretable Model-Agnostic Explanations
   (LIME) was used to explain and enhance physicians' trust in model
   outputs within the medical context. Results: Nine models were included
   in this study, and a set of 100 standardized questions covering general
   information, screening, diagnosis, and treatment was designed based on
   international and national guidelines. Seven models (ChatGPT-4.0 Turbo,
   Claude 2, Gemini Pro, Mistral-7B-v0.2, Starling-LM-7B alpha, HuatuoGPT,
   and BioMedLM 2.7B) provided stable responses. Among all the models
   included, ChatGPT-4.0 Turbo ranked first with a mean score of 2.67 (95%
   CI 2.54-2.80; effective rate 94.00%) with a prompt and 2.52 (95% CI
   2.37-2.67; effective rate 87.00%) without a prompt, outperforming the
   other 8 models (P<.001). Regardless of prompts, QiZhenGPT consistently
   ranked among the lowest-performing models, with P<.01 in comparisons
   against all models except BioMedLM. Interpretability analysis showed
   that prompts improved alignment with human annotations for proprietary
   models (median intersection over union 0.43), while medical-specialized
   models exhibited limited improvement. Conclusions: Proprietary LLMs,
   particularly ChatGPT-4.0 Turbo and Claude 2, show promise in clinical
   decision-making involving logical analysis. The use of prompts can
   enhance the accuracy of some models in cervical cancer management to
   varying degrees. Medical-specialized models, such as HuatuoGPT and
   BioMedLM, did not perform as well as expected in this study. By
   contrast, proprietary models, particularly those augmented with prompts,
   demonstrated notable accuracy and interpretability in medical tasks,
   such as cervical cancer management. However, this study underscores the
   need for further research to explore the practical application of LLMs
   in medical practice.
ZS 0
Z8 0
ZA 0
TC 0
ZB 0
ZR 0
Z9 0
DA 2025-02-27
UT WOS:001424878900005
PM 39908540
ER

PT J
AU Fu, Sidney W.
   Tang, Cong
   Tan, Xiaohui
   Srivastava, Sudhir
TI Liquid biopsy for early cancer detection: technological revolutions and
   clinical dilemma
SO EXPERT REVIEW OF MOLECULAR DIAGNOSTICS
VL 24
IS 10
BP 937
EP 955
DI 10.1080/14737159.2024.2408744
EA OCT 2024
DT Review
PD OCT 2 2024
PY 2024
AB IntroductionLiquid biopsy is an innovative advancement in oncology,
   offering a noninvasive method for early cancer detection and monitoring
   by analyzing circulating tumor cells, DNA, RNA, and other biomarkers in
   bodily fluids. This technique has the potential to revolutionize
   precision oncology by providing real-time analysis of tumor dynamics,
   enabling early detection, monitoring treatment responses, and tailoring
   personalized therapies based on the molecular profiles of individual
   patients.Areas coveredIn this review, the authors discuss current
   methodologies, technological challenges, and clinical applications of
   liquid biopsy. This includes advancements in detecting minimal residual
   disease, tracking tumor evolution, and combining liquid biopsy with
   other diagnostic modalities for precision oncology. Key areas explored
   are the sensitivity, specificity, and integration of multi-omics, AI,
   ML, and LLM technologies.Expert opinionLiquid biopsy holds great
   potential to revolutionize cancer care through early detection and
   personalized treatment strategies. However, its success depends on
   overcoming technological and clinical hurdles, such as ensuring high
   sensitivity and specificity, interpreting results amidst tumor
   heterogeneity, and making tests accessible and affordable. Continued
   innovation and collaboration are crucial to fully realize the potential
   of liquid biopsy in improving early cancer detection, treatment, and
   monitoring.
ZR 0
ZA 0
Z8 0
ZB 2
TC 6
ZS 0
Z9 6
DA 2024-10-09
UT WOS:001325602700001
PM 39360748
ER

PT J
AU Wang, Qingxin
   Wang, Zhongqiu
   Li, Minghua
   Ni, Xinye
   Tan, Rong
   Zhang, Wenwen
   Wubulaishan, Maitudi
   Wang, Wei
   Yuan, Zhiyong
   Zhang, Zhen
   Liu, Cong
TI A feasibility study of automating radiotherapy planning with large
   language model agents
SO PHYSICS IN MEDICINE AND BIOLOGY
VL 70
IS 7
AR 075007
DI 10.1088/1361-6560/adbff1
DT Article
PD APR 6 2025
PY 2025
AB Objective. Radiotherapy planning requires significant expertise to
   balance tumor control and organ-at-risk (OAR) sparing. Automated
   planning can improve both efficiency and quality. This study introduces
   GPT-Plan, a novel multi-agent system powered by the GPT-4 family of
   large language models (LLMs), for automating the iterative radiotherapy
   plan optimization. Approach. GPT-Plan uses LLM-driven agents, mimicking
   the collaborative clinical workflow of a dosimetrist and physicist, to
   iteratively generate and evaluate text-based radiotherapy plans based on
   predefined criteria. Supporting tools assist the agents by leveraging
   historical plans, mitigating LLM hallucinations, and balancing
   exploration and exploitation. Performance was evaluated on 12 lung
   (IMRT) and 5 cervical (VMAT) cancer cases, benchmarked against the ECHO
   auto-planning method and manual plans. The impact of historical plan
   retrieval on efficiency was also assessed. Results. For IMRT lung cancer
   cases, GPT-Plan generated high-quality plans, demonstrating superior
   target coverage and homogeneity compared to ECHO while maintaining
   comparable or better OAR sparing. For VMAT cervical cancer cases, plan
   quality was comparable to a senior physicist and consistently superior
   to a junior physicist, particularly for OAR sparing. Retrieving
   historical plans significantly reduced the number of required
   optimization iterations for lung cases (p < 0.01) and yielded iteration
   counts comparable to those of the senior physicist for cervical cases (p
   = 0.313). Occasional LLM hallucinations have been mitigated by
   self-reflection mechanisms. One limitation was the inaccuracy of
   vision-based LLMs in interpreting dose images. Significance. This
   pioneering study demonstrates the feasibility of automating radiotherapy
   planning using LLM-powered agents for complex treatment decision-making
   tasks. While challenges remain in addressing LLM limitations, ongoing
   advancements hold potential for further refining and expanding
   GPT-Plan's capabilities.
ZS 0
TC 1
ZB 0
ZA 0
ZR 0
Z8 0
Z9 1
DA 2025-04-26
UT WOS:001469440600001
PM 40073507
ER

PT J
AU Li, Ya
   Zheng, Xuecong
   Li, Jiaping
   Dai, Qingyun
   Wang, Chang-Dong
   Chen, Min
TI LKAN: LLM-Based Knowledge-Aware Attention Network for Clinical Staging
   of Liver Cancer
SO IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS
VL 29
IS 4
BP 3007
EP 3020
DI 10.1109/JBHI.2024.3478809
DT Article
PD APR 2025
PY 2025
AB Clinical staging of liver cancer (CSoLC), an important indicator for
   evaluating primary liver cancer (PLC), is key in the diagnosis,
   treatment, and rehabilitation of liver cancer. In China, the current
   CSoLC adopts the China liver cancer (CNLC) staging, which is usually
   evaluated by clinicians based on radiology reports. Therefore, inferring
   clinical information from unstructured radiology reports can provide
   auxiliary decision support for clinicians. The key to solving the
   challenging task is to guide the model to pay attention to the
   staging-related words or sentences, and the following issues may occur:
   1) Imbalanced categories: Early- and mid-stage liver cancer symptoms are
   subtle, resulting in more data in the end-stage. 2) Domain sensitivity
   of liver cancer data: The liver cancer dataset contains substantial
   domain knowledge, leading to out-of-vocabulary issues and reduced
   classification accuracy. 3) Free-text and lengthy report: Radiology
   reports sparsely describe various lesions using domain-specific terms,
   making it hard to mine staging-related information. To address these,
   this article proposes a large language model (LLM)-based Knowledge-aware
   Attention Network (LKAN) for CSoLC. First, for maintaining semantic
   consistency, LLM and a rule-based algorithm are integrated to generate
   more diverse and reasonable data. Second, an unlabeled radiology corpus
   is pre-trained to introduce domain knowledge for subsequent
   representation learning. Third, attention is improved by incorporating
   both global and local features to guide the model's focus on
   staging-relevant information. Compared with the baseline models, LKAN
   has achieved the best results with 90.3% Accuracy, 90.0% Macro_F1 score,
   and 90.0% Macro_Recall.
ZR 0
ZS 0
Z8 0
ZA 0
ZB 0
TC 1
Z9 1
DA 2025-04-19
UT WOS:001459663700029
PM 39392729
ER

PT J
AU Chen, Zikang
   Wang, Qinchuan
   Sun, Yaoqian
   Cai, Hailing
   Lu, Xudong
TI Chat-ePRO: Development and pilot study of an electronic patient-reported
   outcomes system based on ChatGPT
SO JOURNAL OF BIOMEDICAL INFORMATICS
VL 154
AR 104651
DI 10.1016/j.jbi.2024.104651
EA MAY 2024
DT Article
PD JUN 2024
PY 2024
AB Objective: Chatbots have the potential to improve user compliance in
   electronic Patient-Reported Outcome (ePRO) system. Compared to
   rule-based chatbots, Large Language Model (LLM) offers advantages such
   as simplifying the development process and increasing conversational
   flexibility. However, there is currently a lack of practical
   applications of LLMs in ePRO systems. Therefore, this study utilized
   ChatGPT to develop the ChatePRO system and designed a pilot study to
   explore the feasibility of building an ePRO system based on LLM.
   Materials and Methods: This study employed prompt engineering and
   offline knowledge distillation to design a dialogue algorithm and built
   the Chat-ePRO system on the WeChat Mini Program platform. In order to
   compare Chat-ePRO with the form-based ePRO and rule-based chatbot ePRO
   used in previous studies, we conducted a pilot study applying the three
   ePRO systems sequentially at the Sir Run Run Shaw Hospital to collect
   patients' PRO data. Result: Chat-ePRO is capable of correctly generating
   conversation based on PRO forms (success rate: 95.7 %) and accurately
   extracting the PRO data instantaneously from conversation (Macro-F1:
   0.95). The majority of subjective evaluations from doctors (>70 %)
   suggest that Chat-ePRO is able to comprehend questions and consistently
   generate responses. Pilot study shows that Chat-ePRO demonstrates higher
   response rate (9/10, 90 %) and longer interaction time (10.86 s/turn)
   compared to the other two methods. Conclusion: Our study demonstrated
   the feasibility of utilizing algorithms such as prompt engineering to
   drive LLM in completing ePRO data collection tasks, and validated that
   the Chat-ePRO system can effectively enhance patient compliance.
Z8 0
ZA 0
ZR 0
ZS 0
TC 1
ZB 0
Z9 1
DA 2024-06-17
UT WOS:001243033900001
PM 38703936
ER

PT J
AU Gunning, Jordan A.
   Gilman, Kristy E.
   Zuniga, Tiffany M.
   Simpson, Richard J.
   Limesand, Kirsten H.
TI Parotid glands have a dysregulated immune response following radiation
   therapy
SO PLOS ONE
VL 19
IS 3
AR e0297387
DI 10.1371/journal.pone.0297387
DT Article
PD MAR 12 2024
PY 2024
AB Head and neck cancer treatment often consists of surgical resection of
   the tumor followed by ionizing radiation (IR), which can damage
   surrounding tissues and cause adverse side effects. The underlying
   mechanisms of radiation-induced salivary gland dysfunction are not fully
   understood, and treatment options are scarce and ineffective. The wound
   healing process is a necessary response to tissue injury, and broadly
   consists of inflammatory, proliferative, and redifferentiation phases
   with immune cells playing key roles in all three phases. In this study,
   select immune cells were phenotyped and quantified, and certain cytokine
   and chemokine concentrations were measured in mouse parotid glands after
   IR. Further, we used a model where glandular function is restored to
   assess the immune phenotype in a regenerative response. These data
   suggest that irradiated parotid tissue does not progress through a
   typical inflammatory response observed in wounds that heal.
   Specifically, total immune cells (CD45+) decrease at days 2 and 5
   following IR, macrophages (F4/80+CD11b+) decrease at day 2 and 5 and
   increase at day 30, while neutrophils (Ly6G+CD11b+) significantly
   increase at day 30 following IR. Additionally, radiation treatment
   reduces CD3- cells at all time points, significantly increases
   CD3+/CD4+CD8+ double positive cells, and significantly reduces
   CD3+/CD4-CD8- double negative cells at day 30 after IR. Previous data
   indicate that post-IR treatment with IGF-1 restores salivary gland
   function at day 30, and IGF-1 injections attenuate the increase in
   macrophages, neutrophils, and CD4+CD8+ T cells observed at day 30
   following IR. Taken together, these data indicate that parotid salivary
   tissue exhibits a dysregulated immune response following radiation
   treatment which may contribute to chronic loss of function phenotype in
   head and neck cancer survivors.
Z8 0
ZS 0
ZR 0
ZB 1
TC 2
ZA 0
Z9 2
DA 2024-05-02
UT WOS:001192362300028
PM 38470874
ER

PT J
AU Wang, Meng
   Fan, Wei
   Wu, Tianrui
   Li, Min
TI TPepRet: a deep learning model for characterizing T-cell
   receptors-antigen binding patterns
SO BIOINFORMATICS
VL 41
IS 1
AR btaf022
DI 10.1093/bioinformatics/btaf022
DT Article
PD JAN 31 2025
PY 2025
AB Motivation T-cell receptors (TCRs) elicit and mediate the adaptive
   immune response by recognizing antigenic peptides, a process pivotal for
   cancer immunotherapy, vaccine design, and autoimmune disease management.
   Understanding the intricate binding patterns between TCRs and peptides
   is critical for advancing these clinical applications. While several
   computational tools have been developed, they neglect the directional
   semantics inherent in sequence data, which are essential for accurately
   characterizing TCR-peptide interactions.Results To address this gap, we
   develop TPepRet, an innovative model that integrates subsequence mining
   with semantic integration capabilities. TPepRet combines the strengths
   of the Bidirectional Gated Recurrent Unit (BiGRU) network for capturing
   bidirectional sequence dependencies with the Large Language Model
   framework to analyze subsequences and global sequences comprehensively,
   which enables TPepRet to accurately decipher the semantic binding
   relationship between TCRs and peptides. We have evaluated TPepRet to a
   range of challenging scenarios, including performance benchmarking
   against other tools using diverse datasets, analysis of peptide binding
   preferences, characterization of T cells clonal expansion,
   identification of true binder in complex environments, assessment of key
   binding sites through alanine scanning, validation against expression
   rates from large-scale datasets, and ability to screen SARS-CoV-2 TCRs.
   The comprehensive results suggest that TPepRet outperforms existing
   tools. We believe TPepRet will become an effective tool for
   understanding TCR-peptide binding in clinical treatment.Availability and
   implementation The source code can be obtained from
   https://github.com/CSUBioGroup/TPepRet.git.
ZR 0
ZA 0
ZB 0
ZS 0
Z8 0
TC 1
Z9 1
DA 2025-02-07
UT WOS:001410016700001
PM 39880376
ER

PT J
AU Baumgaertner, Kilian
   Byczkowski, Michael
   Schmid, Tamara
   Muschko, Marc
   Woessner, Philipp
   Gerlach, Axel
   Bonekamp, David
   Schlemmer, Heinz-Peter
   Hohenfellner, Markus
   Goertz, Magdalena
TI Effectiveness of the Medical Chatbot PROSCA to Inform Patients About
   Prostate Cancer: Results of a Randomized Controlled Trial
SO EUROPEAN UROLOGY OPEN SCIENCE
VL 69
BP 80
EP 88
DI 10.1016/j.euros.2024.08.022
EA SEP 2024
DT Article
PD NOV 2024
PY 2024
AB Background and objective: Artificial intelligence (AI)-powered
   conversational agents are increasingly finding application in health
   care, as these can provide patient education at any time. However, their
   effectiveness in medical settings remains largely unexplored. This study
   aimed to assess the impact of the chatbot "PROState cancer
   Conversational Agent"(PROSCA), which was trained to provide validated
   support from diagnostic tests to treatment options for men facing
   prosate cancer (PC) diagnosis. Methods: The chatbot PROSCA, developed by
   urologists at Heidelberg University Hospital and SAP SE, was evaluated
   through a randomized controlled trial (RCT). Patients were assigned to
   either the chatbot group, receiving additional access to PROSCA
   alongside standard information by urologists, or the control group
   (1:1), receiving standard information. A total of 112 men were included,
   of whom 103 gave feedback at study completion. Key findings and
   limitations: Overtime, patients' information needs decreased
   significantly more in the chatbot group than in the control group (p =
   0.035). In the chatbot group, 43/54 men (79.6%) used PROSCA, and all of
   them found it easy to use. Of the men, 71.4% agreed that the chatbot
   improved their informedness about PC and 90.7% would like to use PROSCA
   again. Limitations are study sample size, singlecenter design, and
   specific clinical application. Conclusions and clinical implications:
   With the introduction of the PROSCA chatbot, we created and evaluated an
   innovative, evidence-based AI health information tool as an additional
   source of information for PC. Our RCT results showed significant
   benefits of the chatbot in reducing patients' information needs and
   enhancing their understanding of PC. This easy-to-use AI tool provides
   accurate, timely, and accessible support, demonstrating its value in the
   PC diagnosis process. Future steps include further customization of the
   chatbot's responses and integration with the existing health care
   systems to maximize its impact on patient outcomes. Patient summary:
   This study evaluated an artificial intelligence-powered chatbot- PROSCA,
   a digital tool designed to support men facing prostate cancer diagnosis
   by providing validated information from diagnosis to treatment. Results
   showed that patients who used the chatbot as an additional tool felt
   better informed than those who received standard information from
   urologists. The majority of users appreciated the ease of use of the
   chatbot and expressed a desire to use it again; this suggests that
   PROSCA could be a valuable resource to improve patient understanding in
   prostate cancer diagnosis. (c) 2024 The Author(s). Published by Elsevier
   B.V. on behalf of European Association of Urology. This is an open
   access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).
ZB 0
ZA 0
TC 1
ZS 0
Z8 0
ZR 0
Z9 1
DA 2024-09-29
UT WOS:001318013100001
PM 39329071
ER

PT J
AU Sezgin, Emre
   Jackson, Daniel I.
   Kocaballi, A. Baki
   Bibart, Mindy
   Zupanec, Sue
   Landier, Wendy
   Audino, Anthony
   Ranalli, Mark
   Skeens, Micah
TI Can Large Language Models Aid Caregivers of Pediatric Cancer Patients in
   Information Seeking? A Cross-Sectional Investigation
SO CANCER MEDICINE
VL 14
IS 1
AR e70554
DI 10.1002/cam4.70554
DT Article
PD JAN 2025
PY 2025
AB PurposeCaregivers in pediatric oncology need accurate and understandable
   information about their child's condition, treatment, and side effects.
   This study assesses the performance of publicly accessible large
   language model (LLM)-supported tools in providing valuable and reliable
   information to caregivers of children with cancer. MethodsIn this
   cross-sectional study, we evaluated the performance of the four
   LLM-supported tools-ChatGPT (GPT-4), Google Bard (Gemini Pro), Microsoft
   Bing Chat, and Google SGE-against a set of frequently asked questions
   (FAQs) derived from the Children's Oncology Group Family Handbook and
   expert input (In total, 26 FAQs and 104 generated responses). Five
   pediatric oncology experts assessed the generated LLM responses using
   measures including accuracy, clarity, inclusivity, completeness,
   clinical utility, and overall rating. Additionally, the content quality
   was evaluated including readability, AI disclosure, source credibility,
   resource matching, and content originality. We used descriptive analysis
   and statistical tests including Shapiro-Wilk, Levene's, Kruskal-Wallis
   H-tests, and Dunn's post hoc tests for pairwise comparisons.
   ResultsChatGPT shows high overall performance when evaluated by the
   experts. Bard also performed well, especially in accuracy and clarity of
   the responses, whereas Bing Chat and Google SGE had lower overall
   scores. Regarding the disclosure of responses being generated by AI, it
   was observed less frequently in ChatGPT responses, which may have
   affected the clarity of responses, whereas Bard maintained a balance
   between AI disclosure and response clarity. Google SGE generated the
   most readable responses whereas ChatGPT answered with the most
   complexity. LLM tools varied significantly (p < 0.001) across all expert
   evaluations except inclusivity. Through our thematic analysis of expert
   free-text comments, emotional tone and empathy emerged as a unique theme
   with mixed feedback on expectations from AI to be empathetic.
   ConclusionLLM-supported tools can enhance caregivers' knowledge of
   pediatric oncology. Each model has unique strengths and areas for
   improvement, indicating the need for careful selection based on specific
   clinical contexts. Further research is required to explore their
   application in other medical specialties and patient demographics,
   assessing broader applicability and long-term impacts.
ZA 0
TC 2
Z8 0
ZS 0
ZB 1
ZR 0
Z9 2
DA 2025-01-13
UT WOS:001391811100001
PM 39776222
ER

PT J
AU Sharma, P.
   Yoder, R.
   Shen, X.
   Einck, J. P.
   Rhodes-Stark, K. L.
   Gan, G. N.
   Shiao, J. C.
   Cunningham, D.
   Butler-Xu, Y. S.
   Tejwani, A.
   Chen, R. C.
   Stecklein, S. R.
TI Medical Accuracy of Cancer Radiotherapy-Related ChatGPT Al Outputs in
   English and Spanish
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3436
BP E656
EP E656
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
TC 0
ZA 0
ZR 0
Z8 0
ZB 0
Z9 0
DA 2024-12-16
UT WOS:001325892302117
ER

PT J
AU Scherbakov, Dmitry
   Heider, Paul M.
   Wehbe, Ramsey
   Alekseyenko, Alexander V.
   Lenert, Leslie A.
   Obeid, Jihad S.
TI Using large language models for extracting stressful life events to
   assess their impact on preventive colon cancer screening adherence
SO BMC PUBLIC HEALTH
VL 25
IS 1
AR 12
DI 10.1186/s12889-024-21123-2
DT Article
PD JAN 2 2025
PY 2025
AB BackgroundIncrease in early onset colorectal cancer makes adherence to
   screening a significant public health concern, with various social
   determinants playing a crucial role in its incidence, diagnosis,
   treatment, and outcomes. Stressful life events, such as divorce,
   marriage, or sudden loss of job, have a unique position among the social
   determinants of health.MethodsWe applied a large language model (LLM) to
   social history sections of clinical notes in the health records database
   of the Medical University of South Carolina to extract recent stressful
   life events and assess their impact on colorectal cancer screening
   adherence. We used pattern-matching regular expressions to detect a
   possible signal in social histories and ran LLM four times on each
   social history to achieve self-consistency and then used logistic
   regression to estimate the impact of life events on the probability of
   having a code in health records related to colorectal cancer
   screening.ResultsThe LLM detected 380 patients with one or more
   stressful life events and 5,344 patients with no life events. The events
   with the most negative impact on screening were arrest or incarceration
   (OR 0.26 95% CI 0.06-0.77), becoming homeless (OR 0.18 95% CI
   0.01-0.92), separation from spouse or partner (OR 0.32 95% CI
   0.05-1.18), getting married or starting to live with a partner (OR 0.60
   95% CI 0.19-1.53). Death of somebody close to the patient (excluding
   their spouse) increased the chance of screening (OR 1.21 95% CI
   0.71-2.05). Many of the observed effects did not reach statistical
   significance.ConclusionOur findings suggest that stressful life events
   might have a counterintuitive impact on screening, with some events,
   such as bereavement, were associated with increased screening. Future
   work should be focused on validating the research findings using data
   from other health institutions. In addition, expanding the list of
   stressful life events by including a validated scale of stressful life
   events for patients from historically marginalized groups is warranted.
Z8 0
ZB 0
ZS 0
TC 2
ZA 0
ZR 0
Z9 2
DA 2025-01-11
UT WOS:001389970300003
PM 39748338
ER

PT J
AU Griewing, Sebastian
   Knitza, Johannes
   Boekhoff, Jelena
   Hillen, Christoph
   Lechner, Fabian
   Wagner, Uwe
   Wallwiener, Markus
   Kuhn, Sebastian
TI Evolution of publicly available large language models for complex
   decision-making in breast cancer care
SO ARCHIVES OF GYNECOLOGY AND OBSTETRICS
VL 310
IS 1
BP 537
EP 550
DI 10.1007/s00404-024-07565-4
EA MAY 2024
DT Article
PD JUL 2024
PY 2024
AB Purpose This study investigated the concordance of five different
   publicly available Large Language Models (LLM) with the recommendations
   of a multidisciplinary tumor board regarding treatment recommendations
   for complex breast cancer patient profiles.Methods Five LLM, including
   three versions of ChatGPT (version 4 and 3.5, with data access until
   September 3021 and January 2022), Llama2, and Bard were prompted to
   produce treatment recommendations for 20 complex breast cancer patient
   profiles. LLM recommendations were compared to the recommendations of a
   multidisciplinary tumor board (gold standard), including surgical,
   endocrine and systemic treatment, radiotherapy, and genetic testing
   therapy options.Results GPT4 demonstrated the highest concordance
   (70.6%) for invasive breast cancer patient profiles, followed by GPT3.5
   September 2021 (58.8%), GPT3.5 January 2022 (41.2%), Llama2 (35.3%) and
   Bard (23.5%). Including precancerous lesions of ductal carcinoma in
   situ, the identical ranking was reached with lower overall concordance
   for each LLM (GPT4 60.0%, GPT3.5 September 2021 50.0%, GPT3.5 January
   2022 35.0%, Llama2 30.0%, Bard 20.0%). GPT4 achieved full concordance
   (100%) for radiotherapy. Lowest alignment was reached in recommending
   genetic testing, demonstrating a varying concordance (55.0% for GPT3.5
   January 2022, Llama2 and Bard up to 85.0% for GPT4).Conclusion This
   early feasibility study is the first to compare different LLM in breast
   cancer care with regard to changes in accuracy over time, i.e., with
   access to more data or through technological upgrades. Methodological
   advancement, i.e., the optimization of prompting techniques, and
   technological development, i.e., enabling data input control and secure
   data processing, are necessary in the preparation of large-scale and
   multicenter studies to provide evidence on their safe and reliable
   clinical application. At present, safe and evidenced use of LLM in
   clinical breast cancer care is not yet feasible.
ZB 3
Z8 0
ZA 0
ZR 0
ZS 0
TC 14
Z9 14
DA 2024-06-04
UT WOS:001233695900001
PM 38806945
ER

PT J
AU Huang, Yixing
   Gomaa, Ahmed
   Semrau, Sabine
   Haderlein, Marlen
   Lettmaier, Sebastian
   Weissmann, Thomas
   Grigo, Johanna
   Tkhayat, Hassen Ben
   Frey, Benjamin
   Gaipl, Udo
   Distel, Luitpold
   Maier, Andreas
   Fietkau, Rainer
   Bert, Christoph
   Putz, Florian
TI Benchmarking ChatGPT-4 on a radiation oncology in-training exam and Red
   Journal Gray Zone cases: potentials and challenges for ai-assisted
   medical education and decision making in radiation oncology
SO FRONTIERS IN ONCOLOGY
VL 13
AR 1265024
DI 10.3389/fonc.2023.1265024
DT Article
PD SEP 14 2023
PY 2023
AB PurposeThe potential of large language models in medicine for education
   and decision-making purposes has been demonstrated as they have achieved
   decent scores on medical exams such as the United States Medical
   Licensing Exam (USMLE) and the MedQA exam. This work aims to evaluate
   the performance of ChatGPT-4 in the specialized field of radiation
   oncology.MethodsThe 38th American College of Radiology (ACR) radiation
   oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone
   cases are used to benchmark the performance of ChatGPT-4. The TXIT exam
   contains 300 questions covering various topics of radiation oncology.
   The 2022 Gray Zone collection contains 15 complex clinical
   cases.ResultsFor the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved
   the scores of 62.05% and 78.77%, respectively, highlighting the
   advantage of the latest ChatGPT-4 model. Based on the TXIT exam,
   ChatGPT-4's strong and weak areas in radiation oncology are identified
   to some extent. Specifically, ChatGPT-4 demonstrates better knowledge of
   statistics, CNS & eye, pediatrics, biology, and physics than knowledge
   of bone & soft tissue and gynecology, as per the ACR knowledge domain.
   Regarding clinical care paths, ChatGPT-4 performs better in diagnosis,
   prognosis, and toxicity than brachytherapy and dosimetry. It lacks
   proficiency in in-depth details of clinical trials. For the Gray Zone
   cases, ChatGPT-4 is able to suggest a personalized treatment approach to
   each case with high correctness and comprehensiveness. Importantly, it
   provides novel treatment aspects for many cases, which are not suggested
   by any human experts.ConclusionBoth evaluations demonstrate the
   potential of ChatGPT-4 in medical education for the general public and
   cancer patients, as well as the potential to aid clinical
   decision-making, while acknowledging its limitations in certain domains.
   Owing to the risk of hallucinations, it is essential to verify the
   content generated by models such as ChatGPT for accuracy.
ZA 0
ZR 0
TC 56
ZB 10
Z8 0
ZS 1
Z9 56
DA 2023-12-23
UT WOS:001119288400001
PM 37790756
ER

PT J
AU Barabadi, Maede Ashofteh
   Zhu, Xiaodan
   Chan, Wai Yip
   Simpson, Amber L.
   Do, Richard K. G.
TI Targeted generative data augmentation for automatic metastases detection
   from free-text radiology reports
SO FRONTIERS IN ARTIFICIAL INTELLIGENCE
VL 8
AR 1513674
DI 10.3389/frai.2025.1513674
DT Article
PD FEB 6 2025
PY 2025
AB Automatic identification of metastatic sites in cancer patients from
   electronic health records is a challenging yet crucial task with
   significant implications for diagnosis and treatment. In this study, we
   demonstrate how advancements in natural language processing, namely the
   instruction-following capability of recent large language models and
   extensive model pretraining, made it possible to automate metastases
   detection from radiology reports texts with a limited amount of
   gold-labeled data. Specifically, we prompt Llama3, an open-source
   instruction-tuned large language model, to generate synthetic training
   data to expand our limited labeled data and adapt BERT, a small
   pretrained language model, to the task. We further investigate three
   targeted data augmentation techniques which selectively expand the
   original training samples, leading to comparable or superior performance
   compared to vanilla data augmentation, in most cases, while being
   substantially more computationally efficient. In our experiments, data
   augmentation improved the average F1-score by 2.3, 3.5, and 3.9 points
   for lung, liver, and adrenal glands, the organs for which we had access
   to expert-annotated data. This observation suggests that Llama3, which
   has not been specifically tailored to this task or clinical data in
   general, can generate high-quality synthetic data through paraphrasing
   in the clinical context. We also compare metastasis identification
   accuracy between models utilizing institutionally standardized reports
   vs. non-structured reports, which complicate the extraction of relevant
   information, and show how including patient history with a customized
   model architecture narrows the gap between those two setups from 7.3 to
   4.5 points on F1-score under LoRA tuning. Our work delivers a broadly
   applicable solution with remarkable performance that does not require
   model customization for each institution, making large-scale, low-cost
   spatio-temporal cancer progression pattern extraction possible.
ZR 0
Z8 0
TC 0
ZS 0
ZB 0
ZA 0
Z9 0
DA 2025-02-24
UT WOS:001425597700001
PM 39981192
ER

PT J
AU Ra, Sinyoung
   Kim, Jonghun
   Na, Inye
   Ko, Eun Sook
   Park, Hyunjin
TI Enhancing radiomics features via a large language model for classifying
   benign and malignant breast tumors in mammography
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
VL 265
AR 108765
DI 10.1016/j.cmpb.2025.108765
EA APR 2025
DT Article
PD JUN 2025
PY 2025
AB Background and Objectives: Radiomics is widely used to assist in
   clinical decision-making, disease diagnosis, and treatment planning for
   various target organs, including the breast. Recent advances in large
   language models (LLMs) have helped enhance radiomics analysis. Materials
   and Methods: Herein, we sought to improve radiomics analysis by
   incorporating LLM-learned clinical knowledge, to classify benign and
   malignant tumors in breast mammography. We extracted radiomics features
   from the mammograms based on the region of interest and retained the
   features related to the target task. Using prompt engineering, we
   devised an input sequence that reflected the selected features and the
   target task. The input sequence was fed to the chosen LLM (LLaMA
   variant), which was fine-tuned using low-rank adaptation to enhance
   radiomics features. This was then evaluated on two mammogram datasets
   (VinDr-Mammo and INbreast) against conventional baselines. Results: The
   enhanced radiomics-based method performed better than baselines using
   conventional radiomics features tested on two mammogram datasets,
   achieving accuracies of 0.671 for the VinDr-Mammo dataset and 0.839 for
   the INbreast dataset. Conventional radiomics models require retraining
   from scratch for an unseen dataset using a new set of features. In
   contrast, the model developed in this study effectively reused the
   common features between the training and unseen datasets by explicitly
   linking feature names with feature values, leading to extensible
   learning across datasets. Our method performed better than the baseline
   method in this retraining setting using an unseen dataset. Conclusions:
   Our method, one of the first to incorporate LLM into radiomics, has the
   potential to improve radiomics analysis.
ZS 0
Z8 0
ZR 0
TC 0
ZA 0
ZB 0
Z9 0
DA 2025-04-21
UT WOS:001466026900001
PM 40203779
ER

PT J
AU Tan, Xiao-Wen
   Chen, Wen-Fang
   Wang, Na-Na
   Li, Hui-Yu
   Li, Juan
   Cao, Yu-Mei
   Zhu, Meng-Qi
   Li, Kun
   Zhang, Ting-Ling
   Fu, Dian
TI [Efficiency of different large language models in China in response to
   consultations about PCa-related perioperative nursing and health
   education].
SO Zhonghua nan ke xue = National journal of andrology
VL 30
IS 2
BP 151
EP 156
DT English Abstract; Journal Article
PD 2024-Feb
PY 2024
AB OBJECTIVE: To evaluate the efficiency of the four domestic language
   models, ERNIE Bot, ChatGLM2, Spark Desk and Qwen-14B-Chat, all with a
   massive user base and significant social attention, in response to
   consultations about PCa-related perioperative nursing and health
   education.
   METHODS: We designed a questionnaire that includes 15 questions commonly
   concerned by patients undergoing radical prostatectomy and 2 common
   nursing cases, and inputted the questions into each of the four language
   models for simulation consultation. Three nursing experts assessed the
   model responses based on a pre-designed Likert 5-point scale in terms of
   accuracy, comprehensiveness, understandability, humanistic care, and
   case analysis. We evaluated and compared the performance of the four
   models using visualization tools and statistical analyses.
   RESULTS: All the models generated high-quality texts with no misleading
   information and exhibited satisfactory performance. Qwen-14B-Chat scored
   the highest in all aspects and showed relatively stable outputs in
   multiple tests compared with ChatGLM2. Spark Desk performed well in
   terms of understandability but lacked comprehensiveness and humanistic
   care. Both Qwen-14B-Chat and ChatGLM2 demonstrated excellent performance
   in case analysis. The overall performance of ERNIE Bot was slightly
   inferior. All things considered, Qwen-14B-Chat was superior to the other
   three models in consultations about PCa-related perioperative nursing
   and health education.
   CONCLUSION: In PCa-related perioperative nursing, large language models
   represented by Qwen-14B-Chat are expected to become powerful auxiliary
   tools to provide patients with more medical expertise and information
   support, so as to improve the patient compliance and the quality of
   clinical treatment and nursing.
TC 0
Z8 2
ZS 0
ZB 0
ZA 0
ZR 0
Z9 2
DA 2024-08-24
UT MEDLINE:39177349
PM 39177349
ER

PT J
AU Chao, Pei-Ju
   Chang, Chu-Ho
   Wu, Jyun-Jie
   Liu, Yen-Hsien
   Shiau, Junping
   Shih, Hsin-Hung
   Lin, Guang-Zhi
   Lee, Shen-Hao
   Lee, Tsair-Fwu
TI Improving Prediction of Complications Post-Proton Therapy in Lung Cancer
   Using Large Language Models and Meta-Analysis
SO CANCER CONTROL
VL 31
AR 10732748241286749
DI 10.1177/10732748241286749
DT Article
PD SEP 2024
PY 2024
AB Purpose: This study enhances the efficiency of predicting complications
   in lung cancer patients receiving proton therapy by utilizing large
   language models (LLMs) and meta-analytical techniques for literature
   quality assessment. Materials and Methods: We integrated systematic
   reviews with LLM evaluations, sourcing studies from Web of Science,
   PubMed, and Scopus, managed via EndNote X20. Inclusion and exclusion
   criteria ensured literature relevance. Techniques included
   meta-analysis, heterogeneity assessment using Cochran's Q test and I2
   statistics, and subgroup analyses for different complications. Quality
   and bias risk were assessed using the PROBAST tool and further analyzed
   with models such as ChatGPT-4, Llama2-13b, and Llama3-8b. Evaluation
   metrics included AUC, accuracy, precision, recall, F1 score, and time
   efficiency (WPM). Results: The meta-analysis revealed an overall effect
   size of 0.78 for model predictions, with high heterogeneity observed (I2
   = 72.88%, P < 0.001). Subgroup analysis for radiation-induced
   esophagitis and pneumonitis revealed predictive effect sizes of 0.79 and
   0.77, respectively, with a heterogeneity index (I2) of 0%, indicating
   that there were no significant differences among the models in
   predicting these specific complications. A literature assessment using
   LLMs demonstrated that ChatGPT-4 achieved the highest accuracy at 90%,
   significantly outperforming the Llama3 and Llama2 models, which had
   accuracies ranging from 44% to 62%. Additionally, LLM evaluations were
   conducted 3229 times faster than manual assessments were, markedly
   enhancing both efficiency and accuracy. The risk assessment results
   identified nine studies as high risk, three as low risk, and one as
   unknown, confirming the robustness of the ChatGPT-4 across various
   evaluation metrics. Conclusion: This study demonstrated that the
   integration of large language models with meta-analysis techniques can
   significantly increase the efficiency of literature evaluations and
   reduce the time required for assessments, confirming that there are no
   significant differences among models in predicting post proton therapy
   complications in lung cancer patients.
ZB 0
ZA 0
TC 1
ZR 0
ZS 0
Z8 0
Z9 1
DA 2024-09-29
UT WOS:001318729400001
PM 39307562
ER

PT J
AU Bellamkonda, Nikhil
   Farlow, Janice L.
   Haring, Catherine T.
   Sim, Michael W.
   Seim, Nolan B.
   Cannon, Richard B.
   Monroe, Marcus M.
   Agrawal, Amit
   Rocco, James W.
   McCrary, Hilary C.
TI Evaluating the Accuracy of ChatGPT in Common Patient Questions Regarding
   HPV plus Oropharyngeal Carcinoma
SO ANNALS OF OTOLOGY RHINOLOGY AND LARYNGOLOGY
VL 133
IS 9
BP 814
EP 819
DI 10.1177/00034894241259137
EA JUL 2024
DT Article
PD SEP 2024
PY 2024
AB Objectives: Large language model (LLM)-based chatbots such as ChatGPT
   have been publicly available and increasingly utilized by the general
   public since late 2022. This study sought to investigate ChatGPT
   responses to common patient questions regarding Human Papilloma Virus
   (HPV) positive oropharyngeal cancer (OPC). Methods: This was a
   prospective, multi-institutional study, with data collected from high
   volume institutions that perform >50 transoral robotic surgery cases per
   year. The 100 most recent discussion threads including the term "HPV" on
   the American Cancer Society's Cancer Survivors Network's Head and Neck
   Cancer public discussion board were reviewed. The 11 most common
   questions were serially queried to ChatGPT 3.5; answers were recorded. A
   survey was distributed to fellowship trained head and neck oncologic
   surgeons at 3 institutions to evaluate the responses. Results: A total
   of 8 surgeons participated in the study. For questions regarding HPV
   contraction and transmission, ChatGPT answers were scored as clinically
   accurate and aligned with consensus in the head and neck surgical
   oncology community 84.4% and 90.6% of the time, respectively. For
   questions involving treatment of HPV+ OPC, ChatGPT was clinically
   accurate and aligned with consensus 87.5% and 91.7% of the time,
   respectively. For questions regarding the HPV vaccine, ChatGPT was
   clinically accurate and aligned with consensus 62.5% and 75% of the
   time, respectively. When asked about circulating tumor DNA testing, only
   12.5% of surgeons thought responses were accurate or consistent with
   consensus. Conclusion: ChatGPT 3.5 performed poorly with questions
   involving evolving therapies and diagnostics-thus, caution should be
   used when using a platform like ChatGPT 3.5 to assess use of advanced
   technology. Patients should be counseled on the importance of consulting
   their surgeons to receive accurate and up to date recommendations, and
   use LLM's to augment their understanding of these important
   health-related topics.
ZB 0
Z8 0
ZR 0
ZA 0
TC 1
ZS 0
Z9 1
DA 2024-08-06
UT WOS:001280661600001
PM 39075853
ER

PT J
AU Luo, Man
   Trivedi, Shubham
   Kurian, Allison W.
   Ward, Kevin
   Keegan, Theresa H. M.
   Rubin, Daniel
   Banerjee, Imon
TI Automated Extraction of Patient-Centered Outcomes After Breast Cancer
   Treatment: An Open-Source Large Language Model-Based Toolkit
SO JCO CLINICAL CANCER INFORMATICS
VL 8
AR e2300258
DI 10.1200/CCI.23.00258
DT Article
PD AUG 2024
PY 2024
AB PURPOSEPatient-centered outcomes (PCOs) are pivotal in cancer treatment,
   as they directly reflect patients' quality of life. Although multiple
   studies suggest that factors affecting breast cancer-related morbidity
   and survival are influenced by treatment side effects and adherence to
   long-term treatment, such data are generally only available on a smaller
   scale or from a single center. The primary challenge with collecting
   these data is that the outcomes are captured as free text in clinical
   narratives written by clinicians.MATERIALS AND METHODSGiven the
   complexity of PCO documentation in these narratives, computerized
   methods are necessary to unlock the wealth of information buried in
   unstructured text notes that often document PCOs. Inspired by the
   success of large language models (LLMs), we examined the adaptability of
   three LLMs, GPT-2, BioGPT, and PMC-LLaMA, on PCO tasks across three
   institutions, Mayo Clinic, Emory University Hospital, and Stanford
   University. We developed an open-source framework for fine-tuning LLM
   that can directly extract the five different categories of PCO from the
   clinic notes.RESULTSWe found that these LLMs without fine-tuning
   (zero-shot) struggle with challenging PCO extraction tasks, displaying
   almost random performance, even with some task-specific examples
   (few-shot learning). The performance of our fine-tuned, task-specific
   models is notably superior compared with their non-fine-tuned LLM
   models. Moreover, the fine-tuned GPT-2 model has demonstrated a
   significantly better performance than the other two larger
   LLMs.CONCLUSIONOur discovery indicates that although LLMs serve as
   effective general-purpose models for tasks across various domains, they
   require fine-tuning when applied to the clinician domain. Our proposed
   approach has the potential to lead more efficient, adaptable models for
   PCO information extraction, reducing reliance on extensive computational
   resources while still delivering superior performance for specific
   tasks.
TC 1
ZS 0
ZB 0
Z8 0
ZR 0
ZA 0
Z9 1
DA 2024-11-23
UT WOS:001299613900001
PM 39167746
ER

PT J
AU Dhodapkar, Rahul M.
   Jung, Eric
   Lee, Sun Young
TI An Eye on Extracellular Vesicles: Trends and Clinical Translations in
   Vision Research
SO OPHTHALMOLOGY SCIENCE
VL 5
IS 1
AR 100619
DI 10.1016/j.xops.2024.100619
EA NOV 2024
DT Article
PD JAN-FEB 2025
PY 2025
AB Purpose: To perform a review of research, funding, and clinical
   translation efforts for extracellular vesicles (EVs) within vision
   science. Design: Retrospective analysis of publication, funding, and
   clinical trials data. Methods: A pretrained large language model (Jina2)
   was used to create semantic embeddings for 41 282 abstracts from
   articles related to EVs archived on EMBASE and published between January
   1966 and January 2024. The articles were projected and clustered
   according to semantic embedding similarity, and research sub- domains
   for EVs were determined through inspection of term frequency-inverse
   document frequency weighted word clouds. Mann-Kendall trend analysis was
   performed to identify current areas of growth within EV research.
   Additionally, National Institutes of Health funding data from RePORT
   Expenditures and Results and clinical trials data from
   ClinicalTrials.gov were analyzed to correlate publication trends with
   funding support and clinical translation efforts. Results: Unsupervised
   clustering and Mann-Kendall trend analysis identified wound
   healing/regeneration (P = 0.030) and neurodegenerative disease (P =
   0.049) as significantly accelerating in growth of publication over time.
   Ophthalmology-restricted subset analysis identified that publications in
   age-related macular degeneration (P = 0.191) and clinical applications
   (P = 0.086) are no longer growing at a significant rate. Analysis of
   funding data identified that the National Cancer Institute was the top
   funding institution overall, but that the National Institute on Aging is
   rapidly advancing in terms of funding EV research and trials. Analysis
   of ClinicalTrials.gov data highlights a dearth of clinical trials within
   ophthalmology despite a growing number of studies in other medical
   subfields. Conclusions: Extracellular vesicles remain a promising
   substrate for both the identification and treatment of
   vision-threatening diseases. A better understanding of the current
   landscape of research and funding trends should help to inform future
   funding and translational efforts. Financial Disclosures: Proprietary or
   commercial disclosure may be found in the Footnotes and Disclosures at
   the end of this article. Ophthalmology Science 2025;5:100619 (c) 2024 by
   the American Academy of Ophthalmology. This is an open access article
   under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-ncnd/4.0/).
Z8 0
ZS 0
ZA 0
ZB 1
TC 1
ZR 0
Z9 1
DA 2024-11-23
UT WOS:001355802600001
PM 39584184
ER

PT J
AU Aubreville, Marc
   Ganz, Jonathan
   Ammeling, Jonas
   Rosbach, Emely
   Gehrke, Thomas
   Scherzad, Agmal
   Hackenberg, Stephan
   Goncalves, Miguel
TI Prediction of tumor board procedural recommendations using large
   language models
SO EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY
VL 282
IS 3
BP 1619
EP 1629
DI 10.1007/s00405-024-08947-9
EA SEP 2024
DT Article
PD MAR 2025
PY 2025
AB IntroductionMultidisciplinary tumor boards are meetings where a team of
   medical specialists, including medical oncologists, radiation
   oncologists, radiologists, surgeons, and pathologists, collaborate to
   determine the best treatment plan for cancer patients. While
   decision-making in this context is logistically and cost-intensive, it
   has a significant positive effect on overall cancer survival.Methods We
   evaluated the quality and accuracy of predictions by several large
   language models for recommending procedures by a Head and Neck Oncology
   tumor board, which we adapted for the task using parameter-efficient
   fine-tuning or in-context learning. Records were divided into two sets:
   n=229 used for training and n=100 records for validation of our
   approaches. Randomized, blinded, manual human expert classification was
   used to evaluate the different models.Results Treatment line congruence
   varied depending on the model, reaching up to 86%, with medically
   justifiable recommendations up to 98%. Parameter-efficient fine-tuning
   yielded better outcomes than in-context learning, and larger/commercial
   models tend to perform better.ConclusionProviding precise, medically
   justifiable procedural recommendations for complex oncology patients is
   feasible. Extending the data corpus to a larger patient cohort and
   incorporating the latest guidelines, assuming the model can handle
   sufficient context length, could result in more factual and
   guideline-aligned responses and is anticipated to enhance model
   performance. We, therefore, encourage further research in this direction
   to improve the efficacy and reliability of large language models as
   support in medical decision-making processes.
ZR 0
Z8 0
ZS 0
TC 0
ZA 0
ZB 0
Z9 0
DA 2024-09-19
UT WOS:001310590700001
PM 39266750
ER

PT J
AU Cho, Seungbeom
   Lee, Mangyeong
   Yu, Jaewook
   Yoon, Junghee
   Choi, Jae-Boong
   Jung, Kyu-Hwan
   Cho, Juhee
TI Leveraging Large Language Models for Improved Understanding of
   Communications With Patients With Cancer in a Call Center Setting:
   Proof-of-Concept Study
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 26
AR e63892
DI 10.2196/63892
DT Article
PD DEC 11 2024
PY 2024
AB Background: Hospital call centers play a critical role in providing
   support and information to patients with cancer, making it crucial to
   effectively identify and understand patient intent during consultations.
   However, operational efficiency and standardization of telephone
   consultations, particularly when categorizingdiverse patient inquiries,
   remainsignificant challenges. Whiletraditional deep learning models like
   long short-term memory (LSTM) and bidirectional encoder representations
   from transformers (BERT) have been used to address these issues, they
   heavily depend on annotated datasets, which are labor-intensive and
   time-consuming to generate. Large language models (LLMs) like GPT-4,
   with their in-context learning capabilities, offer a promising
   alternative for classifying patient intent without requiring extensive
   retraining. Objective: This study evaluates the performance of GPT-4 in
   classifying the purpose of telephone consultations of patients with
   cancer. In addition, it compares the performance of GPT-4 to that of
   discriminative models, such as LSTM and BERT, with a particular focus on
   their ability to manage ambiguous and complex queries. Methods: We used
   a dataset of 430,355 sentences from telephone consultations with
   patients with cancer between 2016 and 2020. LSTM and BERT models were
   trained on 300,000 sentences using supervised learning, while GPT-4 was
   applied using zero-shot and few-shot approaches without explicit
   retraining. The accuracy of each model was compared using 1,000 randomly
   selected sentences from 2020 onward, with special attention paid to how
   each model handled ambiguous or uncertain queries. Results: GPT-4, which
   uses only a few examples (a few shots), attained a remarkable accuracy
   of 85.2%, considerably outperforming the LSTM and BERT models, which
   achieved accuracies of 73.7% and 71.3%, respectively. Notably,
   categories such as "Treatment," "Rescheduling," and "Symptoms" involve
   multiple contexts and exhibit significant complexity. GPT-4 demonstrated
   morethan 15% superior performance in handling ambiguous queries in these
   categories. In addition, GPT-4 excelled in categories like "Records" and
   "Routine," where contextual clues were clear, outperforming the
   discriminative models. These findings emphasizethe potential of LLMs,
   particularly GPT-4, for interpreting complicated patient interactions
   during cancer-related telephone consultations. Conclusions: This study
   shows the potential of GPT-4 to significantly improve the classification
   of patient intent in cancer-related telephone oncological consultations.
   GPT-4's ability to handle complex and ambiguous queries without
   extensive retraining provides a substantial advantage over
   discriminative models like LSTM and BERT. While GPT-4 demonstrates
   strong performance in various areas, further refinement of prompt design
   and category definitions is necessary to fully leverage its capabilities
   in practical health care applications. Future research will explore the
   integration of LLMs like GPT-4 into hybrid systems that combine human
   oversight with artificial intelligence-driven technologies.
ZB 0
TC 1
ZS 0
Z8 0
ZR 0
ZA 0
Z9 1
DA 2025-01-01
UT WOS:001382690300003
PM 39661975
ER

PT J
AU Cho, Hyeongmin
   Yoo, Sooyoung
   Kim, Borham
   Jang, Sowon
   Sunwoo, Leonard
   Kim, Sanghwan
   Lee, Donghyoung
   Kim, Seok
   Nam, Sejin
   Chung, Jin-Haeng
TI Extracting lung cancer staging descriptors from pathology reports: A
   generative language model approach
SO JOURNAL OF BIOMEDICAL INFORMATICS
VL 157
AR 104720
DI 10.1016/j.jbi.2024.104720
EA SEP 2024
DT Article
PD SEP 2024
PY 2024
AB Background: In oncology, electronic health records contain textual key
   information for the diagnosis, staging, and treatment planning of
   patients with cancer. However, text data processing requires a lot of
   time and effort, which limits the utilization of these data. Recent
   advances in natural language processing (NLP) technology, including
   large language models, can be applied to cancer research. Particularly,
   extracting the information required for the pathological stage from
   surgical pathology reports can be utilized to update cancer staging
   according to the latest cancer staging guidelines. Objectives: This
   study has two main objectives. The first objective is to evaluate the
   performance of extracting information from text-based surgical pathology
   reports and determining pathological stages based on the extracted
   information using fine-tuned generative language models (GLMs) for
   patients with lung cancer. The second objective is to determine the
   feasibility of utilizing relatively small GLMs for information
   extraction in a resource-constrained computing environment. Methods:
   Lung cancer surgical pathology reports were collected from the Common
   Data Model database of Seoul National University Bundang Hospital
   (SNUBH), a tertiary hospital in Korea. We selected 42 descriptors
   necessary for tumor-node (TN) classification based on these reports and
   created a gold standard with validation by two clinical experts. The
   pathology reports and gold standard were used to generate
   prompt-response pairs for training and evaluating GLMs which then were
   used to extract information required for staging from pathology reports.
   Results: We evaluated the information extraction performance of six
   trained models as well as their performance in TN classification using
   the extracted information. The Deductive Mistral-7B model, which was
   pre-trained with the deductive dataset, showed the best performance
   overall, with an exact match ratio of 92.24% in the information
   extraction problem and an accuracy of 0.9876 (predicting T and N
   classification concurrently) in classification. Conclusion: This study
   demonstrated that training GLMs with deductive datasets can improve
   information extraction performance, and GLMs with a relatively small
   number of parameters at approximately seven billion can achieve high
   performance in this problem. The proposed GLM-based information
   extraction method is expected to be useful in clinical decision-making
   support, lung cancer staging and research.
ZS 0
ZA 0
ZR 0
TC 4
ZB 1
Z8 0
Z9 4
DA 2024-09-21
UT WOS:001312772300001
PM 39233209
ER

PT J
AU Yang, Z.
   Kazemimoghadam, M.
   Wang, L.
   Szalkowski, G. A.
   Chuang, C. F.
   Liu, L.
   Soltys, S. G.
   Pollom, E.
   Rahimy, E.
   Jiang, H.
   Park, D.
   Persad, A.
   Hori, Y.
   Fu, J.
   Romero, I. O.
   Zalavari, L.
   Chen, M.
   Lu, W.
   Gu, X.
TI A Deep Learning-Driven Framework for Large Language Model -Assisted
   Automatic Target Volume Localization and Delineation for Enhancing
   Spinal Metastases Stereotactic Body Radiotherapy Workflow
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 195
BP S61
EP S62
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZB 0
ZS 0
Z8 0
ZR 0
TC 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892302564
ER

PT J
AU Zhou, Juexiao
   He, Xiaonan
   Sun, Liyuan
   Xu, Jiannan
   Chen, Xiuying
   Chu, Yuetan
   Zhou, Longxi
   Liao, Xingyu
   Zhang, Bin
   Afvari, Shawn
   Gao, Xin
TI Pre-trained multimodal large language model enhances dermatological
   diagnosis using SkinGPT-4
SO NATURE COMMUNICATIONS
VL 15
IS 1
AR 5649
DI 10.1038/s41467-024-50043-3
DT Article
PD JUL 5 2024
PY 2024
AB Large language models (LLMs) are seen to have tremendous potential in
   advancing medical diagnosis recently, particularly in dermatological
   diagnosis, which is a very important task as skin and subcutaneous
   diseases rank high among the leading contributors to the global burden
   of nonfatal diseases. Here we present SkinGPT-4, which is an interactive
   dermatology diagnostic system based on multimodal large language models.
   We have aligned a pre-trained vision transformer with an LLM named
   Llama-2-13b-chat by collecting an extensive collection of skin disease
   images (comprising 52,929 publicly available and proprietary images)
   along with clinical concepts and doctors' notes, and designing a
   two-step training strategy. We have quantitatively evaluated SkinGPT-4
   on 150 real-life cases with board-certified dermatologists. With
   SkinGPT-4, users could upload their own skin photos for diagnosis, and
   the system could autonomously evaluate the images, identify the
   characteristics and categories of the skin conditions, perform in-depth
   analysis, and provide interactive treatment recommendations.
   Here, authors develop SkinGPT-4, an interactive dermatology diagnostic
   system that uses multimodal large language models and aligns a vision
   transformer with Llama-2-13b-chat. Evaluated by dermatologists, it
   offers autonomous diagnosis and treatment recommendations.
ZA 0
Z8 0
ZB 2
ZR 0
ZS 0
TC 21
Z9 21
DA 2024-07-18
UT WOS:001263353700018
PM 38969632
ER

PT J
AU Zarfati, Mor
   Soffer, Shelly
   Nadkarni, Girish N.
   Klang, Eyal
TI Retrieval-Augmented Generation: Advancing personalized care and research
   in oncology
SO EUROPEAN JOURNAL OF CANCER
VL 220
AR 115341
DI 10.1016/j.ejca.2025.115341
EA MAR 2025
DT Article
PD MAY 2 2025
PY 2025
AB Retrieval-Augmented Generation (RAG) pairs large language models (LLMs)
   with recent data to produce more accurate, context-aware outputs. By
   converting text into numeric embeddings, RAG locates and retrieves
   relevant "chunks" of data, that along with the query, ground the model's
   responses in current, specific information. This process helps reduce
   outdated or fabricated answers. In oncology, RAG has shown particular
   promise. Studies have demonstrated its ability to improve treatment
   recommendations by integrating genetic profiles, strengthened clinical
   trial matching through biomarker analysis, and accelerated drug
   development by clarifying modeldriven insights. Despite its advantages,
   RAG depends on high-quality data. Biased or incomplete sources can lead
   to inaccurate outcomes. Careful implementation and human oversight are
   crucial for ensuring the effectiveness and reliability of RAG in
   oncology.
ZS 0
ZB 0
Z8 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2025-03-21
UT WOS:001444559600001
PM 40068371
ER

PT J
AU Gibson, Damien
   Jackson, Stuart
   Shanmugasundaram, Ramesh
   Seth, Ishith
   Siu, Adrian
   Ahmadi, Nariman
   Kam, Jonathan
   Mehan, Nicholas
   Thanigasalam, Ruban
   Jeffery, Nicola
   Patel, Manish, I
   Leslie, Scott
TI Evaluating the Efficacy of ChatGPT as a Patient Education Toolin
   Prostate Cancer: Multimetric Assessment
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 26
AR e55939
DI 10.2196/55939
DT Article
PD AUG 14 2024
PY 2024
AB Background: Artificial intelligence (AI) chatbots, such as ChatGPT, have
   made significant progress. These chatbots, particularlypopular among
   health care professionals and patients, are transforming patient
   education and disease experience with personalizedinformation. Accurate,
   timely patient education is crucial for informed decision-making,
   especially regarding prostate-specificantigen screening and treatment
   options. However, the accuracy and reliability of AI chatbots'medical
   information must berigorously evaluated. Studies testing ChatGPT's
   knowledge of prostate cancer are emerging, but there is a need for
   ongoingevaluation to ensure the quality and safety of information
   provided to patients. Objective: This study aims to evaluate the
   quality, accuracy, and readability of ChatGPT-4's responses to common
   prostatecancer questions posed by patients. Methods: Overall, 8
   questions were formulated with an inductive approach based on
   information topics in peer-reviewedliterature and Google Trends data.
   Adapted versions of the Patient Education Materials Assessment Tool for
   AI (PEMAT-AI),Global Quality Score, and DISCERN-AI tools were used by 4
   independent reviewers to assess the quality of the AI responses.The 8 AI
   outputs were judged by 7 expert urologists, using an assessment
   framework developed to assess accuracy, safety,appropriateness,
   actionability, and effectiveness. The AI responses'readability was
   assessed using established algorithms (FleschReading Ease score, Gunning
   Fog Index, Flesch-Kincaid Grade Level, The Coleman-Liau Index, and
   Simple Measure ofGobbledygook [SMOG] Index). A brief tool (Reference
   Assessment AI [REF-AI]) was developed to analyze the referencesprovided
   by AI outputs, assessing for reference hallucination, relevance, and
   quality of references. Results: The PEMAT-AI understandability score was
   very good (mean 79.44%, SD 10.44%), the DISCERN-AI rating wasscored as
   "good" quality (mean 13.88, SD 0.93), and the Global Quality Score was
   high (mean 4.46/5, SD 0.50). Natural Language Assessment Tool for AI had
   pooled mean accuracy of 3.96 (SD 0.91), safety of 4.32 (SD 0.86),
   appropriateness of 4.45 (SD0.81), actionability of 4.05 (SD 1.15), and
   effectiveness of 4.09 (SD 0.98). The readability algorithm consensus was
   "difficult toread" (Flesch Reading Ease score mean 45.97, SD 8.69;
   Gunning Fog Index mean 14.55, SD 4.79), averaging an 11th-gradereading
   level, equivalent to 15- to 17-year-olds (Flesch-Kincaid Grade Level
   mean 12.12, SD 4.34; The Coleman-Liau Indexmean 12.75, SD 1.98; SMOG
   Index mean 11.06, SD 3.20). REF-AI identified 2 reference
   hallucinations, while the majority(28/30, 93%) of references
   appropriately supplemented the text. Most references (26/30, 86%) were
   from reputable governmentorganizations, while a handful were direct
   citations from scientific literature. Conclusions: Our analysis found
   that ChatGPT-4 provides generally good responses to common prostate
   cancer queries, makingit a potentially valuable tool for patient
   education in prostate cancer care. Objective quality assessment tools
   indicated that thenatural language processing outputs were generally
   reliable and appropriate, but there is room for improvement. (J Med
   Internet Res 2024;26:e55939) doi: 10.2196/55939
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
TC 11
Z9 11
DA 2024-09-15
UT WOS:001306488700008
PM 39141904
ER

PT J
AU Grilo, Ana
   Marques, Catarina
   Corte-Real, Maria
   Carolino, Elisabete
   Caetano, Marco
TI Assessing the Quality and Reliability of ChatGPT's Responses to
   Radiotherapy-Related Patient Queries: Comparative Study With GPT-3.5 and
   GPT-4
SO JMIR CANCER
VL 11
AR e63677
DI 10.2196/63677
DT Article
PD 2025
PY 2025
AB Background: Patients frequently resort to the internet to access
   information about cancer. However, these websites often lack content
   accuracy and readability. Recently, ChatGPT, an artificial
   intelligence-powered chatbot, has signified a potential paradigm shift
   in how patients with cancer can access vast amounts of medical
   information, including insights into radiotherapy. However, the quality
   of the information provided by ChatGPT remains unclear. This is
   particularly significant given the general public's limited knowledge of
   this treatment and concerns about its possible side effects.
   Furthermore, evaluating the quality of responses is crucial, as
   misinformation can foster a false sense of knowledge and security, lead
   to noncompliance, and result in delays in receiving appropriate
   treatment. Objective: This study aims to evaluate the quality and
   reliability of ChatGPT's responses to common patient queries about
   radiotherapy, comparing the performance of ChatGPT's two versions:
   GPT-3.5 and GPT-4. Methods: We selected 40 commonly asked radiotherapy
   questions and entered the queries in both versions of ChatGPT. Response
   quality and reliability were evaluated by 16 radiotherapy experts using
   the General Quality Score (GQS), a 5-point Likert scale, with the median
   GQS determined based on the experts' ratings. Consistency and similarity
   of responses were assessed using the cosine similarity score, which
   ranges from 0 (complete dissimilarity) to 1 (complete similarity).
   Readability was analyzed using the Flesch Reading Ease Score, ranging
   from 0 to 100, and the Flesch-Kincaid Grade Level, reflecting the
   average number of years of education required for comprehension.
   Statistical analyses were performed using the Mann-Whitney test and
   effect size, with results deemed significant at a 5% level (P=.05). To
   assess agreement between experts, Krippendorff alpha and Fleiss kappa
   were used. Results: GPT-4 demonstrated superior performance, with a
   higher GQS and a lower number of scores of 1 and 2, compared to GPT-3.5.
   The Mann-Whitney test revealed statistically significant differences in
   some questions, with GPT-4 generally receiving higher ratings. The
   median (IQR) cosine similarity score indicated substantial similarity
   (0.81, IQR 0.05) and consistency in the responses of both versions
   (GPT-3.5: 0.85, IQR 0.04; GPT-4: 0.83, IQR 0.04). Readability scores for
   both versions were considered college level, with GPT-4 scoring slightly
   better in the Flesch Reading Ease Score (34.61) and Flesch-Kincaid Grade
   Level (12.32) compared to GPT-3.5 (32.98 and 13.32, respectively).
   Responses by both versions were deemed challenging for the general
   public. Conclusions: Both GPT-3.5 and GPT-4 demonstrated having the
   capability to address radiotherapy concepts, with GPT-4 showing superior
   performance. However, both models present readability challenges for the
   general population. Although ChatGPT demonstrates potential as a
   valuable resource for addressing common patient queries related to
   radiotherapy, it is imperative to acknowledge its limitations, including
   the risks of misinformation and readability issues. In addition, its
   implementation should be supported by strategies to enhance
   accessibility and readability.
ZR 0
ZA 0
Z8 0
ZB 0
ZS 0
TC 0
Z9 0
DA 2025-06-12
UT WOS:001504049700001
PM 40239208
ER

PT C
AU Bandara, Eranga
   Foytik, Peter
   Shetty, Sachin
   Mukkamala, Ravi
   Rahman, Abdul
   Liang, Xueping
   Keon, Ng Wee
   De Zoysa, Kasun
GP IEEE
TI WedaGPT - Generative-AI (with Custom-Trained Meta's Llama2 LLM),
   Blockchain, Self Sovereign Identity, NFT and Model Card Enabled
   Indigenous Medicine Platform
SO 2024 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS, ISCC 2024
SE IEEE Symposium on Computers and Communications ISCC
DI 10.1109/ISCC61673.2024.10733674
DT Proceedings Paper
PD 2024
PY 2024
AB Traditional and indigenous medicine, deeply rooted in ancient traditions
   and wisdom, plays a crucial role in global healthcare and cultural
   identity. These practices provide treatments for illnesses such as
   cancer and bone injuries, which often lack effective remedies in Western
   medicine. However, these valuable systems face challenges like potential
   knowledge loss, undervaluation of practitioners' expertise, and the risk
   of fraud due to the absence of credential verification mechanisms. In
   this research, we introduce "WedaGPT," a Generative AI-enabled platform
   that utilizes a custom-trained Meta's Llama2 Large Language Model (LLM),
   Blockchain, self-sovereign identity (SSI), Non-Fungible Tokens (NFTs),
   and model cards to share traditional medical knowledge and address these
   issues. WedaGPT creates a collaborative ecosystem connecting doctors,
   medicine providers, therapists, patients, and technology experts, all
   committed to preserving and advancing traditional healing practices.
   This platform enables secure and transparent contributions from all
   stakeholders to patient well-being. Ancient medical recipe books are
   translated into English and digitized into PDF formats to enrich the
   platform's knowledge base. These texts are used to fine-tune the Llama2
   LLM, which has been quantized and optimized with Qlora for performance
   on consumer-grade hardware. Through a chat-based interface in the
   SSI-enabled mobile wallet, users can interact with the LLM and access
   detailed information on treatments, recipes, prescriptions, and healing
   methods. Additionally, users can consult remotely with doctors who
   prescribe treatments through this wallet. A key feature of WedaGPT is
   transforming ancient medicinal recipes into NFT tokens for sale on NFT
   marketplaces, giving traditional knowledge digital authenticity and
   economic value. Revenue from these sales is distributed among platform
   contributors, promoting equitable ownership and recognition. Medical
   recipe data, including treatment histories and physician details, are
   encapsulated in Model Cards and securely stored on the blockchain. This
   system offers mechanisms to verify doctors and treatments in a
   privacy-preserving way, potentially reducing fraud and medication
   errors.
CT 29th IEEE Symposium on Computers and Communications (IEEE ISCC)
CY JUN 26-29, 2024
CL Paris, FRANCE
SP IEEE
Z8 0
ZS 0
ZA 0
TC 0
ZR 0
ZB 0
Z9 0
DA 2025-01-03
UT WOS:001363176200111
ER

PT J
AU Haider, Syed Ali
   Pressman, Sophia M.
   Borna, Sahar
   Gomez-Cabello, Cesar A.
   Sehgal, Ajai
   Leibovich, Bradley C.
   Forte, Antonio Jorge
TI Evaluating Large Language Model (LLM) Performance on Established Breast
   Classification Systems
SO DIAGNOSTICS
VL 14
IS 14
AR 1491
DI 10.3390/diagnostics14141491
DT Article
PD JUL 2024
PY 2024
AB Medical researchers are increasingly utilizing advanced LLMs like
   ChatGPT-4 and Gemini to enhance diagnostic processes in the medical
   field. This research focuses on their ability to comprehend and apply
   complex medical classification systems for breast conditions, which can
   significantly aid plastic surgeons in making informed decisions for
   diagnosis and treatment, ultimately leading to improved patient
   outcomes. Fifty clinical scenarios were created to evaluate the
   classification accuracy of each LLM across five established
   breast-related classification systems. Scores from 0 to 2 were assigned
   to LLM responses to denote incorrect, partially correct, or completely
   correct classifications. Descriptive statistics were employed to compare
   the performances of ChatGPT-4 and Gemini. Gemini exhibited superior
   overall performance, achieving 98% accuracy compared to ChatGPT-4's 71%.
   While both models performed well in the Baker classification for
   capsular contracture and UTSW classification for gynecomastia, Gemini
   consistently outperformed ChatGPT-4 in other systems, such as the
   Fischer Grade Classification for gender-affirming mastectomy, Kajava
   Classification for ectopic breast tissue, and Regnault Classification
   for breast ptosis. With further development, integrating LLMs into
   plastic surgery practice will likely enhance diagnostic support and
   decision making.
ZA 0
TC 11
ZS 0
Z8 0
ZB 2
ZR 0
Z9 11
DA 2024-08-02
UT WOS:001276540600001
PM 39061628
ER

PT J
AU Ostrowska, Magdalena
   Kacala, Paulina
   Onolememen, Deborah
   Vaughan-Lane, Katie
   Sisily Joseph, Anitta
   Ostrowski, Adam
   Pietruszewska, Wioletta
   Banaszewski, Jacek
   Wrobel, Maciej J.
TI To trust or not to trust: evaluating the reliability and safety of AI
   responses to laryngeal cancer queries
SO EUROPEAN ARCHIVES OF OTO-RHINO-LARYNGOLOGY
VL 281
IS 11
BP 6069
EP 6081
DI 10.1007/s00405-024-08643-8
EA APR 2024
DT Article
PD NOV 2024
PY 2024
AB Purpose As online health information-seeking surges, concerns mount over
   the quality and safety of accessible content, potentially leading to
   patient harm through misinformation. On one hand, the emergence of
   Artificial Intelligence (AI) in healthcare could prevent it; on the
   other hand, questions raise regarding the quality and safety of the
   medical information provided. As laryngeal cancer is a prevalent head
   and neck malignancy, this study aims to evaluate the utility and safety
   of three large language models (LLMs) as sources of patient information
   about laryngeal cancer.Methods A cross-sectional study was conducted
   using three LLMs (ChatGPT 3.5, ChatGPT 4.0, and Bard). A questionnaire
   comprising 36 inquiries about laryngeal cancer was categorised into
   diagnosis (11 questions), treatment (9 questions), novelties and
   upcoming treatments (4 questions), controversies (8 questions), and
   sources of information (4 questions). The population of reviewers
   consisted of 3 groups, including ENT specialists, junior physicians, and
   non-medicals, who graded the responses. Each physician evaluated each
   question twice for each model, while non-medicals only once. Everyone
   was blinded to the model type, and the question order was shuffled.
   Outcome evaluations were based on a safety score (1-3) and a Global
   Quality Score (GQS, 1-5). Results were compared between LLMs. The study
   included iterative assessments and statistical validations.Results
   Analysis revealed that ChatGPT 3.5 scored highest in both safety (mean:
   2.70) and GQS (mean: 3.95). ChatGPT 4.0 and Bard had lower safety scores
   of 2.56 and 2.42, respectively, with corresponding quality scores of
   3.65 and 3.38. Inter-rater reliability was consistent, with less than 3%
   discrepancy. About 4.2% of responses fell into the lowest safety
   category (1), particularly in the novelty category. Non-medical
   reviewers' quality assessments correlated moderately (r = 0.67) with
   response length.Conclusions LLMs can be valuable resources for patients
   seeking information on laryngeal cancer. ChatGPT 3.5 provided the most
   reliable and safe responses among the models evaluated.
TC 11
ZS 1
ZB 1
ZR 0
ZA 0
Z8 0
Z9 10
DA 2024-04-27
UT WOS:001207064800002
PM 38652298
ER

PT J
AU Oh, Yujin
   Park, Sangjoon
   Byun, Hwa Kyung
   Cho, Yeona
   Lee, Ik Jae
   Kim, Jin Sung
   Ye, Jong Chul
TI LLM-driven multimodal target volume contouring in radiation oncology
SO NATURE COMMUNICATIONS
VL 15
IS 1
AR 9186
DI 10.1038/s41467-024-53387-y
DT Article
PD OCT 24 2024
PY 2024
AB Target volume contouring for radiation therapy is considered
   significantly more challenging than the normal organ segmentation tasks
   as it necessitates the utilization of both image and text-based clinical
   information. Inspired by the recent advancement of large language models
   (LLMs) that can facilitate the integration of the textural information
   and images, here we present an LLM-driven multimodal artificial
   intelligence (AI), namely LLMSeg, that utilizes the clinical information
   and is applicable to the challenging task of 3-dimensional context-aware
   target volume delineation for radiation oncology. We validate our
   proposed LLMSeg within the context of breast cancer radiotherapy using
   external validation and data-insufficient environments, which attributes
   highly conducive to real-world applications. We demonstrate that the
   proposed multimodal LLMSeg exhibits markedly improved performance
   compared to conventional unimodal AI models, particularly exhibiting
   robust generalization performance and data-efficiency.
   The integration of multimodal knowledge would be essential for radiation
   oncologist to determine the therapeutic treatment. Here, inspired by the
   large language models facilitating the integration of textural
   information and images, this group reports a 3D multimodal clinical
   target volume delineation model combining image and text-based clinical
   information for decision-making in radiation oncology.
TC 9
ZS 0
Z8 2
ZB 3
ZR 0
ZA 0
Z9 10
DA 2024-11-07
UT WOS:001342098500028
PM 39448587
ER

PT J
AU Porebski, Benjamin T.
   Balmforth, Matthew
   Browne, Gareth
   Riley, Aidan
   Jamali, Kiarash
   Furst, Maximillian J. L. J.
   Velic, Mirko
   Buchanan, Andrew
   Minter, Ralph
   Vaughan, Tristan
   Holliger, Philipp
TI Rapid discovery of high-affinity antibodies via massively parallel
   sequencing, ribosome display and affinity screening
SO NATURE BIOMEDICAL ENGINEERING
VL 8
IS 3
DI 10.1038/s41551-023-01093-3
EA OCT 2023
DT Article
PD MAR 2024
PY 2024
AB Developing therapeutic antibodies is laborious and costly. Here we
   report a method for antibody discovery that leverages the Illumina HiSeq
   platform to, within 3 days, screen in the order of 108 antibody-antigen
   interactions. The method, which we named 'deep screening', involves the
   clustering and sequencing of antibody libraries, the conversion of the
   DNA clusters into complementary RNA clusters covalently linked to the
   instrument's flow-cell surface on the same location, the in situ
   translation of the clusters into antibodies tethered via ribosome
   display, and their screening via fluorescently labelled antigens. By
   using deep screening, we discovered low-nanomolar nanobodies to a model
   antigen using 4 x 106 unique variants from yeast-display-enriched
   libraries, and high-picomolar single-chain antibody fragment leads for
   human interleukin-7 directly from unselected synthetic repertoires. We
   also leveraged deep screening of a library of 2.4 x 105 sequences of the
   third complementarity-determining region of the heavy chain of an
   anti-human epidermal growth factor receptor 2 (HER2) antibody as input
   for a large language model that generated new single-chain antibody
   fragment sequences with higher affinity for HER2 than those in the
   original library.
   A high-throughput method leveraging the Illumina HiSeq platform to
   screen in the order of 108 individual antibody-antigen interactions
   within 3 days facilitates the rapid discovery of antibodies to
   clinically relevant targets.
ZR 0
ZS 0
TC 20
ZA 0
ZB 11
Z8 1
Z9 21
DA 2023-10-18
UT WOS:001078085500002
PM 37814006
ER

PT J
AU Maida, Marcello
   Celsa, Ciro
   Lau, Louis H. S.
   Ligresti, Dario
   Baraldo, Stefano
   Ramai, Daryl
   Di Maria, Gabriele
   Cannemi, Marco
   Facciorusso, Antonio
   Camma, Calogero
TI The Application of Large Language Models in Gastroenterology: A Review
   of the Literature
SO CANCERS
VL 16
IS 19
AR 3328
DI 10.3390/cancers16193328
DT Review
PD OCT 2024
PY 2024
AB Simple Summary Large language models (LLMs) are revolutionizing the
   field of medicine, particularly in Gastroenterology, by improving access
   to information, diagnostics, treatment customization, and medical
   education. They analyze extensive medical data to enhance
   decision-making, patient outcomes, and educational tasks. While LLMs
   face challenges such as incomplete data, varying response accuracy, and
   reliance on specific input wording, they have shown promising results.
   However, their full integration into medical practice requires further
   research and regulation. Moreover, the successful integration of LLMs
   into medical practice necessitates customization to specific medical
   contexts and adherence to guidelines. This review focuses on the current
   evidence supporting the use of LLMs in Gastroenterology, emphasizing
   their potential and limitations.Abstract Large language models (LLMs)
   are transforming the medical landscape by enhancing access to
   information, diagnostics, treatment customization, and medical
   education, especially in areas like Gastroenterology. LLMs utilize
   extensive medical data to improve decision-making, leading to better
   patient outcomes and personalized medicine. These models are
   instrumental in interpreting medical literature and synthesizing patient
   data, facilitating real-time knowledge for physicians and supporting
   educational pursuits in medicine. Despite their potential, the complete
   integration of LLMs in real-life remains ongoing, particularly requiring
   further study and regulation. This review highlights the existing
   evidence supporting LLMs' use in Gastroenterology, addressing both their
   potential and limitations. Recent studies demonstrate LLMs' ability to
   answer questions from physicians and patients accurately. Specific
   applications in this field, such as colonoscopy, screening for
   colorectal cancer, and hepatobiliary and inflammatory bowel diseases,
   underscore LLMs' promise in improving the communication and
   understanding of complex medical scenarios. Moreover, the review
   discusses LLMs' efficacy in clinical contexts, providing guideline-based
   recommendations and supporting decision-making processes. Despite these
   advancements, challenges such as data completeness, reference
   suitability, variability in response accuracy, dependency on input
   phrasing, and a lack of patient-generated questions underscore
   limitations in reproducibility and generalizability. The effective
   integration of LLMs into medical practice demands refinement tailored to
   specific medical contexts and guidelines. Overall, while LLMs hold
   significant potential in transforming medical practice, ongoing
   development and contextual training are essential to fully realize their
   benefits.
ZB 0
ZA 0
ZR 0
ZS 0
TC 2
Z8 0
Z9 2
DA 2024-10-19
UT WOS:001331756500001
PM 39409948
ER

PT J
AU Li, Caixia
   Zhao, Yina
   Bai, Yang
   Zhao, Baoquan
   Tola, Yetunde Oluwafunmilayo
   Chan, Carmen W. H.
   Zhang, Meifen
   Fu, Xia
TI Unveiling the Potential of Large Language Models in Transforming Chronic
   Disease Management: Mixed Methods Systematic Review
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 27
AR e70535
DI 10.2196/70535
DT Review
PD APR 16 2025
PY 2025
AB Background: Chronic diseases are a major global health burden,
   accounting for nearly three-quarters of the deaths worldwide. Large
   language models (LLMs) are advanced artificial intelligence systems with
   transformative potentialto optimize chronic disease management; however,
   robust evidence is lacking. Objective: This review aims to synthesize
   evidence on the feasibility, opportunities, and challenges of LLMs
   across the disease management spectrum, from prevention to screening,
   diagnosis, treatment, and long-term care. Methods: Following the PRISMA
   (Preferred Reporting Items for Systematic Reviews and Meta-Analysis)
   guidelines, 11 databases (Cochrane Central Register of Controlled
   Trials, CINAHL, Embase, IEEE Xplore, MEDLINE via Ovid, ProQuest Health &
   MedicineCollection, ScienceDirect, Scopus, Web of Science Core
   Collection, China National KnowledgeInternet, and SinoMed) were searched
   on April 17, 2024. Intervention and simulation studies that examined
   LLMs in the management of chronic diseases were included. The
   methodological quality of the included studies was evaluated using a
   rating rubric designed for simulation-based research and the risk of
   bias in nonrandomized studies of interventions tool for
   quasi-experimental studies. Narrative analysis with descriptivefigures
   was used to synthesizethe study findings. Random-effects meta-analyses
   were conducted to assess the pooled effect estimates of the feasibility
   of LLMs in chronic disease management. Results: A total of 20 studies
   examined general-purpose (n=17) and retrieval-augmented
   generation-enhanced LLMs (n=3) for the management of chronic diseases,
   including cancer, cardiovascular diseases, and metabolic disorders. LLMs
   demonstrated feasibility across the chronic disease management spectrum
   by generating relevant, comprehensible, and accurate health
   recommendations (pooled accurate rate 71%, 95% CI 0.59-0.83; I2=88.32%)
   with retrieval-augmented generation-enhanced LLMs having higher accuracy
   rates compared to general-purpose LLMs (odds ratio 2.89, 95% CI
   1.83-4.58; I2=54.45%). LLMs facilitated equitable information access;
   increased patient awareness regarding ailments, preventive measures, and
   treatment options; and promoted self-management behaviors in lifestyle
   modification and symptom coping. Additionally, LLMs facilitate
   compassionate emotional support, social connections, and health care
   resources to improve the health outcomesof chronic diseases. However,
   LLMs face challenges in addressing privacy, language, and cultural
   issues; undertaking advanced tasks, including diagnosis, medication, and
   comorbidity management; and generating personalized regimens with
   real-timeadjustments and multiple modalities. Conclusions:LLMs have
   demonstrated the potentialto transform chronic disease management at the
   individual, social, and health care levels; however, their direct
   application in clinical settings is still in its infancy. A multifaceted
   approach that incorporates robust data security, domain-specific model
   fine-tuning, multimodal data integration, and wearables is crucial for
   the evolution of LLMs into invaluable adjuncts for health care
   professionals to transform chronic disease management. Trial
   Registration: PROSPERO CRD42024545412;
   https://www.crd.york.ac.uk/PROSPERO/view/CRD42024545412
ZB 0
Z8 0
ZS 0
ZR 0
ZA 0
TC 0
Z9 0
DA 2025-05-09
UT WOS:001478857800005
PM 40239198
ER

PT J
AU Zhang, Jingqing
   Sun, Kai
   Jagadeesh, Akshay
   Falakaflaki, Parastoo
   Kayayan, Elena
   Tao, Guanyu
   Ghahfarokhi, Mahta Haghighat
   Gupta, Deepa
   Gupta, Ashok
   Gupta, Vibhor
   Guo, Yike
TI The potential and pitfalls of using a large language model such as
   ChatGPT, GPT-4, or LLaMA as a clinical assistant
SO JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION
VL 31
IS 9
BP 1884
EP 1891
DI 10.1093/jamia/ocae184
EA JUL 2024
DT Article
PD JUL 17 2024
PY 2024
AB Objectives This study aims to evaluate the utility of large language
   models (LLMs) in healthcare, focusing on their applications in enhancing
   patient care through improved diagnostic, decision-making processes, and
   as ancillary tools for healthcare professionals.Materials and Methods We
   evaluated ChatGPT, GPT-4, and LLaMA in identifying patients with
   specific diseases using gold-labeled Electronic Health Records (EHRs)
   from the MIMIC-III database, covering three prevalent diseases-Chronic
   Obstructive Pulmonary Disease (COPD), Chronic Kidney Disease (CKD)-along
   with the rare condition, Primary Biliary Cirrhosis (PBC), and the
   hard-to-diagnose condition Cancer Cachexia.Results In patient
   identification, GPT-4 had near similar or better performance compared to
   the corresponding disease-specific Machine Learning models (F1-score >=
   85%) on COPD, CKD, and PBC. GPT-4 excelled in the PBC use case,
   achieving a 4.23% higher F1-score compared to disease-specific
   "Traditional Machine Learning" models. ChatGPT and LLaMA3 demonstrated
   lower performance than GPT-4 across all diseases and almost all metrics.
   Few-shot prompts also help ChatGPT, GPT-4, and LLaMA3 achieve higher
   precision and specificity but lower sensitivity and Negative Predictive
   Value.Discussion The study highlights the potential and limitations of
   LLMs in healthcare. Issues with errors, explanatory limitations and
   ethical concerns like data privacy and model transparency suggest that
   these models would be in clinical settings. Future studies should
   improve training datasets and model designs for LLMs to gain better
   utility in healthcare.Conclusion The study shows that LLMs have the
   potential to assist clinicians for tasks such as patient identification
   but false positives and false negatives must be mitigated before LLMs
   are adequate for real-world clinical assistance.
ZA 0
ZS 0
ZB 1
ZR 0
TC 15
Z8 1
Z9 16
DA 2024-07-23
UT WOS:001269939400001
PM 39018498
ER

PT J
AU Li, Yiming
   Zhao, Jeff
   Li, Manqi
   Dang, Yifang
   Yu, Evan
   Li, Jianfu
   Sun, Zenan
   Hussein, Usama
   Wen, Jianguo
   Abdelhameed, Ahmed M.
   Mai, Junhua
   Li, Shenduo
   Yu, Yue
   Hu, Xinyue
   Yang, Daowei
   Feng, Jingna
   Li, Zehan
   He, Jianping
   Tao, Wei
   Duan, Tiehang
   Lou, Yanyan
   Li, Fang
   Tao, Cui
TI RefAI: a GPT-powered retrieval-augmented generative tool for biomedical
   literature recommendation and summarization
SO JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION
VL 31
IS 9
BP 2030
EP 2039
DI 10.1093/jamia/ocae129
EA JUN 2024
DT Article
PD JUN 10 2024
PY 2024
AB Objectives: Precise literature recommendation and summarization are
   crucial for biomedical professionals. While the latest iteration of
   generative pretrained transformer (GPT) incorporates 2 distinct
   modes-real-time search and pretrained model utilization-it encounters
   challenges in dealing with these tasks. Specifically, the real-time
   search can pinpoint some relevant articles but occasionally provides
   fabricated papers, whereas the pretrained model excels in generating
   well-structured summaries but struggles to cite specific sources. In
   response, this study introduces RefAI, an innovative retrieval-augmented
   generative tool designed to synergize the strengths of large language
   models (LLMs) while overcoming their limitations. Materials and Methods:
   RefAI utilized PubMed for systematic literature retrieval, employed a
   novel multivariable algorithm for article recommendation, and leveraged
   GPT-4 turbo for summarization. Ten queries under 2 prevalent topics
   ("cancer immunotherapy and target therapy" and "LLMs in medicine") were
   chosen as use cases and 3 established counterparts (ChatGPT-4,
   ScholarAI, and Gemini) as our baselines. The evaluation was conducted by
   10 domain experts through standard statistical analyses for performance
   comparison. Results: The overall performance of RefAI surpassed that of
   the baselines across 5 evaluated dimensions-relevance and quality for
   literature recommendation, accuracy, comprehensiveness, and reference
   integration for summarization, with the majority exhibiting
   statistically significant improvements (P-values <.05). Discussion:
   RefAI demonstrated substantial improvements in literature recommendation
   and summarization over existing tools, addressing issues like fabricated
   papers, metadata inaccuracies, restricted recommendations, and poor
   reference integration. Conclusion: By augmenting LLM with external
   resources and a novel ranking algorithm, RefAI is uniquely capable of
   recommending high-quality literature and generating well-structured
   summaries, holding the potential to meet the critical needs of
   biomedical professionals in navigating and synthesizing vast amounts of
   scientific literature.
Z8 1
ZA 0
ZS 0
ZR 0
ZB 3
TC 13
Z9 14
DA 2024-06-17
UT WOS:001243328800001
PM 38857454
ER

PT J
AU Giannuzzi, Federico
   Carla, Matteo Mario
   Hu, Lorenzo
   Cestrone, Valentina
   Caputo, Carmela Grazia
   Sammarco, Maria Grazia
   Savino, Gustavo
   Rizzo, Stanislao
   Blasi, Maria Antonietta
   Pagliara, Monica Maria
TI Artificial intelligence with ChatGPT 4: a large language model in
   support of ocular oncology cases
SO INTERNATIONAL OPHTHALMOLOGY
VL 45
IS 1
AR 59
DI 10.1007/s10792-024-03399-w
DT Article
PD FEB 7 2025
PY 2025
AB PurposeTo evaluate ChatGPT's ability to analyze comprehensive case
   descriptions of patients with uveal melanoma and provide recommendations
   for the most appropriate management.DesignRetrospective analysis of
   ocular oncology patients' medical records.Subjects.Forty patients
   treated for uveal melanoma between May 2019 and October
   2023.DesignRetrospective analysis of ocular oncology patients' medical
   records.Subjects.Forty patients treated for uveal melanoma between May
   2019 and October 2023.DesignRetrospective analysis of ocular oncology
   patients' medical records.Subjects.Forty patients treated for uveal
   melanoma between May 2019 and October 2023.MethodsWe uploaded each case
   description into the ChatGPT interface (version 4.0) and asked the model
   to provide realistic treatment options by asking the question, "What
   type of treatment do you recommend?" The accuracy of decisions produced
   by ChatGPT was compared to those recorded in patients' files and the
   treatment recommendations provided by three ocular oncologists, each
   with more than 10 years of experience.Main outcome measures.The primary
   objective of this research was to assess the accuracy of ChatGPT replies
   in ocular oncology cases, analyzing its competence in both
   straightforward and intricate situations. Our secondary purpose was to
   assess the concordance between the responses of ChatGPT and those of
   ocular oncology specialists when faced with analogous clinical
   scenarios.MethodsWe uploaded each case description into the ChatGPT
   interface (version 4.0) and asked the model to provide realistic
   treatment options by asking the question, "What type of treatment do you
   recommend?" The accuracy of decisions produced by ChatGPT was compared
   to those recorded in patients' files and the treatment recommendations
   provided by three ocular oncologists, each with more than 10 years of
   experience.Main outcome measures.The primary objective of this research
   was to assess the accuracy of ChatGPT replies in ocular oncology cases,
   analyzing its competence in both straightforward and intricate
   situations. Our secondary purpose was to assess the concordance between
   the responses of ChatGPT and those of ocular oncology specialists when
   faced with analogous clinical scenarios.MethodsWe uploaded each case
   description into the ChatGPT interface (version 4.0) and asked the model
   to provide realistic treatment options by asking the question, "What
   type of treatment do you recommend?" The accuracy of decisions produced
   by ChatGPT was compared to those recorded in patients' files and the
   treatment recommendations provided by three ocular oncologists, each
   with more than 10 years of experience.Main outcome measures.The primary
   objective of this research was to assess the accuracy of ChatGPT replies
   in ocular oncology cases, analyzing its competence in both
   straightforward and intricate situations. Our secondary purpose was to
   assess the concordance between the responses of ChatGPT and those of
   ocular oncology specialists when faced with analogous clinical
   scenarios.ResultsChatGPT's surgical choices matched those in patients'
   files in 55% of cases (22 out of 40). ChatGPT options were agreed upon
   by 50%, 55%, and 57% of the three ocular oncology specialists. The
   investigation revealed significant differences between ChatGPT's
   responses and those of the three cancer specialists when compared to
   patients' files (p = 0.003, p = 0.001, and p = 0.001). ChatGPT's
   surgical responses matched with patient data in 18 out of 24 cases
   (75%), excluding enucleation cases.
   The decisions matched with the three ocular oncology specialists in
   17/24, 18/24, and 18/24 cases, reflecting agreements of 70%, 75%, and
   75%, respectively. The decisions made by ChatGPT were not significantly
   different from those of the three professionals in this cohort (p =
   0.50, p = 0.36, and p = 0.36 for ChatGPT compared to specialists 1, 2,
   and 3).ConclusionChatGPT exhibited a level of proficiency that was
   comparable to that of trained ocular oncology specialists. However, it
   exhibited its limitations when evaluating more complex scenarios, such
   as extrascleral extension or infiltration of the optic nerve, when a
   comprehensive evaluation of the patient is therefore necessary.
ZS 0
Z8 0
ZB 0
ZR 0
TC 0
ZA 0
Z9 0
DA 2025-04-25
UT WOS:001468330700001
PM 39918656
ER

PT J
AU Jang, B. S.
   Alcorn, S. R.
   McNutt, T. R.
   Ehsan, U.
TI Hype or Reality: Utility of Large Language Models in Radiation Oncology
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3382
BP E629
EP E630
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZB 0
Z8 0
ZR 0
ZA 0
TC 0
ZS 0
Z9 0
DA 2024-12-16
UT WOS:001325892302063
ER

PT C
AU Sharma, Manish
   Farough, Samira
   Burkett, Andre
   Prasanth, Jerome
   El-Shafeey, Nabil
   Zygadlo, Dominic
   Dunn, Chera
   Korn, Ron
BE Yoshida, H
   Wu, S
TI Leveraging LLMs like ChatGPT for robust quality checks and medical text
   agreement rationale enhancing adjudication quality and alignment in BICR
   for oncology clinical trials
SO IMAGING INFORMATICS FOR HEALTHCARE, RESEARCH, AND APPLICATIONS, MEDICAL
   IMAGING 2024
SE Proceedings of SPIE
VL 12931
AR 1293103
DI 10.1117/12.3009153
DT Proceedings Paper
PD 2024
PY 2024
AB Purpose: Blinded independent central review (BICR) is recommended by the
   US FDA for registration of oncology trials as image assessment bias is
   avoided and no chance of unblinding of patient data. Double read with
   adjudication is the method used to reduce endpoint assessment
   variability. In cases of disagreement between the readers, a third
   reader called an adjudicator, reviews the assessment by the two
   radiologists and decides which assessment is most accurate. Adjudication
   rate (AR) and adjudicator agreement rate (AAR) are the two indicators
   used to evaluate reviewer performance and overall trial variability and
   quality. Sentiment analysis (SA) is based on natural language processing
   and can tag the data as 'positive', 'negative' or 'neutral' although
   current technologies can provide a more complex analysis of emotions in
   the written text. Medical SA can analyze patients' and doctors'
   opinions, sentiments, attitudes, and emotions in the clinical
   background. Python, the most frequently used programming language for
   deep learning worldwide and ChatGPT, an AI-based chatbot can be used for
   assessing adjudicator comment quality based on sentiment analysis. If
   successful, this analysis can open another novel implementation for
   Large Language Models (LLMs) or ChatGPT in clinical research and medical
   imaging.
   Methods: This prospective study involved the review of cases for 100
   subjects by board-certified radiologists using the Response Evaluation
   Criteria in Solid Tumors (RECIST) 1.1 criteria. The study employed a
   double read with adjudication paradigm in a central imaging review
   setup. The agreement of adjudication was assessed and compared with the
   overall response, agreed reader, and medical text. The medical text
   entered by the adjudicator is usually a free text field that typically
   lacks standardization and control over its content, which may affect its
   correlation with reviewer selection for agreement. Although uncommon,
   errors by the adjudicator can occur due to ambiguous text, mis-clicks,
   or application delay errors. To analyze the adjudicator's comments,
   sentiment analysis was conducted using a Python plug-in with ChatGPT as
   a large language model. Based on this analysis, the subjects were
   categorized as either having "Potential Error" or "No Error".
   Results: The algorithm supported by ChatGPT was evaluated against a Gold
   Standard, determined by a board-certified radiologist with over 20 years
   of experience in the BICR process. A comparison was made to assess
   accuracy and reproducibility, revealing that only 4 out of 100 subjects
   had different outcomes. The sensitivity was calculated as 0.857,
   specificity as 1.0, and accuracy as 0.96.
   Conclusions: The remarkable Natural Language Processing (NLP)
   capabilities of ChatGPT are evident in its ability to classify the
   sentiment as positive, negative, or neutral based on the free-text
   adjudicator comments provided during the review process. This
   classification enables a comparison with the actual assessment,
   adjudicator agreement, and overall patient outcome, highlighting the
   impressive performance of ChatGPT in this regard.
CT Conference on Medical Imaging - Imaging Informatics for Healthcare,
   Research, and Applications
CY FEB 19-21, 2024
CL San Diego, CA
SP SPIE; Amer Assoc Physicists Med; Radiol Soc N Amer; World Mol Imaging
   Soc; Soc Imaging Informat Med; Int Fdn Comp Assisted Radiol & Surg; Med
   Image Percept Soc
ZA 0
ZS 0
ZR 0
TC 0
ZB 0
Z8 0
Z9 0
DA 2024-05-31
UT WOS:001219280700002
ER

PT J
AU Bhayana, Rajesh
   Alwahbi, Omar
   Ladak, Aly Muhammad
   Deng, Yangqing
   Dias, Adriano Basso
   Elbanna, Khaled
   Gomez, Jorge Abreu
   Jajodia, Ankush
   Jhaveri, Kartik
   Johnson, Sarah
   Kajal, Dilkash
   Wang, David
   Soong, Christine
   Kielar, Ania
   Krishna, Satheesh
TI Leveraging Large Language Models to Generate Clinical Histories for
   Oncologic Imaging Requisitions
SO RADIOLOGY
VL 314
IS 2
AR e242134
DI 10.1148/radiol.242134
DT Article
PD FEB 2025
PY 2025
AB Background: Clinical information improves imaging interpretation, but
   physician-provided histories on requisitions for oncologic imaging often
   lack key details. Purpose: To evaluate large language models (LLMs) for
   automatically generating clinical histories for oncologic imaging
   requisitions from clinical notes and compare them with original
   requisition histories. Materials and Methods: In total, 207 patients
   with CT performed at a cancer center from January to November 2023 and
   with an electronic health record clinical note coinciding with ordering
   date were randomly selected. A multidisciplinary team informed selection
   of 10 parameters important for oncologic imaging history, including
   primary oncologic diagnosis, treatment history, and acute symptoms.
   Clinical notes were independently reviewed to establish the reference
   standard regarding presence of each parameter. After prompt engineering
   with seven patients, GPT-4 (version 0613; OpenAI) was prompted on April
   9, 2024, to automatically generate structured clinical histories for the
   200 remaining patients. Using the reference standard, LLM extraction
   performance was calculated (recall, precision, F1 score). LLM-generated
   and original requisition histories were compared for completeness
   (proportion including each parameter), and 10 radiologists performed
   pairwise comparison for quality, preference, and subjective likelihood
   of harm. Results: For the 200 LLM-generated histories, GPT-4 performed
   well, extracting oncologic parameters from clinical notes (F1 = 0.983).
   Compared with original requisition histories, LLM-generated histories
   more frequently included parameters critical for radiologist
   interpretation, including primary oncologic diagnosis (99.5% vs 89% [199
   and 178 of 200 histories, respectively]; P < .001), acute or worsening
   symptoms (15% vs 4% [29 and seven of 200]; P < .001), and relevant
   surgery (61% vs 12% [122 and 23 of 200]; P < .001). Radiologists
   preferred LLM-generated histories for imaging interpretation (89% vs 5%,
   7% equal; P < .001), indicating they would enable more complete
   interpretation (86% vs 0%, 15% equal; P < .001) and have a lower
   likelihood of harm (3% vs 55%, 42% neither; P < .001). Conclusion: An
   LLM enabled accurate automated clinical histories for oncologic imaging
   from clinical notes. Compared with original requisition histories,
   LLM-generated histories were more complete and were preferred by
   radiologists for imaging interpretation and perceived safety.
ZA 0
Z8 0
ZR 0
ZB 0
ZS 0
TC 1
Z9 1
DA 2025-03-08
UT WOS:001434851700023
PM 39903072
ER

PT J
AU Jinia, A. J.
   Chapman, K. L.
   Liu, S.
   Della Biancia, C.
   Li, A.
   Moran, J. M.
TI Challenges in Developing an Al -Based Analysis System for Incident
   Learning
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3198
BP E542
EP E542
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZB 0
ZS 0
Z8 0
ZA 0
ZR 0
TC 0
Z9 0
DA 2024-12-16
UT WOS:001325892301523
ER

PT J
AU Schaden, Eva
   Dier, Helga
   Weixler, Dietmar
   Hasibeder, Walter
   Lenhart-Orator, Andrea
   Roden, Christian
   Fruhwald, Sonja
   Friesenecker, Barbara
CA OGARI
TI Comfort Terminal Care in the intensive care unit: recommendations for
   practice
SO Anaesthesiologie
VL 73
IS 3, Sp. Iss. SI
BP 186
EP 192
DI 10.1007/s00101-024-01382-9
DT Article
PD MAR 2024
PY 2024
AB Background and objective The Working Group on Ethics in Anesthesia and
   Intensive Care Medicine of the Austrian Society for Anesthesiology
   Resuscitation and Intensive Care Medicine (& Ouml;GARI) already
   developed documentation tools for the adaption of therapeutic goals 10
   years ago. Since then the practical implementation of Comfort Terminal
   Care in the daily routine in particular has raised numerous questions,
   which are discussed in this follow-up paper and answered in an
   evidence-based manner whenever possible. Results The practical
   implementation of pain therapy and reduction of anxiety, stress and
   respiratory distress that are indicated in the context of Comfort
   Terminal Care are described in more detail. The measures that are not
   (or no longer) indicated, such as oxygen administration and ventilation
   as well as the administration of fluids and nutrition, are also
   commented on. Furthermore, recommendations are given regarding
   monitoring, (laboratory) findings and drug treatment and the importance
   of nursing actions in the context of Comfort Terminal Care is mentioned.
   Finally, the support for the next of kin and the procedure in the time
   after death are presented. Discussion A change in treatment goals with a
   timely switch to Comfort Terminal Care enables good and humane care for
   seriously ill patients and their relatives at the end of life and the
   appreciation of their previous life with the possibility of positive
   experiences until the end.
TC 0
ZB 0
ZA 0
ZS 0
Z8 0
ZR 0
Z9 0
DA 2024-07-04
UT BCI:BCI202400562911
PM 38315182
ER

PT B
AU Asly, Amneh
Z2  
TI Developing Objective Chronic Pain Assessment Based on Linguistic
   Characteristics of Patients' Narratives
DT Dissertation/Thesis
PD Jan 01 2024
PY 2024
TC 0
ZA 0
ZR 0
ZB 0
Z8 0
ZS 0
Z9 0
UT PQDT:119375393
ER

PT J
AU Busch, Felix
   Hoffmann, Lena
   Rueger, Christopher
   van Dijk, Elon H. C.
   Kader, Rawen
   Ortiz-Prado, Esteban
   Makowski, Marcus R.
   Saba, Luca
   Hadamitzky, Martin
   Kather, Jakob Nikolas
   Truhn, Daniel
   Cuocolo, Renato
   Adams, Lisa C.
   Bressem, Keno K.
TI Current applications and challenges in large language models for patient
   care: a systematic review
SO COMMUNICATIONS MEDICINE
VL 5
IS 1
AR 26
DI 10.1038/s43856-024-00717-2
DT Article
PD JAN 21 2025
PY 2025
AB BackgroundThe introduction of large language models (LLMs) into clinical
   practice promises to improve patient education and empowerment, thereby
   personalizing medical care and broadening access to medical knowledge.
   Despite the popularity of LLMs, there is a significant gap in
   systematized information on their use in patient care. Therefore, this
   systematic review aims to synthesize current applications and
   limitations of LLMs in patient care.MethodsWe systematically searched 5
   databases for qualitative, quantitative, and mixed methods articles on
   LLMs in patient care published between 2022 and 2023. From 4349 initial
   records, 89 studies across 29 medical specialties were included. Quality
   assessment was performed using the Mixed Methods Appraisal Tool 2018. A
   data-driven convergent synthesis approach was applied for thematic
   syntheses of LLM applications and limitations using free line-by-line
   coding in Dedoose.ResultsWe show that most studies investigate
   Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124
   different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering
   medical questions, followed by patient information generation, including
   medical text summarization or translation, and clinical documentation.
   Our analysis delineates two primary domains of LLM limitations: design
   and output. Design limitations include 6 second-order and 12 third-order
   codes, such as lack of medical domain optimization, data transparency,
   and accessibility issues, while output limitations include 9
   second-order and 32 third-order codes, for example, non-reproducibility,
   non-comprehensiveness, incorrectness, unsafety, and bias.ConclusionsThis
   review systematically maps LLM applications and limitations in patient
   care, providing a foundational framework and taxonomy for their
   implementation and evaluation in healthcare settings.
ZR 0
ZA 0
ZB 0
Z8 0
ZS 0
TC 8
Z9 8
DA 2025-01-29
UT WOS:001402540700001
PM 39838160
ER

PT J
AU Horesh, Nir
   Emile, Sameh
   Gupta, Shashank
   Garoufalia, Zoe
   Gefen, Rachel
   Zhou, Peige
   da Silva, Giovanna
   Wexner, Steven
TI Comparing the Management Recommendations of Large Language Model and
   Colorectal Cancer Multidisciplinary Team: A Pilot Study
SO DISEASES OF THE COLON & RECTUM
VL 68
IS 1
BP 41
EP 47
DI 10.1097/DCR.0000000000003504
DT Article
PD JAN 2025
PY 2025
AB BACKGROUND:Management of anorectal cancers requires a multidisciplinary
   team approach. Recently, large language models have been suggested as
   potential tools for various applications in health care.OBJECTIVE:Assess
   suggested management recommendations provided by a generative artificial
   intelligence chatbot with those of a colorectal cancer multidisciplinary
   team to evaluate applicability in clinical settings.DESIGN:Comparative
   pilot study where management recommendations from a generative
   artificial intelligence chatbot for patients with anal or colorectal
   cancers were compared against historical consensus decisions from
   multidisciplinary team meetings.SETTING:Single referral tertiary
   center.PATIENTS:Fifteen patients (mean age of 66.5 years; 53.5% woman)
   were included; 80% were primarily diagnosed with rectal cancer,
   predominantly stage II and III disease (46.6%). The mean tumor height
   from the anal verge was 4 cm.INTERVENTIONS:From a generative artificial
   intelligence chatbot, we generated management recommendations for each
   patient, which were subsequently compared to historical decisions from a
   multidisciplinary team to gauge concordance.MAIN OUTCOME
   MEASURES:Primary outcomes included a degree of concordance between
   generative artificial intelligence chatbot recommendations and the
   multidisciplinary team decisions, assessed on a scale from 1 (complete
   disagreement) to 5 (complete agreement), and justification was evaluated
   by 3 experienced colorectal surgeons.RESULTS:A generative artificial
   intelligence chatbot achieved a high concordance rate with
   multidisciplinary team decisions, with an average concordance rating of
   4.08. Multidisciplinary team treatment strategies included neoadjuvant
   therapy for 33.3% of patients, upfront surgery for 26.6%, and further
   diagnostic assessment for 20%. Interrater agreement on concordance was
   found to be moderate (kappa coefficient range, 0.333-0.577), whereas
   agreement on decision justification was slight (kappa coefficient range,
   0.047-0.094).LIMITATIONS:Retrospective study with small sample
   size.CONCLUSIONS:The findings indicate a high level of concordance
   between generative artificial intelligence chatbot recommendations and
   the decisions from a colorectal cancer multidisciplinary team,
   suggesting the potential of large language models to support clinical
   decision-making in the management of anal and colorectal cancers. See
   Video Abstract.COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL
   MODELO EXTENSO DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C &
   Aacute;NCER COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los
   c & aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.
   PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a & ntilde;os;
   53,5% mujeres); el 80% fueron diagnosticados principalmente de c &
   aacute;ncer de recto, con predominio de la enfermedad en estadio II-III
   (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.
   INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la concordancia.
   PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios incluyeron el
   grado de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones del equipo
   multidisciplinario, evaluadas en una escala de 1 (desacuerdo total) a 5
   (acuerdo total), y la justificaci & oacute;n evaluada por tres cirujanos
   colorrectales experimentados.RESULTADOS:Un chatbot de inteligencia
   artificial generativa logr & oacute; una alta tasa de concordancia con
   las decisiones del equipo multidisciplinario, con una calificaci &
   oacute;n media de concordancia de 4,08. Las estrategias de tratamiento
   del equipo multidisciplinario incluyeron terapia neoadyuvante para el
   33,3% de los pacientes, cirug & iacute;a inicial para el 26,6% y
   evaluaci & oacute;n diagn & oacute;stica adicional para el 20%. La
   concordancia entre los evaluadores fue moderada (rango del coeficiente
   kappa: 0,333 a 0,577), mientras que la concordancia en la justificaci &
   oacute;n de las decisiones fue leve (rango del coeficiente kappa: 0,047
   a 0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales experimentados.
   RESULTADOS:Un chatbot de inteligencia artificial generativa logr &
   oacute; una alta tasa de concordancia con las decisiones del equipo
   multidisciplinario, con una calificaci & oacute;n media de concordancia
   de 4,08. Las estrategias de tratamiento del equipo multidisciplinario
   incluyeron terapia neoadyuvante para el 33,3% de los pacientes, cirug &
   iacute;a inicial para el 26,6% y evaluaci & oacute;n diagn &
   oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%.
   La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.
   CONCLUSIONES:Los hallazgos indican un alto nivel de concordancia entre
   las recomendaciones de un chatbot de inteligencia artificial generativa
   y las decisiones de un equipo multidisciplinario de c & aacute;ncer
   colorrectal, lo que sugiere el potencial de los modelos extensos de
   lenguaje en apoyar la toma de decisiones cl & iacute;nicas en el manejo
   del c & aacute;ncer anal y colorrectal. (Traducci & oacute;n: Dr. Fidel
   Ruiz Healy).COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL
   MODELO EXTENSO DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C &
   Aacute;NCER COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los
   c & aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).
   COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO
   DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER
   COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los c &
   aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.
   OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n sugeridos por
   un chatbot de inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones
   LUGAR:Un & uacute;nico centro terciario de referencia.PACIENTES:Se
   incluyeron 15 pacientes (edad media de 66,5 a & ntilde;os; 53,5%
   mujeres); el 80% fueron diagnosticados principalmente de c & aacute;ncer
   de recto, con predominio de la enfermedad en estadio II-III (46,6%). La
   altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.
   INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la concordancia.
   PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios incluyeron el
   grado de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones del equipo
   multidisciplinario, evaluadas en una escala de 1 (desacuerdo total) a 5
   (acuerdo total), y la justificaci & oacute;n evaluada por tres cirujanos
   colorrectales experimentados.RESULTADOS:Un chatbot de inteligencia
   artificial generativa logr & oacute; una alta tasa de concordancia con
   las decisiones del equipo multidisciplinario, con una calificaci &
   oacute;n media de concordancia de 4,08. Las estrategias de tratamiento
   del equipo multidisciplinario incluyeron terapia neoadyuvante para el
   33,3% de los pacientes, cirug & iacute;a inicial para el 26,6% y
   evaluaci & oacute;n diagn & oacute;stica adicional para el 20%. La
   concordancia entre los evaluadores fue moderada (rango del coeficiente
   kappa: 0,333 a 0,577), mientras que la concordancia en la justificaci &
   oacute;n de las decisiones fue leve (rango del coeficiente kappa: 0,047
   a 0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales experimentados.
   RESULTADOS:Un chatbot de inteligencia artificial generativa logr &
   oacute; una alta tasa de concordancia con las decisiones del equipo
   multidisciplinario, con una calificaci & oacute;n media de concordancia
   de 4,08. Las estrategias de tratamiento del equipo multidisciplinario
   incluyeron terapia neoadyuvante para el 33,3% de los pacientes, cirug &
   iacute;a inicial para el 26,6% y evaluaci & oacute;n diagn &
   oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%.
   La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.
   CONCLUSIONES:Los hallazgos indican un alto nivel de concordancia entre
   las recomendaciones de un chatbot de inteligencia artificial generativa
   y las decisiones de un equipo multidisciplinario de c & aacute;ncer
   colorrectal, lo que sugiere el potencial de los modelos extensos de
   lenguaje en apoyar la toma de decisiones cl & iacute;nicas en el manejo
   del c & aacute;ncer anal y colorrectal. (Traducci & oacute;n: Dr. Fidel
   Ruiz Healy).COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL
   MODELO EXTENSO DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C &
   Aacute;NCER COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los
   c & aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).
   COMPARACI & Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO
   DE LENGUAJE Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER
   COLORRECTAL: UN ESTUDIO PILOTOANTECEDENTES:El manejo de los c &
   aacute;nceres anorrectales requiere un enfoque de equipo
   multidisciplinario. Recientemente, se han sugerido modelos extensos de
   lenguaje como herramientas potenciales para diversas aplicaciones en la
   asistencia sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti &
   oacute;n sugeridos por un chatbot de inteligencia artificial generativa
   con las de un equipo multidisciplinario de c & aacute;ncer colorrectal
   para evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.
   OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n sugeridos por
   un chatbot de inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones consensuadas hist &
   oacute;ricas de reuniones de equipos multidisciplinarios.LUGAR:Un &
   uacute;nico centro terciario de referencia.PACIENTES:Se incluyeron 15
   pacientes (edad media de 66,5 a & ntilde;os; 53,5% mujeres); el 80%
   fueron diagnosticados principalmente de c & aacute;ncer de recto, con
   predominio de la enfermedad en estadio II-III (46,6%). La altura media
   del tumor desde el borde anal fue de 4 cm.INTERVENCIONESUtilizando de un
   chatbot de inteligencia artificial generativa, producimos
   recomendaciones de manejo para cada paciente, que posteriormente se
   compararon con las decisiones del equipo multidisciplinario hist &
   oacute;rico para medir la concordancia.PRINCIPALES MEDIDAS DE
   RESULTADO:Los resultados primarios incluyeron el grado de concordancia
   entre las recomendaciones de un chatbot de inteligencia artificial
   generativa y las decisiones del equipo multidisciplinario, evaluadas en
   una escala de 1 (desacuerdo total) a 5 (acuerdo total), y la justificaci
   & oacute;n evaluada por tres cirujanos colorrectales
   experimentados.RESULTADOS:Un chatbot de inteligencia artificial
   generativa logr & oacute; una alta tasa de concordancia con las
   decisiones del equipo multidisciplinario, con una calificaci & oacute;n
   media de concordancia de 4,08. Las estrategias de tratamiento del equipo
   multidisciplinario incluyeron terapia neoadyuvante para el 33,3% de los
   pacientes, cirug & iacute;a inicial para el 26,6% y evaluaci & oacute;n
   diagn & oacute;stica adicional para el 20%. La concordancia entre los
   evaluadores fue moderada (rango del coeficiente kappa: 0,333 a 0,577),
   mientras que la concordancia en la justificaci & oacute;n de las
   decisiones fue leve (rango del coeficiente kappa: 0,047 a
   0,094).LIMITACIONES:Estudio retrospectivo con peque & ntilde;o tama &
   ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un alto nivel de
   concordancia entre las recomendaciones de un chatbot de inteligencia
   artificial generativa y las decisiones de un equipo multidisciplinario
   de c & aacute;ncer colorrectal, lo que sugiere el potencial de los
   modelos extensos de lenguaje en apoyar la toma de decisiones cl &
   iacute;nicas en el manejo del c & aacute;ncer anal y colorrectal.
   (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI & Oacute;N ENTRE
   RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE Y EL EQUIPO
   MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN ESTUDIO
   PILOTOANTECEDENTES:El manejo de los c & aacute;nceres anorrectales
   requiere un enfoque de equipo multidisciplinario. Recientemente, se han
   sugerido modelos extensos de lenguaje como herramientas potenciales para
   diversas aplicaciones en la asistencia sanitaria.OBJETIVO:Evaluar las
   recomendaciones de gesti & oacute;n sugeridos por un chatbot de
   inteligencia artificial generativa con las de un equipo
   multidisciplinario de c & aacute;ncer colorrectal para evaluar la
   aplicabilidad en entornos cl & iacute;nicos.DISE & Ntilde;O:Estudio
   piloto comparativo entre las recomendaciones de gesti & oacute;n de un
   chatbot de inteligencia artificial generativa con pacientes de c &
   aacute;ncer anal o colorrectal y con las decisiones
   LUGAR:Un & uacute;nico centro terciario de referencia.PACIENTES:Se
   incluyeron 15 pacientes (edad media de 66,5 a & ntilde;os; 53,5%
   mujeres); el 80% fueron diagnosticados principalmente de c & aacute;ncer
   de recto, con predominio de la enfermedad en estadio II-III (46,6%). La
   altura media del tumor desde el borde anal fue de 4
   cm.INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).COMPARACI &
   Oacute;N ENTRE RECOMENDACIONES DE MANEJO DEL MODELO EXTENSO DE LENGUAJE
   Y EL EQUIPO MULTIDISCIPLINARIO DE C & Aacute;NCER COLORRECTAL: UN
   ESTUDIO PILOTOANTECEDENTES:El manejo de los c & aacute;nceres
   anorrectales requiere un enfoque de equipo multidisciplinario.
   Recientemente, se han sugerido modelos extensos de lenguaje como
   herramientas potenciales para diversas aplicaciones en la asistencia
   sanitaria.OBJETIVO:Evaluar las recomendaciones de gesti & oacute;n
   sugeridos por un chatbot de inteligencia artificial generativa con las
   de un equipo multidisciplinario de c & aacute;ncer colorrectal para
   evaluar la aplicabilidad en entornos cl & iacute;nicos.DISE &
   Ntilde;O:Estudio piloto comparativo entre las recomendaciones de gesti &
   oacute;n de un chatbot de inteligencia artificial generativa con
   pacientes de c & aacute;ncer anal o colorrectal y con las decisiones
   consensuadas hist & oacute;ricas de reuniones de equipos
   multidisciplinarios.LUGAR:Un & uacute;nico centro terciario de
   referencia.PACIENTES:Se incluyeron 15 pacientes (edad media de 66,5 a &
   ntilde;os; 53,5% mujeres); el 80% fueron diagnosticados principalmente
   de c & aacute;ncer de recto, con predominio de la enfermedad en estadio
   II-III (46,6%). La altura media del tumor desde el borde anal fue de 4
   cm.
   INTERVENCIONESUtilizando de un chatbot de inteligencia artificial
   generativa, producimos recomendaciones de manejo para cada paciente, que
   posteriormente se compararon con las decisiones del equipo
   multidisciplinario hist & oacute;rico para medir la
   concordancia.PRINCIPALES MEDIDAS DE RESULTADO:Los resultados primarios
   incluyeron el grado de concordancia entre las recomendaciones de un
   chatbot de inteligencia artificial generativa y las decisiones del
   equipo multidisciplinario, evaluadas en una escala de 1 (desacuerdo
   total) a 5 (acuerdo total), y la justificaci & oacute;n evaluada por
   tres cirujanos colorrectales experimentados.RESULTADOS:Un chatbot de
   inteligencia artificial generativa logr & oacute; una alta tasa de
   concordancia con las decisiones del equipo multidisciplinario, con una
   calificaci & oacute;n media de concordancia de 4,08. Las estrategias de
   tratamiento del equipo multidisciplinario incluyeron terapia
   neoadyuvante para el 33,3% de los pacientes, cirug & iacute;a inicial
   para el 26,6% y evaluaci & oacute;n diagn & oacute;stica adicional para
   el 20%. La concordancia entre los evaluadores fue moderada (rango del
   coeficiente kappa: 0,333 a 0,577), mientras que la concordancia en la
   justificaci & oacute;n de las decisiones fue leve (rango del coeficiente
   kappa: 0,047 a 0,094).LIMITACIONES:Estudio retrospectivo con peque &
   ntilde;o tama & ntilde;o muestral.CONCLUSIONES:Los hallazgos indican un
   alto nivel de concordancia entre las recomendaciones de un chatbot de
   inteligencia artificial generativa y las decisiones de un equipo
   multidisciplinario de c & aacute;ncer colorrectal, lo que sugiere el
   potencial de los modelos extensos de lenguaje en apoyar la toma de
   decisiones cl & iacute;nicas en el manejo del c & aacute;ncer anal y
   colorrectal. (Traducci & oacute;n: Dr. Fidel Ruiz Healy).
ZS 0
ZA 0
ZB 0
ZR 0
Z8 0
TC 0
Z9 0
DA 2025-01-05
UT WOS:001385242400023
PM 39679608
ER

EF