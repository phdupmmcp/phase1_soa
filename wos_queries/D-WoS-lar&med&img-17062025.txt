FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Chao, Chieh Ju
   Banerjee, Imon
   Arsanjani, Reza
   Ayoub, Chadi
   Tseng, Andrew S.
   Kane, Garvan C.
   Oh, Jae K.
   Li, Fei-Fei
   Adeli, Ehsan
   Langlotz, Curtis P.
TI BUILDING ECHOGPT: A LARGE LANGUAGE MODEL FOR ECHOCARDIOGRAPHY REPORT
   SUMMARIZATION
SO JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY
VL 85
IS 12
BP 4691
EP 4691
SU S
DT Meeting Abstract
PD APR 1 2025
PY 2025
CT Annual Meeting of the American-College-of-Cardiology (ACC)
CY MAR 29-31, 2025
CL Chicago, IL
SP Amer Coll Cardiol
Z8 0
TC 0
ZS 0
ZA 0
ZB 0
ZR 0
Z9 0
DA 2025-04-23
UT WOS:001469683900040
ER

PT J
AU Shamsi, Amrollah
   Sebo, Paul
   Wang, Ting
TI Radiology, Nuclear Medicine and Medical Imaging at the vanguard of
   artificial intelligence integration: A preliminary study of AI in
   medical research
SO IRANIAN JOURNAL OF NUCLEAR MEDICINE
VL 33
IS 1
BP 1
EP 3
DI 10.22034/irjnm.2024.129795.1645
DT Letter
PD JAN 2025
PY 2025
ZR 0
Z8 0
TC 0
ZS 0
ZA 0
ZB 0
Z9 0
DA 2024-11-30
UT WOS:001362632600001
ER

PT J
AU Li, Chuanxue
   Wang, Ping
   Zheng, Meifang
   Li, Wenxiang
   Zhou, Jun
   Fu, Lin
TI One-stop multi-sensor fusion and multimodal precise quantified
   traditional Chinese medicine imaging health examination technology
SO JOURNAL OF RADIATION RESEARCH AND APPLIED SCIENCES
VL 17
IS 4
AR 101038
DI 10.1016/j.jrras.2024.101038
EA DEC 2024
DT Article
PD DEC 2024
PY 2024
AB Except for single-mode traditional Chinese medicine imaging techniques
   such as infrared thermal imaging, the one-stop multimodal whole-body
   imaging health examination technology and device is still blank. We
   focus on infrared thermal imaging as the main modality, integrated
   various modalities of medical imaging intelligent sensing agents such as
   terahertz imaging. The upper and lower computer and virtual instrument
   architecture are used, and the imaging data are collected by the lower
   computers that each is an intelligent sensing agent. The upper computer
   is used for image reconstruction with intelligent algorithms. Based on
   the core theory of traditional Chinese medicine, intelligent fusion
   imaging is achieved through various modalities to achieve the
   'observation, hearing, questioning, and palpation' four diagnostic
   integration. We use fractional Fourier transform to filter imaging data,
   Laplacian pyramid for image fusion. We have proposed an implementation
   method and process for combining traditional Chinese medicine imaging
   large language model with knowledge graph, and based on deep learning,
   we have studied the image and report generation algorithm that combines
   traditional Chinese medicine pathology and four diagnostic methods with
   knowledge graph fusion, as well as the traditional Chinese medicine
   human physiological and pathological interpretation and evaluation
   system. We have achieved some results, and through further research and
   development, we can achieve commercial applications.
ZA 0
Z8 0
ZB 0
TC 2
ZR 0
ZS 0
Z9 2
DA 2025-01-03
UT WOS:001385575000001
ER

PT J
AU Han, Tianyu
   Adams, Lisa C.
   Bressem, Keno K.
   Busch, Felix
   Nebelung, Sven
   Truhn, Daniel
TI Comparative Analysis of Multimodal Large Language Model Performance on
   Clinical Vignette Questions
SO JAMA-JOURNAL OF THE AMERICAN MEDICAL ASSOCIATION
VL 331
IS 15
BP 1320
EP 1321
DI 10.1001/jama.2023.27861
EA APR 2024
DT Letter
PD APR 16 2024
PY 2024
AB This study compares 2 large language models and their performance vs
   that of competing open-source models.
ZA 0
ZB 1
Z8 0
ZS 0
ZR 0
TC 29
Z9 29
DA 2024-04-04
UT WOS:001189212200002
PM 38497956
ER

PT J
AU Wray, Rick
   Yeh, Randy
TI Multimodal Large Language Model Based PET/CT Report Generation:
   Capabilities and Comparison of ChatGPT-4 Versus ChatGPT-3.5
SO JOURNAL OF NUCLEAR MEDICINE
VL 65
MA 242447
SU 2
DT Meeting Abstract
PD JUN 1 2024
PY 2024
CT Annual Meeting of the Society-of-Nuclear-Medicine-and-Molecular-Imaging
   (SNMMI)
CY JUN 08-11, 2024
CL Toronto, CANADA
SP Soc Nuclear Med & Mol Imaging
ZS 0
TC 0
ZR 0
ZA 0
Z8 0
ZB 0
Z9 0
DA 2024-12-16
UT WOS:001289165603284
ER

PT J
AU Orlhac, Fanny
   Bradshaw, Tyler
   Buvat, Irene
TI Can a large language model be an effective assistant for literature
   reviews? An example in Radiomics
SO JOURNAL OF NUCLEAR MEDICINE
VL 65
MA 241031
SU 2
DT Meeting Abstract
PD JUN 1 2024
PY 2024
CT Annual Meeting of the Society-of-Nuclear-Medicine-and-Molecular-Imaging
   (SNMMI)
CY JUN 08-11, 2024
CL Toronto, CANADA
SP Soc Nuclear Med & Mol Imaging
Z8 0
ZB 0
ZA 0
TC 0
ZS 0
ZR 0
Z9 0
DA 2024-12-16
UT WOS:001289165600066
ER

PT J
AU Hirata, Kenji
   Matsui, Yusuke
   Yamada, Akira
   Fujioka, Tomoyuki
   Yanagawa, Masahiro
   Nakaura, Takeshi
   Ito, Rintaro
   Ueda, Daiju
   Fujita, Shohei
   Tatsugami, Fuminari
   Fushimi, Yasutaka
   Tsuboyama, Takahiro
   Kamagata, Koji
   Nozaki, Taiki
   Fujima, Noriyuki
   Kawamura, Mariko
   Naganawa, Shinji
TI Generative AI and large language models in nuclear medicine: current
   status and future prospects
SO ANNALS OF NUCLEAR MEDICINE
VL 38
IS 11
BP 853
EP 864
DI 10.1007/s12149-024-01981-x
EA SEP 2024
DT Review
PD NOV 2024
PY 2024
AB This review explores the potential applications of Large Language Models
   (LLMs) in nuclear medicine, especially nuclear medicine examinations
   such as PET and SPECT, reviewing recent advancements in both fields.
   Despite the rapid adoption of LLMs in various medical specialties, their
   integration into nuclear medicine has not yet been sufficiently
   explored. We first discuss the latest developments in nuclear medicine,
   including new radiopharmaceuticals, imaging techniques, and clinical
   applications. We then analyze how LLMs are being utilized in radiology,
   particularly in report generation, image interpretation, and medical
   education. We highlight the potential of LLMs to enhance nuclear
   medicine practices, such as improving report structuring, assisting in
   diagnosis, and facilitating research. However, challenges remain,
   including the need for improved reliability, explainability, and bias
   reduction in LLMs. The review also addresses the ethical considerations
   and potential limitations of AI in healthcare. In conclusion, LLMs have
   significant potential to transform existing frameworks in nuclear
   medicine, making it a critical area for future research and development.
ZR 0
ZA 0
TC 4
ZB 1
ZS 0
Z8 0
Z9 4
DA 2024-09-30
UT WOS:001319554200001
PM 39320419
ER

PT J
AU Amin, Kanhai
   Khosla, Pavan
   Doshi, Rushabh
   Chheang, Sophie
   Forman, Howard P.
TI Artificial Intelligence to Improve Patient Understanding of Radiology
   Reports
SO YALE JOURNAL OF BIOLOGY AND MEDICINE
VL 96
IS 3
BP 407
EP 414
DT Review
PD SEP 2023
PY 2023
AB Diagnostic imaging reports are generally written with a target audience
   of other providers. As a result, the reports are written with medical
   jargon and technical detail to ensure accurate communication. With
   implementation of the 21st Century Cures Act, patients have greater and
   quicker access to their imaging reports, but these reports are still
   written above the comprehension level of the average patient.
   Consequently, many patients have requested reports to be conveyed in
   language accessible to them. Numerous studies have shown that improving
   patient understanding of their condition results in better outcomes, so
   driving comprehension of imaging reports is essential. Summary
   statements, second reports, and the inclusion of the radiologist's phone
   number have been proposed, but these solutions have implications for
   radiologist workflow. Artificial intelligence (AI) has the potential to
   simplify imaging reports without significant disruptions. Many AI
   technologies have been applied to radiology reports in the past for
   various clinical and research purposes, but patient focused solutions
   have largely been ignored. New natural language processing technologies
   and large language models (LLMs) have the potential to improve patient
   understanding of their imaging reports. However, LLMs are a nascent
   technology and significant research is required before LLM-driven report
   simplification is used in patient care.
ZS 0
ZR 0
Z8 0
ZB 3
TC 22
ZA 0
Z9 22
DA 2023-11-28
UT WOS:001098576800008
PM 37780992
ER

PT J
AU Waisberg, Ethan
   Ong, Joshua
   Masalkhi, Mouayad
   Zaman, Nasif
   Sarker, Prithul
   Lee, Andrew G.
   Tavakkoli, Alireza
TI Meta smart glasses-large language models and the future for assistive
   glasses for individuals with vision impairments
SO EYE
VL 38
IS 6
BP 1036
EP 1038
DI 10.1038/s41433-023-02842-z
EA DEC 2023
DT Editorial Material
PD APR 2024
PY 2024
ZS 0
TC 9
ZA 0
ZB 1
Z8 0
ZR 0
Z9 9
DA 2023-12-14
UT WOS:001113616200003
PM 38049627
ER

PT J
AU Takahashi, Ippei
   Obara, Taku
   Kuriyama, Shinichi
TI An Overarching Framework for the Ethics of Artificial Intelligence in
   Pediatrics
SO JAMA PEDIATRICS
VL 178
IS 3
BP 213
EP 214
DI 10.1001/jamapediatrics.2023.5761.
EA MAR 2024
DT Editorial Material
PD MAR 2024
PY 2024
ZR 0
ZS 0
ZB 2
ZA 0
Z8 0
TC 6
Z9 6
DA 2024-11-07
UT WOS:001136984300001
PM 38165711
ER

PT J
AU Koga, Shunsuke
   Du, Wei
TI From text to image: challenges in integrating vision into ChatGPT for
   medical image interpretation
SO NEURAL REGENERATION RESEARCH
VL 20
IS 2
BP 487
EP 488
DI 10.4103/NRR.NRR-D-24-00165
DT Review
PD FEB 2025
PY 2025
ZS 0
TC 6
ZR 0
ZA 0
Z8 0
ZB 1
Z9 6
DA 2024-07-18
UT WOS:001236392200024
PM 38819060
ER

PT J
AU Ge, Jin
   Li, Michael
   Delk, Molly B.
   Lai, Jennifer C.
TI PHI-PROTECTED GPT-4 ACHIEVES 93.4% OVERALL ACCURACY IN NATURAL LANGUAGE
   PROCESSING OF LONGITUDINAL HEPATOCELLULAR CARCINOMA IMAGING REPORTS
SO HEPATOLOGY
VL 79
IS 2
MA 5041-C
BP E70
EP E70
DT Meeting Abstract
PD FEB 2024
PY 2024
CT Meeting of the American-Association-for-the-Study-of-Liver-Diseases
   (AASLD)
CY NOV 10-14, 2023
CL Boston, MA
SP Amer Assoc Study Liver Dis
ZA 0
ZB 0
TC 0
ZS 0
Z8 0
ZR 0
Z9 0
DA 2024-10-18
UT WOS:001271642700075
ER

PT J
AU Kim, Kiduk
   Cho, Kyungjin
   Jang, Ryoungwoo
   Kyung, Sunggu
   Lee, Soyoung
   Ham, Sungwon
   Choi, Edward
   Hong, Gil-Sun
   Kim, Namkug
TI Updated Primer on Generative Artificial Intelligence and Large Language
   Models in Medical Imaging for Medical Professionals
SO KOREAN JOURNAL OF RADIOLOGY
VL 25
IS 3
BP 224
EP 242
DI 10.3348/kjr.2023.0818
DT Article
PD MAR 2024
PY 2024
AB The emergence of Chat Generative Pre-trained Transformer (ChatGPT), a
   chatbot developed by OpenAI, has garnered interest in the application of
   generative artificial intelligence (AI) models in the medical field.
   This review summarizes different generative AI models and their
   potential applications in the field of medicine and explores the
   evolving landscape of Generative Adversarial Networks and diffusion
   models since the introduction of generative AI models. These models have
   made valuable contributions to the field of radiology. Furthermore, this
   review also explores the significance of synthetic data in addressing
   privacy concerns and augmenting data diversity and quality within the
   medical domain, in addition to emphasizing the role of inversion in the
   investigation of generative models and outlining an approach to
   replicate this process. We provide an overview of Large Language Models,
   such as GPTs and bidirectional encoder representations (BERTs), that
   focus on prominent representatives and discuss recent initiatives
   involving language-vision models in radiology, including innovative
   large language and vision assistant for biomedicine (LLaVa-Med), to
   illustrate their practical application. This comprehensive review offers
   insights into the wide-ranging applications of generative AI models in
   clinical research and emphasizes their transformative potential.
ZR 0
Z8 0
TC 15
ZA 0
ZS 0
ZB 0
Z9 15
DA 2024-03-31
UT WOS:001179444600005
PM 38413108
ER

PT J
AU Mihalache, Andrew
   Popovic, Marko M.
   Muni, Rajeev H.
TI Need for Custom Artificial Intelligence Chatbots in Ophthalmology
SO JAMA OPHTHALMOLOGY
VL 142
IS 9
BP 806
EP 807
DI 10.1001/jamaophthalmol.2024.2738
EA JUL 2024
DT Editorial Material
PD SEP 2024
PY 2024
ZA 0
ZR 0
ZS 0
Z8 0
ZB 0
TC 1
Z9 1
DA 2024-07-27
UT WOS:001272861100001
PM 39023863
ER

PT J
AU Lyu, Zhiliang
   Zeng, Fang
   Guo, Ning
   Li, Xiang
   Li, Quanzheng
TI NM-GPT: Advancing Nuclear Medicine Report Processing Through a
   Specialized Fine-tuned Large Language Model
SO JOURNAL OF NUCLEAR MEDICINE
VL 65
MA 241934
SU 2
DT Meeting Abstract
PD JUN 1 2024
PY 2024
CT Annual Meeting of the Society-of-Nuclear-Medicine-and-Molecular-Imaging
   (SNMMI)
CY JUN 08-11, 2024
CL Toronto, CANADA
SP Soc Nuclear Med & Mol Imaging
TC 0
ZR 0
Z8 0
ZA 0
ZS 0
ZB 0
Z9 0
DA 2024-12-16
UT WOS:001289165602162
ER

PT J
AU Gerstung, Moritz
   Liu, David
   Ghassemi, Marzyeh
   Zou, James
   Chowell, Diego
   Teuwen, Jonas
   Mahmood, Faisal
   Kather, Jakob Nikolas
TI Artificial intelligence
SO CANCER CELL
VL 42
IS 6
BP 915
EP 918
DT Editorial Material
PD JUN 10 2024
PY 2024
ZS 0
ZR 0
ZA 0
Z8 0
TC 2
ZB 1
Z9 2
DA 2025-02-12
UT WOS:001412853800001
PM 38861926
ER

PT J
AU Koga, Shunsuke
   Du, Wei
TI Challenges of Integrating Chatbot Use in Ophthalmology Diagnostics
SO JAMA OPHTHALMOLOGY
VL 142
IS 9
BP 883
EP 884
DI 10.1001/jamaophthalmol.2024.2303
EA JUL 2024
DT Letter
PD SEP 2024
PY 2024
TC 0
Z8 0
ZS 0
ZA 0
ZR 0
ZB 0
Z9 0
DA 2024-07-14
UT WOS:001263237700003
PM 38958958
ER

PT J
AU Hu, Mingzhe
   Qian, Joshua
   Pan, Shaoyan
   Li, Yuheng
   Qiu, Richard L. J.
   Yang, Xiaofeng
TI Advancing medical imaging with language models: featuring a spotlight on
   ChatGPT
SO PHYSICS IN MEDICINE AND BIOLOGY
VL 69
IS 10
AR 10TR01
DI 10.1088/1361-6560/ad387d
DT Review
PD MAY 21 2024
PY 2024
AB This review paper aims to serve as a comprehensive guide and
   instructional resource for researchers seeking to effectively implement
   language models in medical imaging research. First, we presented the
   fundamental principles and evolution of language models, dedicating
   particular attention to large language models. We then reviewed the
   current literature on how language models are being used to improve
   medical imaging, emphasizing a range of applications such as image
   captioning, report generation, report classification, findings
   extraction, visual question response systems, interpretable diagnosis
   and so on. Notably, the capabilities of ChatGPT were spotlighted for
   researchers to explore its further applications. Furthermore, we covered
   the advantageous impacts of accurate and efficient language models in
   medical imaging analysis, such as the enhancement of clinical workflow
   efficiency, reduction of diagnostic errors, and assistance of clinicians
   in providing timely and accurate diagnoses. Overall, our goal is to have
   better integration of language models with medical imaging, thereby
   inspiring new ideas and innovations. It is our aspiration that this
   review can serve as a useful resource for researchers in this field,
   stimulating continued investigative and innovative pursuits of the
   application of language models in medical imaging.
ZB 2
ZS 0
Z8 0
ZR 0
ZA 0
TC 8
Z9 8
DA 2024-06-25
UT WOS:001251008700001
PM 38537293
ER

PT J
AU Sozer, Alperen
   Sahin, Mustafa Caglar
   Sozer, Batuhan
   Erol, Gokberk
   Tufek, Ozan Yavuz
   Nernekli, Kerem
   Demirtas, Zuhal
   Celtikci, Emrah
TI Do LLMs Have 'the Eye' for MRI? Evaluating GPT-4o, Grok, and Gemini on
   Brain MRI Performance: First Evaluation of Grok in Medical Imaging and a
   Comparative Analysis
SO DIAGNOSTICS
VL 15
IS 11
AR 1320
DI 10.3390/diagnostics15111320
DT Article
PD MAY 24 2025
PY 2025
AB Background/Objectives: Large language models (LLMs) are revolutionizing
   the world and the field of medicine while constantly improving
   themselves. With recent advancements in image interpretation, evaluating
   the reasoning capabilities of these models and benchmarking their
   performance on brain MRI tasks has become crucial, as they may be
   utilized-albeit off-label-for patient care by both neurosurgeons and
   non-neurosurgeons. Methods: ChatGPT-4o, Grok, and Gemini were presented
   with 35,711 slices of brain MRI, including various pathologies and
   normal MRIs. Models were asked to identify the MRI sequence and
   determine the presence of pathology. Their individual performances were
   measured and compared with one another. Results: GPT refused to answer
   28.02% of the slices despite three attempts, whereas Grok and Gemini
   provided responses on the first attempt for every slice. Gemini achieved
   74.54% pathology prediction and 46.38% sequence prediction accuracy.
   GPT-4o achieved 74.33% pathology prediction and 85.98% sequence
   prediction accuracy for questions that it had answered (53.50% and
   61.67% in total, respectively). Grok achieved 65.64% pathology
   prediction and 66.23% sequence prediction accuracy. Conclusions: The
   image interpretation capabilities of the investigated LLMs are limited
   for now and require further refinement before competing with
   specifically trained and fine-tuned dedicated applications. Amongst
   them, Gemini outperforms the others in pathology prediction while Grok
   outperforms others in sequence prediction. These limitations should be
   kept in mind if use during patient care is planned.
ZR 0
ZS 0
TC 0
Z8 0
ZA 0
ZB 0
Z9 0
DA 2025-06-15
UT WOS:001505931600001
PM 40506892
ER

PT J
AU Kar, Sujoy
   Jallepalli, Shivkumar
   Potla, Bharath
   Haranath, Sai Praveen P.
   Reddy, Sangita
TI Conversion of Dicom ECG Images to Tabular Format for Building Large
   Language Model in Diagnoses and Disease Progression of Cardiovascular
   Conditions
SO CIRCULATION
VL 148
MA A17760
DI 10.1161/circ.148.suppl_1.17760
SU 1
DT Meeting Abstract
PD NOV 7 2023
PY 2023
CT American-Heart-Association's Epidemiology and Prevention/Lifestyle and
   Cardiometabolic Health Scientific Sessions
CY NOV 11-13, 2023
CL Philadelphia, PA
SP Amer Heart Assoc
TC 0
ZB 0
ZR 0
ZS 0
ZA 0
Z8 0
Z9 0
DA 2024-03-04
UT WOS:001157891308043
ER

PT J
AU Wada, Akihiko
   Akashi, Toshiaki
   Shih, George
   Hagiwara, Akifumi
   Nishizawa, Mitsuo
   Hayakawa, Yayoi
   Kikuta, Junko
   Shimoji, Keigo
   Sano, Katsuhiro
   Kamagata, Koji
   Nakanishi, Atsushi
   Aoki, Shigeki
TI Optimizing GPT-4 Turbo Diagnostic Accuracy in Neuroradiology through
   Prompt Engineering and Confidence Thresholds
SO DIAGNOSTICS
VL 14
IS 14
AR 1541
DI 10.3390/diagnostics14141541
DT Article
PD JUL 2024
PY 2024
AB Background and Objectives: Integrating large language models (LLMs) such
   as GPT-4 Turbo into diagnostic imaging faces a significant challenge,
   with current misdiagnosis rates ranging from 30-50%. This study
   evaluates how prompt engineering and confidence thresholds can improve
   diagnostic accuracy in neuroradiology. Methods: We analyze 751
   neuroradiology cases from the American Journal of Neuroradiology using
   GPT-4 Turbo with customized prompts to improve diagnostic precision.
   Results: Initially, GPT-4 Turbo achieved a baseline diagnostic accuracy
   of 55.1%. By reformatting responses to list five diagnostic candidates
   and applying a 90% confidence threshold, the highest precision of the
   diagnosis increased to 72.9%, with the candidate list providing the
   correct diagnosis at 85.9%, reducing the misdiagnosis rate to 14.1%.
   However, this threshold reduced the number of cases that responded.
   Conclusions: Strategic prompt engineering and high confidence thresholds
   significantly reduce misdiagnoses and improve the precision of the LLM
   diagnostic in neuroradiology. More research is needed to optimize these
   approaches for broader clinical implementation, balancing accuracy and
   utility.
Z8 0
ZS 0
ZR 0
ZA 0
TC 5
ZB 0
Z9 5
DA 2024-08-01
UT WOS:001276606000001
PM 39061677
ER

PT J
AU Liu, Yuxiao
   Liu, Mianxin
   Zhang, Yuanwang
   Guan, Yihui
   Guo, Qihao
   Xie, Fang
   Shen, Dinggang
TI Amyloid-β Deposition Prediction With Large Language Model Driven and
   Task-Oriented Learning of Brain Functional Networks
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
VL 44
IS 4
BP 1809
EP 1820
DI 10.1109/TMI.2024.3525022
DT Article
PD APR 2025
PY 2025
AB Amyloid-beta positron emission tomography can reflect the Amyloid-beta
   protein deposition in the brain and thus serves as one of the golden
   standards for Alzheimer's disease (AD) diagnosis. However, its practical
   cost and high radioactivity hinder its application in large-scale early
   AD screening. Recent neuroscience studies suggest a strong association
   between changes in functional connectivity network (FCN) derived from
   functional MRI (fMRI), and deposition patterns of Amyloid-beta protein
   in the brain. This enables an FCN-based approach to assess the
   Amyloid-beta protein deposition with less expense and radioactivity.
   However, an effective FCN-based Amyloid-beta assessment remains lacking
   for practice. In this paper, we introduce a novel deep learning
   framework tailored for this task. Our framework comprises three
   innovative components: 1) a pre-trained Large Language Model Nodal
   Embedding Encoder, designed to extract task-related features from fMRI
   signals; 2) a task-oriented Hierarchical-order FCN Learning module, used
   to enhance the representation of complex correlations among different
   brain regions for improved prediction of Amyloid-beta deposition; and 3)
   task-feature consistency losses for promoting similarity between
   predicted and real Amyloid-beta values and ensuring effectiveness of
   predicted Amyloid-beta in downstream classification task. Experimental
   results show superiority of our method over several state-of-the-art
   FCN-based methods. Additionally, we identify crucial functional
   sub-networks for predicting Amyloid-beta depositions. The proposed
   method is anticipated to contribute valuable insights into the
   understanding of mechanisms of AD and its prevention.
ZR 0
ZA 0
ZB 0
Z8 0
TC 0
ZS 0
Z9 0
DA 2025-04-13
UT WOS:001459777800001
PM 40030867
ER

PT J
AU Chappidi, Shreya
   Lee, Hawon
   Jagasia, Sarisha
   Syal, Casey
   Zaki, George
   Junkin, Dylan
   Golightly, Nathan
   Chitwood, Patrick
   Camphausen, Kevin
   Krauze, Andra
TI Defining and capturing progression in glioma by harnessing NLP in
   unstructured electronic health records
SO CANCER RESEARCH
VL 84
IS 6
MA 6199
DI 10.1158/1538-7445.AM2024-6199
SU S
DT Meeting Abstract
PD MAR 15 2024
PY 2024
CT Annual Meeting of the American-Association-for-Cancer-Research (AACR)
CY APR 05-10, 2024
CL San Diego, CA
SP Amer Assoc Cancer Res
ZA 0
ZR 0
TC 2
Z8 0
ZB 1
ZS 0
Z9 2
DA 2024-10-03
UT WOS:001253000904152
ER

PT J
AU Johnston, Edward W.
   Goldberg, S. Nahum
TI Photodynamic Therapy for Abdominopelvic Abscesses
SO RADIOLOGY
VL 310
IS 3
AR e240408
DI 10.1148/radiol.240408
DT Editorial Material
PD MAR 2024
PY 2024
ZA 0
ZB 0
TC 0
ZR 0
ZS 0
Z8 0
Z9 0
DA 2024-06-21
UT WOS:001208969200019
PM 38501955
ER

PT J
AU Diaz-Gonzalez, Alvaro
   Forner, Alejandro
   Turnes, Juan
TI Advancing radiology reporting with large language models: Is GPT-4 the
   LI-RADS game changer or just a wild card?
SO LIVER INTERNATIONAL
VL 44
IS 7
BP 1575
EP 1577
DI 10.1111/liv.15952
DT Editorial Material
PD JUL 2024
PY 2024
ZB 1
ZS 0
TC 1
ZA 0
ZR 0
Z8 0
Z9 1
DA 2024-08-09
UT WOS:001281745200003
PM 38886910
ER

PT J
AU Deng, Ailin
   Chen, Wenyi
   Dai, Jinjie
   Jiang, Liu
   Chen, Yicai
   Jiang, Jinyan
   Chen, Yuhua
   Rao, Maohua
TI Current application of ChatGPT in undergraduate nuclear medicine
   education: Taking Chongqing Medical University as an example
SO MEDICAL TEACHER
VL 47
IS 6
BP 997
EP 1003
DI 10.1080/0142159X.2024.2399673
EA SEP 2024
DT Article
PD JUN 3 2025
PY 2025
AB BackgroundNuclear Medicine(NM), as an inherently interdisciplinary
   field, integrates diverse scientific principles and advanced imaging
   techniques. The advent of ChatGPT, a large language model, opens new
   avenues for medical educational innovation. With its advanced natural
   language processing abilities and complex algorithms, ChatGPT holds the
   potential to substantially enrich medical education, particularly in
   NM.ObjectiveTo investigate the current application of ChatGPT in
   undergraduate Nuclear Medicine Education(NME).MethodsEmploying a
   mixed-methods sequential explanatory design, the research investigates
   the current status of NME, the use of ChatGPT and the attitude towards
   ChatGPT among teachers and students in the Second Clinical College of
   Chongqing Medical University.ResultsThe investigation yields several
   salient findings: (1) Students and educators in NM face numerous
   challenges in the learning process; (2) ChatGPT is found to possess
   significant applicability and potential benefits in NME; (3) There is a
   pronounced inclination among respondents to adopt ChatGPT, with a keen
   interest in its diverse applications within the educational
   sphere.ConclusionChatGPT has been utilized to address the difficulties
   faced by undergraduates at Chongqing Medical University in NME, and has
   been applied in various aspects to assist learning. The findings of this
   survey may offer some insights into how ChatGPT can be integrated into
   practical medical education.
ZR 0
Z8 1
TC 2
ZS 0
ZA 0
ZB 1
Z9 2
DA 2024-10-03
UT WOS:001320285500001
PM 39305476
ER

PT J
AU Cunningham, Jonathan W.
   Abraham, William T.
   Bhatt, Ankeet S.
   Dunn, Jessilyn
   Felker, G. Michael
   Jain, Sneha S.
   Lindsell, Christopher J.
   Mace, Matthew
   Martyn, Trejeeve
   Shah, Rashmee U.
   Tison, Geoffrey H.
   Fakhouri, Tala
   Psotka, Mitchell A.
   Krumholz, Harlan
   Fiuzat, Mona
   O'Connor, Christopher M.
   Solomon, Scott D.
CA Heart Failure Collaboratory
TI Artificial Intelligence in Cardiovascular Clinical Trials
SO JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY
VL 84
IS 20
BP 2051
EP 2062
DI 10.1016/j.jacc.2024.08.069
EA NOV 2024
DT Review
PD NOV 12 2024
PY 2024
AB Randomized clinical trials are the gold standard for establishing the
   efficacy and safety of cardiovascular therapies. However, current
   pivotal trials are expensive, lengthy, and insufficiently diverse.
   Emerging artificial intelligence (AI) technologies can potentially
   automate and streamline clinical trial operations. This review describes
   opportunities to integrate AI throughout a trial's life cycle, including
   designing the trial, identifying eligible patients, obtaining informed
   consent, ascertaining physiological and clinical event outcomes,
   interpreting imaging, and analyzing or disseminating the results.
   Nevertheless, AI poses risks, including generating inaccurate results,
   amplifying biases against underrepresented groups, and violating patient
   privacy. Medical journals and regulators are developing new frameworks
   to evaluate AI research tools and the data they generate. Given the
   high-stakes role of randomized trials in medical decision making, AI
   must be integrated carefully and transparently to protect the validity
   of trial results. (JACC. 2024;84:2051-2062) (c) 2024 The Authors.
   Published by Elsevier on behalf of the American College of Cardiology
   Foundation. This is an open access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).
ZA 0
Z8 0
ZB 1
ZR 0
TC 5
ZS 0
Z9 5
DA 2024-11-16
UT WOS:001351382100001
PM 39505413
ER

PT J
AU Tordjman, Mickael
   Liu, Zelong
   Yuce, Murat
   Fauveau, Valentin
   Mei, Yunhao
   Hadjadj, Jerome
   Bolger, Ian
   Almansour, Haidara
   Horst, Carolyn
   Parihar, Ashwin Singh
   Geahchan, Amine
   Meribout, Anis
   Yatim, Nader
   Ng, Nicole
   Robson, Phillip
   Zhou, Alexander
   Lewis, Sara
   Huang, Mingqian
   Deyer, Timothy
   Taouli, Bachir
   Lee, Hao-Chih
   Fayad, Zahi A.
   Mei, Xueyan
TI Comparative benchmarking of the DeepSeek large language model on medical
   tasks and clinical reasoning
SO NATURE MEDICINE
DI 10.1038/s41591-025-03726-3
EA APR 2025
DT Article; Early Access
PY 2025
AB DeepSeek is a newly introduced large language model (LLM) designed for
   enhanced reasoning, but its medical-domain capabilities have not yet
   been evaluated. Here we assessed the capabilities of three LLMs-
   DeepSeek-R1, ChatGPT-o1 and Llama 3.1-405B-in performing four different
   medical tasks: answering questions from the United States Medical
   Licensing Examination (USMLE), interpreting and reasoning on the basis
   of text-based diagnostic and management cases, providing tumor
   classification according to RECIST 1.1 criteria and providing summaries
   of diagnostic imaging reports across multiple modalities. In the USMLE
   test, the performance of DeepSeek-R1 (accuracy 0.92) was slightly
   inferior to that of ChatGPT-o1 (accuracy 0.95; P = 0.04) but better than
   that of Llama 3.1-405B (accuracy 0.83; P < 10(-3)). For text-based case
   challenges, DeepSeek-R1 performed similarly to ChatGPT-o1 (accuracy of
   0.57 versus 0.55; P = 0.76 and 0.74 versus 0.76; P = 0.06, using New
   England Journal of Medicine and M & eacute;dicilline databases,
   respectively). For RECIST classifications, DeepSeek-R1 also performed
   similarly to ChatGPT-o1 (0.74 versus 0.81; P = 0.10). Diagnostic
   reasoning steps provided by DeepSeek were deemed more accurate than
   those provided by ChatGPT and Llama 3.1-405B (average Likert score of
   3.61, 3.22 and 3.13, respectively, P = 0.005 and P < 10(-3)). However,
   summarized imaging reports provided by DeepSeek-R1 exhibited lower
   global quality than those provided by ChatGPT-o1 (5-point Likert score:
   4.5 versus 4.8; P < 10(-3)). This study highlights the potential of
   DeepSeek-R1 LLM for medical applications but also underlines areas
   needing improvements.
ZB 0
TC 2
ZS 0
ZR 0
ZA 0
Z8 0
Z9 2
DA 2025-05-15
UT WOS:001484083100001
PM 40267969
ER

PT J
AU Odisho, Anobel Y.
   Liu, Andrew W.
   Pace, William A.
   Krumm, Robert
   Cowan, Janet E.
   Carroll, Peter R.
   Cooperberg, Matthew R.
TI DEVELOPMENT OF A GENERATIVE ARTIFICIAL INTELLIGENCE DATA PIPELINE TO
   AUTOMATE THE CAPTURE OF UNSTRUCTURED MRI DATA FOR PROSTATE CANCER CARE
SO JOURNAL OF UROLOGY
VL 211
IS 5
MA MP07-14
BP E110
EP E110
DI 10.1097/01.JU.0001008728.41882.d7.14
SU S
DT Meeting Abstract
PD MAY 2024
PY 2024
CT Annual Meeting of the American-Urological-Association (AUA)
CY MAY 03-06, 2024
CL San Antonio, TX
SP Amer Urolog Assoc
TC 0
ZB 0
ZS 0
ZA 0
ZR 0
Z8 0
Z9 0
DA 2024-08-04
UT WOS:001263885300214
ER

PT J
AU Hong, Julie
   Calais, Jeremie
   Benz, Matthias
   Auerbach, Martin
   Salavati, Ali
TI Comparative analysis of large language models: ChatGPT and Google Bard
   answer nonexpert questions related to the diagnostic and therapeutic
   applications of prostate-specific membrane antigen (PSMA) in patients
   with prostate cancer
SO JOURNAL OF NUCLEAR MEDICINE
VL 65
MA 242416
SU 2
DT Meeting Abstract
PD JUN 1 2024
PY 2024
CT Annual Meeting of the Society-of-Nuclear-Medicine-and-Molecular-Imaging
   (SNMMI)
CY JUN 08-11, 2024
CL Toronto, CANADA
SP Soc Nuclear Med & Mol Imaging
ZS 0
TC 1
ZR 0
ZB 0
ZA 0
Z8 0
Z9 1
DA 2024-12-16
UT WOS:001289165603256
ER

PT J
AU Chien, Aichi
   Tang, Hubert
   Jagessar, Bhavita
   Chang, Kai-wei
   Peng, Nanyun
   Nael, Kambiz
   Salamon, Noriko
TI AI-Assisted Summarization of Radiological Reports: Evaluating
   GPT3davinci, BARTcnn, LongT5booksum, LEDbooksum, LEDlegal, and
   LEDclinical
SO AMERICAN JOURNAL OF NEURORADIOLOGY
VL 45
IS 2
BP 244
EP 248
DI 10.3174/ajnr.A8102
EA JAN 2024
DT Article
PD FEB 2024
PY 2024
AB BACKGROUND AND PURPOSE: The review of clinical reports is an essential
   part of monitoring disease progression. Synthesizing multiple imaging
   reports is also important for clinical decisions. It is critical to
   aggregate information quickly and accurately. Machine learning natural
   language processing (NLP) models hold promise to address an unmet need
   for report summarization.MATERIALS AND METHODS: We evaluated NLP methods
   to summarize longitudinal aneurysm reports. A total of 137 clinical
   reports and 100 PubMed case reports were used in this study. Models were
   1) compared against expert-generated summary using longitudinal imaging
   notes collected in our institute and 2) compared using publicly
   accessible PubMed case reports. Five AI models were used to summarize
   the clinical reports, and a sixth model, the online GPT3davinci NLP
   large language model (LLM), was added for the summarization of PubMed
   case reports. We assessed the summary quality through comparison with
   expert summaries using quantitative metrics and quality reviews by
   experts.RESULTS: In clinical summarization, BARTcnn had the best
   performance (BERTscore = 0.8371), followed by LongT5Booksum and
   LEDlegal. In the analysis using PubMed case reports, GPT3davinci
   demonstrated the best performance, followed by models BARTcnn and then
   LEDbooksum (BERTscore = 0.894, 0.872, and 0.867,
   respectively).CONCLUSIONS: AI NLP summarization models demonstrated
   great potential in summarizing longitudinal aneurysm reports, though
   none yet reached the level of quality for clinical usage. We found the
   online GPT LLM outperformed the others; however, the BARTcnn model is
   potentially more useful because it can be implemented on-site. Future
   work to improve summarization, address other types of neuroimaging
   reports, and develop structured reports may allow NLP models to ease
   clinical workflow.
ZB 0
TC 6
ZA 0
Z8 0
ZR 0
ZS 0
Z9 6
DA 2024-01-29
UT WOS:001145523300001
PM 38238092
ER

PT J
AU Choi, Hongyoon
   Lee, Dongjoo
   Kang, Yeon-koo
   Suh, Minseok
TI Empowering PET imaging reporting with retrieval-augmented large language
   models and reading reports database: a pilot single center study
SO EUROPEAN JOURNAL OF NUCLEAR MEDICINE AND MOLECULAR IMAGING
VL 52
IS 7
BP 2452
EP 2462
DI 10.1007/s00259-025-07101-9
EA JAN 2025
DT Article
PD JUN 2025
PY 2025
AB PurposeThe potential of Large Language Models (LLMs) in enhancing a
   variety of natural language tasks in clinical fields includes medical
   imaging reporting. This pilot study examines the efficacy of a
   retrieval-augmented generation (RAG) LLM system considering zero-shot
   learning capability of LLMs, integrated with a comprehensive database of
   PET reading reports, in improving reference to prior reports and
   decision making.MethodsWe developed a custom LLM framework with
   retrieval capabilities, leveraging a database of over 10 years of PET
   imaging reports from a single center. The system uses vector space
   embedding to facilitate similarity-based retrieval. Queries prompt the
   system to generate context-based answers and identify similar cases or
   differential diagnoses. From routine clinical PET readings, experienced
   nuclear medicine physicians evaluated the performance of system in terms
   of the relevance of queried similar cases and the appropriateness score
   of suggested potential diagnoses.ResultsThe system efficiently organized
   embedded vectors from PET reports, showing that imaging reports were
   accurately clustered within the embedded vector space according to the
   diagnosis or PET study type. Based on this system, a proof-of-concept
   chatbot was developed and showed the framework's potential in
   referencing reports of previous similar cases and identifying exemplary
   cases for various purposes. From routine clinical PET readings, 84.1% of
   the cases retrieved relevant similar cases, as agreed upon by all three
   readers. Using the RAG system, the appropriateness score of the
   suggested potential diagnoses was significantly better than that of the
   LLM without RAG. Additionally, it demonstrated the capability to offer
   differential diagnoses, leveraging the vast database to enhance the
   completeness and precision of generated reports.ConclusionThe
   integration of RAG LLM with a large database of PET imaging reports
   suggests the potential to support clinical practice of nuclear medicine
   imaging reading by various tasks of AI including finding similar cases
   and deriving potential diagnoses from them. This study underscores the
   potential of advanced AI tools in transforming medical imaging reporting
   practices.
ZR 0
Z8 0
TC 1
ZS 0
ZB 0
ZA 0
Z9 1
DA 2025-01-25
UT WOS:001401835700001
PM 39843863
ER

PT J
AU Shanbhag, Nandan M
   Bin Sumaida, Abdulrahman
   Al Shamisi, Khalifa
   Balaraj, Khalid
TI Apple Vision Pro: A Paradigm Shift in Medical Technology.
SO Cureus
VL 16
IS 9
BP e69608
EP e69608
DI 10.7759/cureus.69608
DT Journal Article; Review
PD 2024-Sep
PY 2024
AB The introduction of Apple Vision Pro (AVP) marks a significant milestone
   in the intersection of technology and healthcare, offering unique
   capabilities in mixed reality, which Apple terms "spatial computing."
   This narrative review aims to explore the various applications of AVP in
   medical technology, emphasizing its impact on patient care, clinical
   practices, medical education, and future directions. The review
   synthesizes findings from multiple studies and articles published
   between January 2023 and May 2024, highlighting AVP's potential to
   enhance visualization in diagnostic imaging and surgical planning,
   assist visually impaired patients, and revolutionize medical education
   through immersive learning environments. Despite its promise, challenges
   remain in integrating AVP into existing healthcare systems and
   understanding its long-term impact on patient outcomes. As research
   continues, AVP is poised to play a pivotal role in the future of
   medicine, offering a transformative tool for healthcare professionals.
ZB 0
ZR 0
ZA 0
ZS 0
Z8 0
TC 0
Z9 0
DA 2024-09-25
UT MEDLINE:39308843
PM 39308843
ER

PT J
AU Lu, Zhiyong
TI Multimodal Large Language Models in Vision and Ophthalmology
SO INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE
VL 65
IS 7
MA 3876
DT Meeting Abstract
PD JUN 2024
PY 2024
CT Annual Meeting of the
   Association-for-Research-in-Vision-and-Ophthalmology (ARVO)
CY MAY 05-09, 2024
CL Seattle, WA
SP Assoc Res Vision & Ophthalmol
ZA 0
Z8 0
TC 1
ZB 0
ZS 0
ZR 0
Z9 1
DA 2024-11-30
UT WOS:001313316201235
ER

PT J
AU Hong, Huixiao
   Slikker, William
TI Integrating artificial intelligence with bioinformatics promotes public
   health
SO EXPERIMENTAL BIOLOGY AND MEDICINE
VL 248
IS 21
BP 1905
EP 1907
DI 10.1177/15353702231223575
EA JAN 2024
DT Editorial Material
PD NOV 2023
PY 2023
Z8 0
ZA 0
TC 1
ZS 0
ZR 0
ZB 0
Z9 1
DA 2024-01-12
UT WOS:001137033900001
PM 38179798
ER

PT J
AU Tran, Hao
   Joseph, Viren
   Al-Falahi, Zaidon
   Dharmadmajan, Anoop
   Shaw, Elizabeth
   Xu, Aaron
   Akrawi, Daniel
   Juergens, Craig
   French, Bruce
   Wilson, Michael
   Otton, James
   Scalia, Greg
   Badie, Tamer Naguib
   Kay, Sharon
   Guo, Yi
   Tran, Tu Tak
   Chang, Anthony
   Lo, Sidney
TI Large Language Model based multi-agent Transcatheter Aortic Valve
   Implantation team to augment multidisciplinary meetings - proof of
   concept.
SO CIRCULATION
VL 150
MA 4138722
DI 10.1161/circ.150.suppl_1.4138722
SU 1
DT Meeting Abstract
PD NOV 12 2024
PY 2024
ZS 0
Z8 0
ZB 0
ZR 0
ZA 0
TC 0
Z9 0
DA 2025-02-10
UT WOS:001398742702398
ER

PT J
AU Han, B.
   Chen, Y.
   Buyyounouski, M. K.
   Gensheimer, M. F.
   Xing, L.
TI RadAlonc: Enhancing Decision-Making in Radiation Oncology with a
   GPT-4-Based Prompt-Driven Large Language Model
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 2297
BP E134
EP E134
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
ZB 0
TC 0
Z8 0
ZR 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892300029
ER

PT J
AU Raja, Hina
   Huang, Xiaoqin
   Delsoz, Mohammad
   Madadi, Yeganeh
   Poursoroush, Asma
   Munawar, Asim
   Kahook, Malik
   Yousefi, Siamak
TI Diagnosing Glaucoma Based on a Large Language Model Chatbot
SO INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE
VL 65
IS 7
MA 1636
DT Meeting Abstract
PD JUN 2024
PY 2024
CT Annual Meeting of the
   Association-for-Research-in-Vision-and-Ophthalmology (ARVO)
CY MAY 05-09, 2024
CL Seattle, WA
SP Assoc Res Vision & Ophthalmol
ZR 0
TC 0
ZB 0
Z8 0
ZA 0
ZS 0
Z9 0
DA 2024-12-01
UT WOS:001312227704264
ER

PT J
AU Voinea, Stefan-Vlad
   Mamuleanu, Madalin
   Teica, Rossy Vladut
   Florescu, Lucian Mihai
   Selisteanu, Dan
   Gheonea, Ioana Andreea
TI GPT-Driven Radiology Report Generation with Fine-Tuned Llama 3
SO BIOENGINEERING-BASEL
VL 11
IS 10
AR 1043
DI 10.3390/bioengineering11101043
DT Article
PD OCT 2024
PY 2024
AB The integration of deep learning into radiology has the potential to
   enhance diagnostic processes, yet its acceptance in clinical practice
   remains limited due to various challenges. This study aimed to develop
   and evaluate a fine-tuned large language model (LLM), based on Llama
   3-8B, to automate the generation of accurate and concise conclusions in
   magnetic resonance imaging (MRI) and computed tomography (CT) radiology
   reports, thereby assisting radiologists and improving reporting
   efficiency. A dataset comprising 15,000 radiology reports was collected
   from the University of Medicine and Pharmacy of Craiova's Imaging
   Center, covering a diverse range of MRI and CT examinations made by four
   experienced radiologists. The Llama 3-8B model was fine-tuned using
   transfer-learning techniques, incorporating parameter quantization to
   4-bit precision and low-rank adaptation (LoRA) with a rank of 16 to
   optimize computational efficiency on consumer-grade GPUs. The model was
   trained over five epochs using an NVIDIA RTX 3090 GPU, with intermediary
   checkpoints saved for monitoring. Performance was evaluated
   quantitatively using Bidirectional Encoder Representations from
   Transformers Score (BERTScore), Recall-Oriented Understudy for Gisting
   Evaluation (ROUGE), Bilingual Evaluation Understudy (BLEU), and Metric
   for Evaluation of Translation with Explicit Ordering (METEOR) metrics on
   a held-out test set. Additionally, a qualitative assessment was
   conducted, involving 13 independent radiologists who participated in a
   Turing-like test and provided ratings for the AI-generated conclusions.
   The fine-tuned model demonstrated strong quantitative performance,
   achieving a BERTScore F1 of 0.8054, a ROUGE-1 F1 of 0.4998, a ROUGE-L F1
   of 0.4628, and a METEOR score of 0.4282. In the human evaluation, the
   artificial intelligence (AI)-generated conclusions were preferred over
   human-written ones in approximately 21.8% of cases, indicating that the
   model's outputs were competitive with those of experienced radiologists.
   The average rating of the AI-generated conclusions was 3.65 out of 5,
   reflecting a generally favorable assessment. Notably, the model
   maintained its consistency across various types of reports and
   demonstrated the ability to generalize to unseen data. The fine-tuned
   Llama 3-8B model effectively generates accurate and coherent conclusions
   for MRI and CT radiology reports. By automating the conclusion-writing
   process, this approach can assist radiologists in reducing their
   workload and enhancing report consistency, potentially addressing some
   barriers to the adoption of deep learning in clinical practice. The
   positive evaluations from independent radiologists underscore the
   model's potential utility. While the model demonstrated strong
   performance, limitations such as dataset bias, limited sample diversity,
   a lack of clinical judgment, and the need for large computational
   resources require further refinement and real-world validation. Future
   work should explore the integration of such models into clinical
   workflows, address ethical and legal considerations, and extend this
   approach to generate complete radiology reports.
ZS 0
ZB 0
Z8 1
ZA 0
ZR 0
TC 1
Z9 1
DA 2024-11-03
UT WOS:001342753500001
PM 39451418
ER

PT J
AU Baxter, Sally Liu
TI Transforming Patient Experience: Harnessing AI -Powered Virtual
   Assistants in Ophthalmology
SO INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE
VL 65
IS 7
MA 6
DT Meeting Abstract
PD JUN 2024
PY 2024
CT Annual Meeting of the
   Association-for-Research-in-Vision-and-Ophthalmology (ARVO)
CY MAY 05-09, 2024
CL Seattle, WA
SP Assoc Res Vision & Ophthalmol
ZS 0
ZB 0
TC 0
ZR 0
Z8 0
ZA 0
Z9 0
DA 2024-12-01
UT WOS:001312227700084
ER

PT J
AU Farhat, Faiza
TI ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane
SO ANNALS OF BIOMEDICAL ENGINEERING
VL 52
IS 5
BP 1111
EP 1114
DI 10.1007/s10439-023-03326-7
EA JUL 2023
DT Article
PD MAY 2024
PY 2024
AB The launch of Open AI's chatbot, ChatGPT, has generated a lot of
   attention and discussion among professionals in several fields. Many
   concerns and challenges have been brought up by researchers from various
   fields, particularly in relation to the harm that using these tools for
   medical diagnosis and treatment recommendations can cause. In addition,
   it has been debated if ChatGPT is dependable, efficient, and helpful for
   clinicians and medical professionals. Therefore, in this study, we
   assess ChatGPT's effectiveness in providing mental health support,
   particularly for issues related to anxiety and depression, based on the
   chatbot's responses and cross-questioning. The findings indicate that
   there are significant inconsistencies and that ChatGPT's reliability is
   low in this specific domain. As a result, care must be used when using
   ChatGPT as a complementary mental health resource.
Z8 0
ZS 0
ZR 0
ZB 4
ZA 0
TC 33
Z9 33
DA 2023-08-10
UT WOS:001035550600001
PM 37477707
ER

PT J
AU Gu, Kyowon
   Lee, Jeong Hyun
   Shin, Jaeseung
   Hwang, Jeong Ah
   Min, Ji Hye
   Jeong, Woo Kyoung
   Lee, Min Woo
   Song, Kyoung Doo
   Bae, Sung Hwan
TI Using GPT-4 for LI-RADS feature extraction and categorization with
   multilingual free-text reports
SO LIVER INTERNATIONAL
VL 44
IS 7
BP 1578
EP 1587
DI 10.1111/liv.15891
EA APR 2024
DT Article
PD JUL 2024
PY 2024
AB Background and Aims: The Liver Imaging Reporting and Data System
   (LI-RADS) offers a standardized approach for imaging hepatocellular
   carcinoma. However, the diverse styles and structures of radiology
   reports complicate automatic data extraction. Large language models hold
   the potential for structured data extraction from free-text reports. Our
   objective was to evaluate the performance of Generative Pre-trained
   Transformer (GPT)-4 in extracting LI-RADS features and categories from
   free-text liver magnetic resonance imaging (MRI) reports. Methods: Three
   radiologists generated 160 fictitious free-text liver MRI reports
   written in Korean and English, simulating real-world practice. Of these,
   20 were used for prompt engineering, and 140 formed the internal test
   cohort. Seventy-two genuine reports, authored by 17 radiologists were
   collected and de-identified for the external test cohort. LI-RADS
   features were extracted using GPT-4, with a Python script calculating
   categories. Accuracies in each test cohort were compared. Results: On
   the external test, the accuracy for the extraction of major LI-RADS
   features, which encompass size, nonrim arterial phase hyperenhancement,
   nonperipheral 'washout', enhancing 'capsule' and threshold growth,
   ranged from .92 to .99. For the rest of the LI-RADS features, the
   accuracy ranged from .86 to .97. For the LI-RADS category, the model
   showed an accuracy of .85 (95% CI: .76, .93). Conclusions: GPT-4 shows
   promise in extracting LI-RADS features, yet further refinement of its
   prompting strategy and advancements in its neural network architecture
   are crucial for reliable use in processing complex real-world MRI
   reports.
ZB 1
ZS 0
ZR 0
ZA 0
Z8 1
TC 11
Z9 12
DA 2024-04-30
UT WOS:001206383700001
PM 38651924
ER

PT J
AU Lefkes, Judith
   D'Amato, Marina
   Sun, Susu
   Litjens, Geert
   Ciompi, Francesco
TI Large Language Models Automate Diagnostic Conclusions Generation from
   Microscopic Descriptions in Multiple Cancer Types
SO LABORATORY INVESTIGATION
VL 105
IS 3
MA 1370
AR 103608
DI 10.1016/j.labinv.2024.103608
EA MAR 2025
SU S
DT Meeting Abstract
PD MAR 2025
PY 2025
CT Annual Meeting of the United-States-and-Canadian-Academy-of-Pathology
   (USCAP)
CY MAR 22-27, 2025
CL Boston, MA
SP United States & Canadian Acad Pathol
ZA 0
ZR 0
Z8 0
ZB 0
ZS 0
TC 0
Z9 0
DA 2025-04-19
UT WOS:001464120600063
ER

PT J
AU Hwang, Eui fin
   Goo, Mo
   Park, Chang Min
TI AI Applications for Thoracic Imaging: Considerations for Best Practice
SO RADIOLOGY
VL 314
IS 2
AR e240650
DI 10.1148/radiol.240650
DT Article
PD FEB 2025
PY 2025
AB Artificial intelligence (AI) technology is rapidly being introduced into
   thoracic radiology practice. Current representative use cases for AI in
   thoracic imaging show cumulative evidence of effectiveness. These
   include AI assistance for reading chest radiographs and low-dose
   (1.5-mSv) chest CT scans for lung cancer screening and triaging
   pulmonary embolism on chest CT scans. Other potential use cases are also
   under investigation, including filtering out normal chest radiographs,
   monitoring reading errors, and automated opportunistic screening of
   nontarget diseases. However, implementing AI tools in daily practice
   requires establishing practical strategies. Practical AI implementation
   will require objective on-site performance evaluation, institutional
   information technology infrastructure integration, and postdeployment
   monitoring. Meanwhile, the remaining challenges of adopting AI
   technology need to be addressed. These challenges include educating
   radiologists and radiology trainees, alleviating liability risk, and
   addressing potential disparities due to the uneven distribution of data
   and AI technology. Finally, next-generation AI technology represented by
   large language models (LLMs), including multimodal models, which can
   interpret both text and images, is expected to innovate the current
   landscape of AI in thoracic radiology practice. These LLMs offer
   opportunities ranging from generating text reports from images to
   explaining examination results to patients. However, these models
   require more research into their feasibility and efficacy.
ZS 0
Z8 0
ZA 0
ZR 0
TC 1
ZB 0
Z9 1
DA 2025-03-09
UT WOS:001434835900015
PM 39998373
ER

PT J
AU Hooshangnejad, Hamed
   Huang, Gaofeng
   Kelly, Katelyn
   Feng, Xue
   Luo, Yi
   Zhang, Rui
   Xu, Ziyue
   Chen, Quan
   Ding, Kai
TI EXACT-Net: Framework for EHR-Guided Lung Tumor Auto-Segmentation for
   Non-Small Cell Lung Cancer Radiotherapy
SO CANCERS
VL 16
IS 23
AR 4097
DI 10.3390/cancers16234097
DT Article
PD DEC 2024
PY 2024
AB Background/Objectives: Lung cancer is a devastating disease with the
   highest mortality rate among cancer types. Over 60% of non-small cell
   lung cancer (NSCLC) patients, accounting for 87% of lung cancer
   diagnoses, require radiation therapy. Rapid treatment initiation
   significantly increases the patient's survival rate and reduces the
   mortality rate. Accurate tumor segmentation is a critical step in
   diagnosing and treating NSCLC. Manual segmentation is time- and
   labor-consuming and causes delays in treatment initiation. Although many
   lung nodule detection methods, including deep learning-based models,
   have been proposed. Most of these methods still have a long-standing
   problem of high false positives (FPs). Methods: Here, we developed an
   electronic health record (EHR)-guided lung tumor auto-segmentation
   called EXACT-Net (EHR-enhanced eXACtitude in Tumor segmentation), where
   the extracted information from EHRs using a pre-trained large language
   model (LLM) was used to remove the FPs and keep the TP nodules only.
   Results: The auto-segmentation model was trained on NSCLC patients'
   computed tomography (CT), and the pre-trained LLM was used with the
   zero-shot learning approach. Our approach resulted in a 250% boost in
   successful nodule detection using the data from ten NSCLC patients
   treated in our institution. Conclusions: We demonstrated that combining
   vision-language information in EXACT-Net multi-modal AI framework
   greatly enhances the performance of vision only models, paving the road
   to multimodal AI framework for medical image processing.
ZS 0
ZA 0
ZR 0
ZB 0
TC 0
Z8 0
Z9 0
DA 2024-12-19
UT WOS:001376131100001
PM 39682283
ER

PT J
AU Nascimento, Jose Jerovane da Costa
   Marques, Adriell Gomes
   Souza, Lucas do Nascimento
   Dourado, Carlos Mauricio Jaborandy de Mattos
   Barros, Antonio Carlos da Silva
   de Albuquerque, Victor Hugo C.
   Sousa, Luis Fabricio de Freitas
TI A novel generative model for brain tumor detection using magnetic
   resonance imaging
SO COMPUTERIZED MEDICAL IMAGING AND GRAPHICS
VL 121
AR 102498
DI 10.1016/j.compmedimag.2025.102498
EA FEB 2025
DT Article
PD APR 2025
PY 2025
AB Brain tumors area disease that kills thousands of people worldwide each
   year. Early identification through diagnosis is essential for monitoring
   and treating patients. The proposed study brings anew method through
   intelligent computational cells that are capable of segmenting the tumor
   region with high precision. The method uses deep learning to detect
   brain tumors with the "You only look once"(Yolov8) framework, and a
   fine-tuning process at the end of the network layer using intelligent
   computational cells capable of traversing the detected region,
   segmenting the edges of the brain tumor. In addition, the method uses a
   classification pipeline that combines a set of classifiers and
   extractors combined with grid search, to find the best combination and
   the best parameters for the dataset. The method obtained satisfactory
   results above 98% accuracy for region detection, and above 99% for brain
   tumor segmentation and accuracies above 98% for binary classification of
   brain tumor, and segmentation time obtaining less than 1 s, surpassing
   the state of the art compared to the same database, demonstrating the
   effectiveness of the proposed method. The new approach proposes the
   classification of different databases through data fusion to classify
   the presence of tumor in MRI images, as well as the patient's life span.
   The segmentation and classification steps are validated by comparing
   them with the literature, with comparisons between works that used the
   same dataset. The method addresses anew generative AI for brain tumor
   capable of generating a pre-diagnosis through input data through Large
   Language Model (LLM), and can be used in systems to aid medical imaging
   diagnosis. As a contribution, this study employs new detection models
   combined with innovative methods based on digital image processing to
   improve segmentation metrics, as well as the use of Data Fusion,
   combining two tumor datasets to enhance classification performance. The
   study also utilizes LLM models to refine the pre-diagnosis obtained
   post-classification. Thus, this study proposes a Computer-Aided
   Diagnosis (CAD) method through AI with PDI, CNN, and LLM.
TC 0
ZR 0
ZA 0
ZB 0
Z8 0
ZS 0
Z9 0
DA 2025-03-03
UT WOS:001431977700001
PM 39985841
ER

PT J
AU Xiong, Yichun
   Li, Jiaqi
   Jin, Wang
   Sheng, Xiaoran
   Peng, Hui
   Wang, Zhiyi
   Jia, Caifeng
   Zhuo, Lili
   Zhang, Yibo
   Huang, Jingzhe
   Zhai, Modi
   Lyu, Beibei
   Sun, Jie
   Zhou, Meng
TI PCMR: a comprehensive precancerous molecular resource
SO SCIENTIFIC DATA
VL 12
IS 1
AR 551
DI 10.1038/s41597-025-04899-9
DT Article
PD APR 1 2025
PY 2025
AB Early detection and intervention of precancerous lesions are crucial in
   reducing cancer morbidity and mortality. Comprehensive analysis of
   genomic, transcriptomic, proteomic and epigenomic alterations can
   provide insights into the early stages of carcinogenesis. However, the
   lacke of an integrated, well-curated data resource of molecular
   signatures limits our understanding of precancerous processes. Here, we
   introduce a comprehensive PreCancerous Molecular Resource (PCMR), which
   compiles 25,828 molecular profiles of precancerous samples paired with
   normal or malignant counterparts. These profiles cover precancerous
   lesions of 35 cancer types across 20 organs and tissues, derived from
   tissue samples, liquid biopsies, cell lines and organoids, with data
   from transcriptomics, proteomics and epigenomics. PCMR includes 62,566
   precancer-gene associations derived from differential analysis and
   text-mining using the ChatGPT large language model. We examined PCMR
   dataset reliability and significance by the authoritative precancerous
   molecular signature, along with its biological and clinical relevance.
   Overall, PCMR will serve as a valuable resource for advancing precancer
   research and ultimately improving patient outcomes.
ZR 0
ZB 0
ZS 0
ZA 0
Z8 0
TC 0
Z9 0
DA 2025-04-11
UT WOS:001459759400009
PM 40169679
ER

PT J
AU Kumar, Rahul
   Waisberg, Ethan
   Ong, Joshua
   Paladugu, Phani
   Sporn, Kyle
   Chima, Karsten
   Amiri, Dylan
   Zaman, Nasif
   Tavakkoli, Alireza
TI Precision health monitoring in spaceflight with integration of lower
   body negative pressure and advanced large language model artificial
   intelligence
SO LIFE SCIENCES IN SPACE RESEARCH
VL 47
BP 57
EP 60
DI 10.1016/j.lssr.2025.05.010
DT Article
PD NOV 2025
PY 2025
AB Long-term exposure to microgravity influences musculoskeletal health and
   enhances the likelihood of sustaining orthopedic injuries while on a
   microgravity mission and upon return to Earth. Although countermeasures
   are being investigated to alleviate some risks of injury, such as
   resistive (or weight) exercise and Lower Body Negative Pressure (LBNP),
   evidence is accumulating that current paradigms do not ensure the safety
   or health of astronauts because of a lack of in-flight diagnostic
   methods, in which load/diagnostic metrics can be assessed over time.
   Here, we suggest the integration of a new vision-language large language
   model (DeepSeek-VL) as a potential autonomous diagnostic agent for
   monitoring musculoskeletal health in a microgravity environment.
   DeepSeek-VL will autonomously analyze radiographic data and
   biomechanical data streamed from a LBNP device. Determinations will be
   made based on lost or compromised density in bone, lost joint-centered
   stability, or ineffective loading patterns - providing personalized and
   specific feedback regarding musculoskeletal health with the astronaut as
   the primary user. Unlike conventional reporting approaches that rely on
   cross-institutional analysis by household experts, DeepSeek-VL allows
   for real-time, and autonomous interpretation of musculoskeletal imaging
   metrics (and physiological metrics) for on-time personalized
   countermeasure development. Here, we review architectural adaptations
   including microgravity specific samplings of data, training protocols
   and implications of deployment in the ISS. We anticipate DeepSeek's
   timely development of flight-ready diagnostic reporting will facilitate
   in-flight/systematic monitoring of musculoskeletal health and safety,
   especially for astronauts undergoing load management training (e.g.,
   LBNP) and ensure effectiveness of countermeasures, their outputs. We
   will address methods to circumvent limitations and barriers to risk, and
   establish the importance of a federated, adaptive, and resilient
   AI-based platform to mitigate risk for astronaut musculoskeletal health
   during extended missions. Finally, we address some considerations for
   terrestrial model and a healthcare authority within a current context of
   growing importance for effective orthopedic healthcare.
ZR 0
Z8 0
ZA 0
ZB 0
TC 0
ZS 0
Z9 0
DA 2025-06-11
UT WOS:001503625900001
ER

PT J
AU Rajendran, Praveenbalaji
   Yang, Yong
   Niedermayr, Thomas R.
   Gensheimer, Michael
   Beadle, Beth
   Le, Quynh-Thu
   Xing, Lei
   Dai, Xianjin
TI Large language model-augmented learning for auto-delineation of
   treatment targets in head-and-neck cancer radiotherapy
SO RADIOTHERAPY AND ONCOLOGY
VL 205
AR 110740
DI 10.1016/j.radonc.2025.110740
EA JAN 2025
DT Article
PD APR 2025
PY 2025
AB Background and Purpose: Radiation therapy (RT) is highly effective, but
   its success depends on accurate, manual target delineation, which is
   time-consuming, labor-intensive, and prone to variability. Despite AI
   advancements in auto-contouring normal tissues, accurate RT target
   volume delineation remains challenging. This study presents Radformer, a
   novel visual language model that integrates text-rich clinical data with
   medical imaging for accurate automated RT target volume delineation.
   Materials and Methods: We developed Radformer, an innovative network
   that utilizes a hierarchical vision transformer as its backbone and
   integrates large language models (LLMs) to extract and embed clinical
   data in text-rich form. The model features a novel visual language
   attention module (VLAM) to combine visual and linguistic features,
   enabling language-aware visual encoding (LAVE). The Radformer was
   evaluated on a dataset of 2985 patients with head-and-neck cancer who
   underwent RT. Quantitative evaluations were performed utilizing metrics
   such as the Dice similarity coefficient (DSC), intersection over union
   (IOU), and 95th percentile Hausdorff distance (HD95). Results: The
   Radformer demonstrated superior performance in segmenting RT target
   volumes compared to stateof-the-art models. On the head-and-neck cancer
   dataset, Radformer achieved a mean DSC of 0.76 f 0.09 versus 0.66 f
   0.09, a mean IOU of 0.69 f 0.08 versus 0.59 f 0.07, and a mean HD95 of
   7.82 f 6.87 mm versus 14.28 f 6.85 mm for gross tumor volume
   delineation, compared to the baseline 3D-UNETR. Conclusions: The
   Radformer model offers a clinically optimal means of RT target
   auto-delineation by integrating both imaging and clinical data through a
   visual language model. This approach improves the accuracy of RT target
   volume delineation, facilitating broader AI-assisted automation in RT
   treatment planning.
TC 1
ZR 0
ZA 0
Z8 0
ZS 0
ZB 0
Z9 1
DA 2025-03-06
UT WOS:001433650900001
PM 39855601
ER

PT J
AU Park, Hyung Jun
   Huh, Jin-Young
   Chae, Ganghee
   Choi, Myeong Geun
TI Extraction of clinical data on major pulmonary diseases from
   unstructured radiologic reports using a large language model
SO PLOS ONE
VL 19
IS 11
AR e0314136
DI 10.1371/journal.pone.0314136
DT Article
PD NOV 25 2024
PY 2024
AB Despite significant strides in big data technology, extracting
   information from unstructured clinical data remains a formidable
   challenge. This study investigated the utility of large language models
   (LLMs) for extracting clinical data from unstructured radiological
   reports without additional training. In this retrospective study, 1800
   radiologic reports, 600 from each of the three university hospitals,
   were collected, with seven pulmonary outcomes defined. Three
   pulmonology-trained specialists discerned the presence or absence of
   diseases. Data extraction from the reports was executed using Google
   Gemini Pro 1.0, OpenAI's GPT-3.5, and GPT-4. The gold standard was
   predicated on agreement between at least two pulmonologists. This study
   evaluated the performance of the three LLMs in diagnosing seven
   pulmonary diseases (active tuberculosis, emphysema, interstitial lung
   disease, lung cancer, pleural effusion, pneumonia, and pulmonary edema)
   utilizing chest radiography and computed tomography scans. All models
   exhibited high accuracy (0.85-1.00) for most conditions. GPT-4
   consistently outperformed its counterparts, demonstrating a sensitivity
   of 0.71-1.00; specificity of 0.89-1.00; and accuracy of 0.89 and 0.99
   across both modalities, thus underscoring its superior capability in
   interpreting radiological reports. Notably, the accuracy of pleural
   effusion and emphysema on chest radiographs and pulmonary edema on chest
   computed tomography scans reached 0.99. The proficiency of LLMs,
   particularly GPT-4, in accurately classifying unstructured radiological
   data hints at their potential as alternatives to the traditional manual
   chart reviews conducted by clinicians.
TC 1
Z8 0
ZA 0
ZS 0
ZR 0
ZB 0
Z9 1
DA 2024-12-13
UT WOS:001363435700050
PM 39585830
ER

PT J
AU Mugu, Vamshi K.
   Carr, Brendan M.
   Olson, Mike C.
   Schupbach, John C.
   Eguia, Francisco A.
   Schmitz, John J.
   Khandelwal, Ashish
TI Increasing Adherence to Societal Recommendations in Radiology Reporting:
   A Feasibility Study Using Society of Radiologists in Ultrasound
   Guidelines for Incidentally Detected Gallbladder Polyps
SO ULTRASOUND QUARTERLY
VL 41
IS 1
AR e00699
DI 10.1097/RUQ.0000000000000699
DT Article
PD MAR 2025
PY 2025
AB Incidental findings in diagnostic imaging are common, but follow-up
   recommendations often lack consistency. The Society of Radiologists in
   Ultrasound (SRU) issued guidelines in 2021 for managing incidentally
   detected gallbladder polyps, aiming to balance follow-up with avoiding
   overtreatment. There is variable adherence to these guidelines in
   radiology reports, however, which makes it difficult for the clinician
   to pursue appropriate follow-up for the patient. The purpose of this
   project is to test the feasibility of a Large Language Model (LLM)-based
   tool to incorporate SRU guidelines into radiology reports. Additionally,
   we propose a framework for closely integrating societal follow-up
   recommendations into radiology reports, using this tool as an example.
   Following institutional review board approval, we retrospectively
   reviewed gallbladder ultrasound examinations performed on adult ED
   patients in 2022. Data on patient demographics and radiology report
   content were collected. Using the 2021 SRU guidelines, we developed an
   interactive tool employing a retriever-augmented generator (RAG) and
   prompt engineering. A board-certified radiologist tested the accuracy,
   whereas a board-certified emergency medicine physician assessed the
   clarity and consistency of the recommendations. The interactive tool,
   GB-PRL, outperformed leading closed-source and open-source LLMs,
   achieving 100% accuracy in risk categorization and follow-up
   recommendations on hypothetical user queries (P < 0.001). The tool also
   showed superior accuracy compared to radiology reports on retrospective
   data (P = 0.04). Although GB-PRL demonstrated greater clarity and
   consistency, the improvement from the radiology reports was not
   statistically significant (P = 0.22). Further work is needed for
   prospective testing of GB-PRL before integrating it into clinical
   practice.
ZS 0
ZB 0
ZA 0
ZR 0
TC 0
Z8 0
Z9 0
DA 2024-12-24
UT WOS:001379791700001
PM 39690147
ER

PT J
AU Alonso-Sanchez, Maria Francisca
   Hinzen, Wolfram
   He, Rui
   Gati, Joseph
   Palaniyappan, Lena
TI Perplexity of utterances in untreated first-episode psychosis: an
   ultra-high field MRI dynamic causal modelling study of the semantic
   network
SO JOURNAL OF PSYCHIATRY & NEUROSCIENCE
VL 49
IS 4
BP E252
EP E262
DI 10.1503/jpn.240031
DT Article
PD JUL-AUG 2024
PY 2024
AB Background: Psychosis involves a distortion of thought content, which is
   partly reflected in anomalous ways in which words are semantically
   connected into utterances in speech. We sought to explore how these
   linguistic anomalies are realized through putative circuit-level
   abnormalities in the brain's semantic network.Methods: Using a
   computational large-language model, Bidirectional Encoder
   Representations from Transformers (BERT), we quantified the contextual
   expectedness of a given word sequence (perplexity) across 180 samples
   obtained from descriptions of 3 pictures by patients with first-episode
   schizophrenia (FES) and controls matched for age, parental social
   status, and sex, scanned with 7 T ultra-high field functional magnetic
   resonance imaging (fMRI). Subsequently, perplexity was used to
   parametrize a spectral dynamic causal model (DCM) of the effective
   connectivity within (intrinsic) and between (extrinsic) 4 key regions of
   the semantic network at rest, namely the anterior temporal lobe, the
   inferior frontal gyrus (IFG), the posterior middle temporal gyrus (MTG),
   and the angular gyrus.Results: We included 60 participants, including 30
   patients with FES and 30 controls. We observed higher perplexity in the
   FES group, indicating that speech was less predictable by the preceding
   context among patients. Results of Bayesian model comparisons showed
   that a DCM including the group by perplexity interaction best explained
   the underlying patterns of neural activity. We observed an increase of
   self-inhibitory effective connectivity within the IFG, as well as
   reduced self-inhibitory tone within the pMTG, in the FES group. An
   increase in self-inhibitory tone in the IFG correlated strongly and
   positively with inter-regional excitation between the IFG and posterior
   MTG, while self-inhibition of the posterior MTG was negatively
   correlated with this interregional excitation.Limitation: Our design did
   not address connectivity in the semantic network during tasks that
   selectively activated the semantic network, which could corroborate
   findings from this resting-state fMRI study. Furthermore, we do not
   present a replication study, which would ideally use speech in a
   different language.Conclusion: As an explanation for peculiar speech in
   psychosis, these results index a shift in the excitatory-inhibitory
   balance regulating information flow across the semantic network,
   confined to 2 regions that were previously linked specifically to the
   executive control of meaning. Based on our approach of combining a large
   language model with causal connectivity estimates, we propose loss in
   semantic control as a potential neurocognitive mechanism contributing to
   disorganization in psychosis.
TC 3
ZR 0
Z8 0
ZB 1
ZS 0
ZA 0
Z9 3
DA 2024-08-16
UT WOS:001288854500001
PM 39122409
ER

PT J
AU Goldfinger, Shir
   Mackay, Emily
   Chan, Trevor
   Eswar, Vikram
   Grasfield, Rachel
   Yan, Vivian
   Barreto, David
   Pouch, Alison
TI A Systematic Approach to Prompting Large Language Models for Automated
   Feature Extraction from Cardiovascular Imaging Reports
SO CIRCULATION
VL 150
MA 4139198
DI 10.1161/circ.150.suppl_1.4139198
SU 1
DT Meeting Abstract
PD NOV 12 2024
PY 2024
TC 0
ZB 0
ZR 0
ZA 0
Z8 0
ZS 0
Z9 0
DA 2025-02-10
UT WOS:001398742703073
ER

PT J
AU Nakao, Takahiro
   Miki, Soichiro
   Nakamura, Yuta
   Kikuchi, Tomohiro
   Nomura, Yukihiro
   Hanaoka, Shouhei
   Yoshikawa, Takeharu
   Abe, Osamu
TI Capability of GPT-4V(ision) in the Japanese National Medical Licensing
   Examination: Evaluation Study
SO JMIR MEDICAL EDUCATION
VL 10
AR e54393
DI 10.2196/54393
DT Article
PD 2024
PY 2024
AB Background: Previous research applying large language models (LLMs) to
   medicine was focused on text -based information. Recently, multimodal
   variants of LLMs acquired the capability of recognizing images.
   Objective: We aim to evaluate the image recognition capability of
   generative pretrained transformer (GPT)-4V, a recent multimodal LLM
   developed by OpenAI, in the medical field by testing how visual
   information affects its performance to answer questions in the 117th
   Japanese National Medical Licensing Examination. Methods: We focused on
   108 questions that had 1 or more images as part of a question and
   presented GPT-4V with the same questions under two conditions: (1) with
   both the question text and associated images and (2) with the question
   text only. We then compared the difference in accuracy between the 2
   conditions using the exact McNemar test. Results: Among the 108
   questions with images, GPT-4V's accuracy was 68% (73/108) when presented
   with images and 72% (78/108) when presented without images (P=.36). For
   the 2 question categories, clinical and general, the accuracies with and
   those without images were 71% (70/98) versus 78% (76/98; P=.21) and 30%
   (3/10) versus 20% (2/10; P >=.99), respectively. Conclusions: The
   additional information from the images did not significantly improve the
   performance of GPT-4V in the Japanese National Medical Licensing
   Examination.
ZS 0
ZR 0
Z8 0
TC 19
ZB 3
ZA 0
Z9 19
DA 2024-03-28
UT WOS:001189066800001
PM 38470459
ER

PT J
AU Holmes, Jason
   Zhang, Lian
   Ding, Yuzhen
   Feng, Hongying
   Liu, Zhengliang
   Liu, Tianming
   Wong, William W.
   Vora, Sujay A.
   Ashman, Jonathan B.
   Liu, Wei
TI Benchmarking a Foundation Large Language Model on its Ability to Relabel
   Structure Names in Accordance With the American Association of
   Physicists in Medicine Task Group-263 Report
SO PRACTICAL RADIATION ONCOLOGY
VL 14
IS 6
BP e515
EP e521
DI 10.1016/j.prro.2024.04.017
EA OCT 2024
DT Article
PD NOV-DEC 2024
PY 2024
AB Purpose: To introduce the concept of using large language models (LLMs)
   to relabel structure names in accordance with the American Association
   of Physicists in Medicine Task Group-263 standard and to establish a
   benchmark for future studies to reference. Methods and Materials:
   Generative Pretrained Transformer (GPT)-4 was implemented within a
   Digital Imaging and Communications in Medicine server. Upon receiving a
   structure-set Digital Imaging and Communications in Medicine fi le, the
   server prompts GPT-4 to relabel the structure names according to the
   American Association of Physicists in Medicine Task Group-263 report.
   The results were evaluated for 3 disease sites: prostate, head and neck,
   and thorax. For each disease site, 150 patients were randomly selected
   for manually tuning the instructions prompt (in batches of 50), and 50
   patients were randomly selected for evaluation. Structure names
   considered were those that were most likely to be relevant for studies
   using structure contours for many patients. Results: The per-patient
   accuracy was 97.2%, 98.3%, and 97.1% for prostate, head and neck, and
   thorax disease sites, respectively. On a per-structure basis, the
   clinical target volume was relabeled correctly in 100%, 95.3%, and 92.9%
   of cases, respectively. Conclusions: Given the accuracy of GPT-4 in
   relabeling structure names as presented in this work, LLMs are poised to
   become an important method for standardizing structure names in
   radiation oncology, especially considering the rapid advancements in LLM
   capabilities that are likely to continue. (c) 2024 American Society for
   Radiation Oncology. Published by Elsevier Inc. All rights are reserved,
   including those for text and data mining, AI training, and similar
   technologies.
ZB 0
TC 1
ZA 0
Z8 0
ZR 0
ZS 0
Z9 1
DA 2024-11-14
UT WOS:001348827900001
PM 39243241
ER

PT J
AU Yang, Z.
   Kazemimoghadam, M.
   Wang, L.
   Szalkowski, G. A.
   Chuang, C. F.
   Liu, L.
   Soltys, S. G.
   Pollom, E.
   Rahimy, E.
   Jiang, H.
   Park, D.
   Persad, A.
   Hori, Y.
   Fu, J.
   Romero, I. O.
   Zalavari, L.
   Chen, M.
   Lu, W.
   Gu, X.
TI A Deep Learning-Driven Framework for Large Language Model -Assisted
   Automatic Target Volume Localization and Delineation for Enhancing
   Spinal Metastases Stereotactic Body Radiotherapy Workflow
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 195
BP S61
EP S62
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZB 0
ZS 0
Z8 0
ZR 0
TC 0
ZA 0
Z9 0
DA 2024-12-16
UT WOS:001325892302564
ER

PT J
AU Rosskopf, Steffen
   Meder, Benjamin
TI Healthcare 4.0-Medizin im Wandel
SO HERZ
VL 49
IS 5
BP 350
EP 354
DI 10.1007/s00059-024-05267-w
EA AUG 2024
DT Review
PD OCT 2024
PY 2024
AB Healthcare 4.0 describes the future transformation of the healthcare
   sector driven by the combination of digital technologies, such as
   artificial intelligence (AI), big data and the Internet of Medical
   Things, enabling the advancement of precision medicine. This overview
   article addresses various areas such as large language models (LLM),
   diagnostics and robotics, shedding light on the positive aspects of
   Healthcare 4.0 and showcasing exciting methods and application examples
   in cardiology. It delves into the broad knowledge base and enormous
   potential of LLMs, highlighting their immediate benefits as digital
   assistants or for administrative tasks. In diagnostics, the increasing
   usefulness of wearables is emphasized and an AI for predicting heart
   filling pressures based on cardiac magnetic resonance imaging (MRI) is
   introduced. Additionally, it discusses the revolutionary methodology of
   a digital simulation of the physical heart (digital twin). Finally, it
   addresses both regulatory frameworks and a brief vision of data-driven
   healthcare delivery, explaining the need for investments in technical
   personnel and infrastructure to achieve a more effective medicine.
Z8 0
ZB 0
ZS 0
ZA 0
TC 0
ZR 0
Z9 0
DA 2024-08-14
UT WOS:001287384100001
PM 39115627
ER

PT J
AU Inojosa, Hernan
   Voigt, Isabel
   Wenk, Judith
   Ferber, Dyke
   Wiest, Isabella
   Antweiler, Dario
   Weicken, Eva
   Gilbert, Stephen
   Kather, Jakob Nikolas
   Akguen, Katja
   Ziemssen, Tjalf
TI Integrating large language models in care, research, and education in
   multiple sclerosis management
SO MULTIPLE SCLEROSIS JOURNAL
VL 30
IS 11-12
BP 1392
EP 1401
DI 10.1177/13524585241277376
EA SEP 2024
DT Review
PD OCT 2024
PY 2024
AB Use of techniques derived from generative artificial intelligence (AI),
   specifically large language models (LLMs), offer a transformative
   potential on the management of multiple sclerosis (MS). Recent LLMs have
   exhibited remarkable skills in producing and understanding human-like
   texts. The integration of AI in imaging applications and the deployment
   of foundation models for the classification and prognosis of disease
   course, including disability progression and even therapy response, have
   received considerable attention. However, the use of LLMs within the
   context of MS remains relatively underexplored. LLMs have the potential
   to support several activities related to MS management. Clinical
   decision support systems could help selecting proper disease-modifying
   therapies; AI-based tools could leverage unstructured real-world data
   for research or virtual tutors may provide adaptive education materials
   for neurologists and people with MS in the foreseeable future. In this
   focused review, we explore practical applications of LLMs across the
   continuum of MS management as an initial scope for future analyses,
   reflecting on regulatory hurdles and the indispensable role of human
   supervision.
ZA 0
Z8 1
ZS 0
ZR 0
TC 4
ZB 0
Z9 4
DA 2024-09-29
UT WOS:001318637000001
PM 39308156
ER

PT J
AU Zhong, Jiayang
   Sehgal, Kanika
   Hickey, Kyle
   Mohammad, Aziza
   Robinson, Stephen
   Farrell, James J.
   Shung, Dennis
TI A LOCAL LARGE LANGUAGE MODEL PIPELINE AUTOMATICALLY RISK STRATIFIES
   PANCREATIC CYSTS FOR POPULATION HEALTH MANAGEMENT FROM SERIAL RADIOLOGY
   REPORTS
SO GASTROENTEROLOGY
VL 166
IS 5
MA Su1183
BP S687
EP S687
SU S
DT Meeting Abstract
PD MAY 2024
PY 2024
CT Digestive Disease Week (DDW)
CY MAY 18-21, 2024
CL Washington, DC
ZA 0
ZS 0
Z8 0
ZB 0
TC 0
ZR 0
Z9 0
DA 2024-10-30
UT WOS:001282837702549
ER

PT J
AU Al-Maadid, Fatima
   Hadid, Faisal
   Mohamedzain, Ali
   Ali, Farhan
   Thabet, Farouq
TI Teaching Video NeuroImage: Rectus Femoris Muscle Fibrosis Presenting as
   Abnormal Gait in Childhood With a Positive Ely Maneuver
SO NEUROLOGY
VL 101
IS 23
BP E2456
EP E2457
DI 10.1212/WNL.0000000000207970
DT Editorial Material
PD DEC 5 2023
PY 2023
AB A 7-year-old boy presented with abnormal gait since age 3 years.
   Examination showed left-sided limping with external rotation. On passive
   flexion of the knees while the patient lied prone, the left heel could
   not reach the buttock and the left hip rose up (Video 1). This indicated
   a positive Ely test indicating limited flexibility of the rectus femoris
   muscle.1 Neurological examination was otherwise unremarkable. He had
   normal skeletal x-rays of both legs. MRI showed fibrosis of the rectus
   femoris muscle (Figure), which may be idiopathic or related to trauma.
   The patient had no history of muscle injury thus distant intramuscular
   injection was suspected to be the cause. Early diagnosis helps improve
   mobility through early surgical intervention.2 Ely test, which is not
   routinely performed during neurological examination, might be helpful in
   evaluating children with abnormal gait.
ZB 0
TC 0
ZS 0
ZR 0
Z8 0
ZA 0
Z9 0
DA 2024-01-14
UT WOS:001110273400003
PM 37816650
ER

PT C
AU Guo, Yuhang
   Wan, Zhiyu
GP IEEE COMPUTER SOC
TI Performance Evaluation of Multimodal Large Language Models (LLaVA and
   GPT-4-based ChatGPT) in Medical Image Classification Tasks
SO 2024 IEEE 12TH INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS, ICHI
   2024
SE IEEE International Conference on Healthcare Informatics
BP 541
EP 543
DI 10.1109/ICHI61247.2024.00080
DT Proceedings Paper
PD 2024
PY 2024
AB Large language models (LLMs) have gained significant attention due to
   their prospective applications in medicine. Utilizing multimodal LLMs
   can potentially assist clinicians in medical image classification tasks.
   It is important to evaluate the performance of LLMs in medical image
   processing to potentially improve the medical system. We evaluated two
   multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG
   in tumor classification across brain MRI, breast ultrasound, and kidney
   CT datasets. Despite LLMs facing significant hallucination issue in
   medical imaging, prompt engineering markedly enhanced their performance.
   In comparison to the baseline method, GPT-4-based ChatGPT with prompt
   engineering achieves 98%, 112%, and 69% of the baseline's performance in
   terms of accuracy (or 99%, 107%, and 62% in terms of F1-score) in those
   three datasets, respectively. However, privacy, bias, accountability,
   and transparency concerns necessitate caution. Our study underscore
   LLMs' potential in medical imaging but emphasize the need for thorough
   performance and safety evaluations for their practical application.
CT 12th IEEE International Conference on Healthcare Informatics (IEEE-ICHI)
CY JUN 03-06, 2024
CL Orlando, FL
SP IEEE; IEEE Comp Soc Tech Community Intelligent Informat; Univ Minnesota,
   Div Computat Hlth Sci; Weill Cornell Med Inst Artificial Intelligence &
   Digital Hlth; Univ Florida Hlth; Yale Univ, Sch Med; Springer; Florida
   State Univ, Coll Commun & Informat
ZR 0
Z8 0
TC 2
ZB 0
ZA 0
ZS 0
Z9 2
DA 2024-11-02
UT WOS:001304501700073
ER

PT J
AU Zhu, L.
   Anand, A.
   Gevorkyan, G.
   Mcgee, L. A.
   Rwigema, J. C.
   Rong, Y.
   Patel, S. H.
TI Testing and Validation of a Custom Trained Large Language Model for HN
   Patients with Guardrails
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 118
IS 5
MA 182
BP E52
EP E53
DT Meeting Abstract
PD APR 1 2024
PY 2024
CT Multidisciplinary Head and Neck Cancers Symposium
CY FEB 29-MAR 02, 2024
CL Phoenix, AZ
TC 1
ZS 0
Z8 0
ZA 0
ZR 0
ZB 0
Z9 1
DA 2024-10-18
UT WOS:001300212900102
ER

PT J
AU Lee, Aric
   Ong, Wilson
   Makmur, Andrew
   Ting, Yong Han
   Tan, Wei Chuan
   Lim, Shi Wei Desmond
   Low, Xi Zhen
   Tan, Jonathan Jiong Hao
   Kumar, Naresh
   Hallinan, James T. P. D.
TI Applications of Artificial Intelligence and Machine Learning in Spine
   MRI
SO BIOENGINEERING-BASEL
VL 11
IS 9
AR 894
DI 10.3390/bioengineering11090894
DT Review
PD SEP 2024
PY 2024
AB Diagnostic imaging, particularly MRI, plays a key role in the evaluation
   of many spine pathologies. Recent progress in artificial intelligence
   and its subset, machine learning, has led to many applications within
   spine MRI, which we sought to examine in this review. A literature
   search of the major databases (PubMed, MEDLINE, Web of Science,
   ClinicalTrials.gov) was conducted according to the Preferred Reporting
   Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The
   search yielded 1226 results, of which 50 studies were selected for
   inclusion. Key data from these studies were extracted. Studies were
   categorized thematically into the following: Image Acquisition and
   Processing, Segmentation, Diagnosis and Treatment Planning, and Patient
   Selection and Prognostication. Gaps in the literature and the proposed
   areas of future research are discussed. Current research demonstrates
   the ability of artificial intelligence to improve various aspects of
   this field, from image acquisition to analysis and clinical care. We
   also acknowledge the limitations of current technology. Future work will
   require collaborative efforts in order to fully exploit new technologies
   while addressing the practical challenges of generalizability and
   implementation. In particular, the use of foundation models and
   large-language models in spine MRI is a promising area, warranting
   further research. Studies assessing model performance in real-world
   clinical settings will also help uncover unintended consequences and
   maximize the benefits for patient care.
ZB 0
ZS 0
Z8 0
TC 3
ZR 0
ZA 0
Z9 3
DA 2024-10-07
UT WOS:001322923600001
PM 39329636
ER

PT J
AU Raja, Hina
   Huang, Xiaoqin
   Delsoz, Mohammad
   Madadi, Yeganeh
   Poursoroush, Asma
   Munawar, Asim
   Kahook, Malik Y.
   Yousefi, Siamak
TI Diagnosing Glaucoma Based on the Ocular Hypertension Treatment Study
   Dataset Using Chat Generative Pre-Trained Transformer as a Large
   Language Model
SO OPHTHALMOLOGY SCIENCE
VL 5
IS 1
AR 100599
DI 10.1016/j.xops.2024.100599
EA SEP 2024
DT Article
PD JAN-FEB 2025
PY 2025
AB Purpose: To evaluate the capabilities of Chat Generative Pre-Trained
   Transformer (ChatGPT), as a large language model (LLM), for diagnosing
   glaucoma using the Ocular Hypertension Treatment Study (OHTS) dataset,
   and comparing the diagnostic capability of ChatGPT 3.5 and ChatGPT 4.0.
   Design: Prospective data collection study. Participants: A total of 3170
   eyes of 1585 subjects from the OHTS were included in this study.
   Methods: We selected demographic, clinical, ocular, visual field, optic
   nerve head photo, and history of disease parameters of each participant
   and developed case reports by converting tabular data into textual
   format based on information from both eyes of all subjects. We then
   developed a procedure using the application programming interface of
   ChatGPT, a LLM-based chatbot, to automatically input prompts into a chat
   box. This was followed by querying 2 different generations of ChatGPT
   (versions 3.5 and 4.0) regarding the underlying diagnosis of each
   subject. We then evaluated the output responses based on several
   objective metrics. Main Outcome Measures: Area under the receiver
   operating characteristic curve (AUC), accuracy, specificity,
   sensitivity, and F1 score. Results: Chat Generative Pre-Trained
   Transformer 3.5 achieved AUC of 0.74, accuracy of 66%, specificity of
   64%, sensitivity of 85%, and F1 score of 0.72. Chat Generative
   Pre-Trained Transformer 4.0 obtained AUC of 0.76, accuracy of 87%,
   specificity of 90%, sensitivity of 61%, and F1 score of 0.92.
   Conclusions: The accuracy of ChatGPT 4.0 in diagnosing glaucoma based on
   input data from OHTS was promising. The overall accuracy of ChatGPT 4.0
   was higher than ChatGPT 3.5. However, ChatGPT 3.5 was found to be more
   sensitive than ChatGPT 4.0. In its current forms, ChatGPT may serve as a
   useful tool in exploring disease status of ocular hypertensive eyes when
   specific data are available for analysis. In the future, leveraging LLMs
   with multimodal capabilities, allowing for integration of imaging and
   diagnostic testing as part of the analyses, could further enhance
   diagnostic capabilities and enhance diagnostic accuracy. Financial
   Disclosures: Proprietary or commercial disclosure may be found in the
   Footnotes and Disclosures at the end of this article. Ophthalmology
   Science 2025;5:100599 (c) 2024 by the American Academy of Ophthalmology.
   This is an open access article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-ncnd/4.0/).
ZR 0
ZB 0
ZA 0
Z8 0
TC 3
ZS 0
Z9 3
DA 2024-10-16
UT WOS:001330416200001
PM 39346574
ER

PT J
AU Chen, Xiaolan
   Zhang, Weiyi
   Zhao, Ziwei
   Xu, Pusheng
   Zheng, Yingfeng
   Shi, Danli
   He, Mingguang
TI ICGA-GPT: report generation and question answering for indocyanine green
   angiography images
SO BRITISH JOURNAL OF OPHTHALMOLOGY
VL 108
IS 10
BP 1450
EP 1456
DI 10.1136/bjo-2023-324446
EA MAR 2024
DT Article
PD OCT 2024
PY 2024
AB Background Indocyanine green angiography (ICGA) is vital for diagnosing
   chorioretinal diseases, but its interpretation and patient communication
   require extensive expertise and time-consuming efforts. We aim to
   develop a bilingual ICGA report generation and question-answering (QA)
   system.
   Methods Our dataset comprised 213 129 ICGA images from 2919
   participants. The system comprised two stages: image-text alignment for
   report generation by a multimodal transformer architecture, and large
   language model (LLM)-based QA with ICGA text reports and human-input
   questions. Performance was assessed using both qualitative metrics
   (including Bilingual Evaluation Understudy (BLEU), Consensus-based Image
   Description Evaluation (CIDEr), Recall-Oriented Understudy for Gisting
   Evaluation-Longest Common Subsequence (ROUGE-L), Semantic Propositional
   Image Caption Evaluation (SPICE), accuracy, sensitivity, specificity,
   precision and F1 score) and subjective evaluation by three experienced
   ophthalmologists using 5-point scales (5 refers to high quality).
   Results We produced 8757 ICGA reports covering 39 disease-related
   conditions after bilingual translation (66.7% English, 33.3% Chinese).
   The ICGA-GPT model's report generation performance was evaluated with
   BLEU scores (1-4) of 0.48, 0.44, 0.40 and 0.37; CIDEr of 0.82; ROUGE of
   0.41 and SPICE of 0.18. For disease-based metrics, the average
   specificity, accuracy, precision, sensitivity and F1 score were 0.98,
   0.94, 0.70, 0.68 and 0.64, respectively. Assessing the quality of 50
   images (100 reports), three ophthalmologists achieved substantial
   agreement (kappa=0.723 for completeness, kappa=0.738 for accuracy),
   yielding scores from 3.20 to 3.55. In an interactive QA scenario
   involving 100 generated answers, the ophthalmologists provided scores of
   4.24, 4.22 and 4.10, displaying good consistency (kappa=0.779).
   Conclusion This pioneering study introduces the ICGA-GPT model for
   report generation and interactive QA for the first time, underscoring
   the potential of LLMs in assisting with automated ICGA image
   interpretation.
ZS 0
TC 10
ZB 3
ZR 0
Z8 0
ZA 0
Z9 10
DA 2024-03-30
UT WOS:001189002900001
PM 38508675
ER

PT J
AU MacKay, Emily J.
   Goldfinger, Shir
   Chan, Trevor J.
   Grasfield, Rachel H.
   Eswar, Vikram J.
   Li, Kelly
   Cao, Quy
   Pouch, Alison M.
TI Automated structured data extraction from intraoperative
   echocardiography reports using large language models
SO BRITISH JOURNAL OF ANAESTHESIA
VL 134
IS 5
BP 1308
EP 1317
DI 10.1016/j.bja.2025.01.028
EA APR 2025
DT Article
PD MAY 2025
PY 2025
AB Background: Consensus-based large language model (LLM) ensembles might
   provide an automated solution for extracting structured data from
   unstructured text in echocardiography reports. Methods: This
   cross-sectional study utilised 600 intraoperative transoesophageal
   reports (100 for prompt engineering; 500 for testing) randomly sampled
   from 7106 adult patients undergoing cardiac surgery at two hospitals
   within the University of Pennsylvania Healthcare System. Three
   echocardiographic parameters (left ventricular ejection fraction, right
   ventricular systolic function, and tricuspid regurgitation) were
   extracted from both the presurgical and postsurgical sections of the
   reports. LLM ensembles were generated using five open-source LLMs and
   four voting strategies: (1) unanimous (five out of five in agreement);
   (2) supermajority (four or more of five in agreement); (3) majority
   (three or more of five in agreement); and (4) plurality (two or more of
   five in agreement). Returned LLM ensemble responses were compared with
   the reference standard dataset to calculate raw accuracy, consensus
   accuracy, error rate, and yield. Results: Of the four LLM ensembles, the
   unanimous LLM ensemble achieved the highest consensus accuracies (99.4%
   presurgical; 97.9% postsurgical) and the lowest error rates (0.6%
   presurgical; 2.1% postsurgical) but had the lowest data extraction
   yields (81.7% presurgical; 80.5% postsurgical) and the lowest raw
   accuracies (81.2% presurgical; 78.9% postsurgical). In contrast, the
   plurality LLM ensemble achieved the highest raw accuracies (96.1%
   presurgical; 93.7% postsurgical) and the highest data extraction yields
   (99.4% presurgical; 98.9% postsurgical) but had the lowest consensus
   accuracies (96.7% presurgical; 94.7% postsurgical) and highest error
   rates (3.3% presurgical; 5.3% postsurgical). Conclusions: A
   consensus-based LLM ensemble successfully generated structured data from
   unstructured text contained in intraoperative transoesophageal reports.
Z8 0
ZB 0
ZR 0
ZS 0
TC 0
ZA 0
Z9 0
DA 2025-05-07
UT WOS:001477112200001
PM 40037947
ER

PT J
AU Dai, Jiayi
   Kim, Mi-Young
   Sutton, Reed T.
   Mitchell, Joseph R.
   Goebel, Randolph G.
   Baumgart, Daniel C.
TI DEVELOPMENT OF IBDBERT - NATURAL LANGUAGE PROCESSING ANALYSIS OF CROHN'S
   DISEASE COMPUTED TOMOGRAPHY ENTEROGRAPHY (CTE) REPORTS
SO GASTROENTEROLOGY
VL 166
IS 5
MA Sa2032
BP S612
EP S612
SU S
DT Meeting Abstract
PD MAY 2024
PY 2024
CT Digestive Disease Week (DDW)
CY MAY 18-21, 2024
CL Washington, DC
ZS 0
ZR 0
TC 0
ZA 0
Z8 0
ZB 0
Z9 0
DA 2024-10-30
UT WOS:001282837702379
ER

PT J
AU Sun, Di
   Hadjiiski, Lubomir
   Gormley, John
   Chan, Heang-Ping
   Caoili, Elaine
   Cohan, Richard
   Alva, Ajjai
   Bruno, Grace
   Mihalcea, Rada
   Zhou, Chuan
   Gulani, Vikas
TI Outcome Prediction Using Multi-Modal Information: Integrating Large
   Language Model-Extracted Clinical Information and Image Analysis
SO CANCERS
VL 16
IS 13
AR 2402
DI 10.3390/cancers16132402
DT Article
PD JUL 2024
PY 2024
AB Simple Summary: Predicting the survival of bladder cancer patients
   following cystectomy can offer valuable information for treatment
   planning, decision-making, patient counseling, and resource allocation.
   Our aim was to develop large language model (LLM)-aided multi-modal
   predictive models, based on clinical information and CT images. These
   models achieved performances comparable to those of multi-modal
   predictive models that rely on manually extracted clinical information.
   This study demonstrates the potential of employing LLMs to process
   medical data, and of integrating LLM-processed data into modeling for
   prognosis.
   Survival prediction post-cystectomy is essential for the follow-up care
   of bladder cancer patients. This study aimed to evaluate artificial
   intelligence (AI)-large language models (LLMs) for extracting clinical
   information and improving image analysis, with an initial application
   involving predicting five-year survival rates of patients after radical
   cystectomy for bladder cancer. Data were retrospectively collected from
   medical records and CT urograms (CTUs) of bladder cancer patients
   between 2001 and 2020. Of 781 patients, 163 underwent chemotherapy, had
   pre- and post-chemotherapy CTUs, underwent radical cystectomy, and had
   an available post-surgery five-year survival follow-up. Five AI-LLMs
   (Dolly-v2, Vicuna-13b, Llama-2.0-13b, GPT-3.5, and GPT-4.0) were used to
   extract clinical descriptors from each patient's medical records. As a
   reference standard, clinical descriptors were also extracted manually.
   Radiomics and deep learning descriptors were extracted from CTU images.
   The developed multi-modal predictive model, CRD, was based on the
   clinical (C), radiomics (R), and deep learning (D) descriptors. The LLM
   retrieval accuracy was assessed. The performances of the survival
   predictive models were evaluated using AUC and Kaplan-Meier analysis.
   For the 163 patients (mean age 64 +/- 9 years; M:F 131:32), the LLMs
   achieved extraction accuracies of 74%similar to 87% (Dolly), 76%similar
   to 83% (Vicuna), 82%similar to 93% (Llama), 85%similar to 91% (GPT-3.5),
   and 94%similar to 97% (GPT-4.0). For a test dataset of 64 patients, the
   CRD model achieved AUCs of 0.89 +/- 0.04 (manually extracted
   information), 0.87 +/- 0.05 (Dolly), 0.83 +/- 0.06 similar to 0.84 +/-
   0.05 (Vicuna), 0.81 +/- 0.06 similar to 0.86 +/- 0.05 (Llama), 0.85 +/-
   0.05 similar to 0.88 +/- 0.05 (GPT-3.5), and 0.87 +/- 0.05 similar to
   0.88 +/- 0.05 (GPT-4.0). This study demonstrates the use of LLM
   model-extracted clinical information, in conjunction with imaging
   analysis, to improve the prediction of clinical outcomes, with bladder
   cancer as an initial example.
ZB 0
Z8 0
ZS 0
ZR 0
ZA 0
TC 4
Z9 4
DA 2024-07-24
UT WOS:001270395100001
PM 39001463
ER

PT J
AU Anonymous
TI Meeting of the Anaesthetic-Research-Society, London, UK, May 16 -17,
   2024 
SO British Journal of Anaesthesia
VL 133
IS 2
BP 458
EP 472
DT Meeting
PD AUG 2024
PY 2024
AB This "Abstracts from Anesthetic Research Society Meeting", which focuses
   on different anesthesia treatments to patient during various treatment
   interventions like surgery or other diagnostic or therapeutic
   procedures, contains approximately 23 abstract presentations, written in
   English. Topics include local anaesthetic treatment, cancer surgery,
   perioperative management, cell apoptosis, cell proliferation, general
   anaesthesia, colorectal cancer, quality-of-life, length of hospital
   stay, patient-reported ethnicity, postpartum hemorrhage. Other topics
   include large language model, hallucination, questionnaire,
   perioperative medication advice, proteomic analysis, lung resection,
   cardiac magnetic resonance imaging, extracellular volume, plasma
   protein, lung protective ventilation, conventional ventilation,
   postoperative pulmonary complication, major noncardiac surgery:,
   myocardial inflammation.
CT Meeting of the Anaesthetic-Research-Society
CY May 16 -17, 2024
CL London, UK
HO London, UK
SP Anaesthet Res Soc
Z8 0
TC 0
ZA 0
ZB 0
ZR 0
ZS 0
Z9 0
DA 2024-08-30
UT BCI:BCI202400741698
ER

PT J
AU Jiang, Bin
   Pham, Nancy
   Staalduinen, Eric K. van
   Liu, Yongkai
   Nazari-Farsani, Sanaz
   Sanaat, Amirhossein
   van Voorst, Henk
   Fettahoglu, Ates
   Kim, Donghoon
   Ouyang, Jiahong
   Kumar, Ashwin
   Srivatsan, Aditya
   Hussein, Ramy
   Lansberg, Maarten G.
   Boada, Fernando
   Zaharchuck, Gerg
TI Deep Learning Applications in Imaging of Acute Ischemic Stroke: A
   Systematic Review and Narrative Summary
SO RADIOLOGY
VL 315
IS 1
BP 1
EP 16
DI 10.1148/radiol.240775
DT Article
PD APR 2025
PY 2025
AB Background: Acute ischemic stroke (AIS) is a major cause of morbidity
   and mortality, requiring swift and precise clinical decisions based on
   neuroimaging. Recent advances in deep learning-based computer vision and
   language artificial intelligence (AI) models have demonstrated
   transformative performance for several stroke-related applications.
   Purpose: To evaluate deep learning applications for imaging in AIS in
   adult patients, providing a comprehensive overview of the current state
   of the technology and identifying opportunities for advancement.
   Materials and Methods: A systematic literature review was conducted
   following Preferred Reporting Items for Systematic Reviews and
   Meta-Analyses guidelines. A comprehensive search of four databases from
   January 2016 to January 2024 was performed, targeting deep learning
   applications for imaging of AIS, including automated detection of large
   vessel occlusion and measurement of Alberta Stroke Program Early CT
   Score. Articles were selected based on predefined inclusion and
   exclusion criteria, focusing on convolutional neural networks and
   transformers. The top-represented areas were addressed, and the relevant
   information was extracted and summarized. Results: Of 380 studies
   included, 171 (45.0%) focused on stroke lesion segmentation, 129 (33.9%)
   on classification and triage, 31 (8.2%) on outcome prediction, 15 (3.9%)
   on generative AI and large language models, and 11 (2.9%) on rapid or
   low-dose imaging specific to stroke applications. Detailed data
   extraction was performed for 68 studies. Public AIS datasets are also
   highlighted, for researchers developing AI models for stroke imaging.
   Conclusion: Deep learning applications have permeated AIS imaging,
   particularly for stroke lesion segmentation. However, challenges remain,
   including the need for standardized protocols and test sets, larger
   public datasets, and performance validation in real-world settings.
ZB 0
Z8 0
TC 0
ZR 0
ZS 0
ZA 0
Z9 0
DA 2025-04-27
UT WOS:001469734000006
PM 40197098
ER

PT J
AU Moore, N. S.
   Laird, J. H., Jr.
   Verma, N.
   Hager, T.
   Sritharan, D.
   Lee, V.
   Maresca, R.
   Chadha, S.
   Park, H. S. M.
   Aneja, S.
TI Applying Language Models to Radiology Text for Identifying
   Oligometastatic Non-Small Cell Lung Cancer
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3413
BP E644
EP E644
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZS 0
TC 0
ZB 0
Z8 0
ZA 0
ZR 0
Z9 0
DA 2024-12-16
UT WOS:001325892302094
ER

PT J
AU Chen, Tse Chiang
   Couldwell, Mitchell W.
   Singer, Jorie
   Singer, Alyssa
   Koduri, Laila
   Kaminski, Emily
   Nguyen, Khoa
   Multala, Evan
   Dumont, Aaron S.
   Wang, Arthur
TI Assessing the clinical reasoning of ChatGPT for mechanical thrombectomy
   in patients with stroke
SO JOURNAL OF NEUROINTERVENTIONAL SURGERY
VL 16
IS 3
BP 253
EP 260
DI 10.1136/jnis-2023-021163
EA JAN 2024
DT Article
PD MAR 2024
PY 2024
AB BackgroundArtificial intelligence (AI) has become a promising tool in
   medicine. ChatGPT, a large language model AI Chatbot, shows promise in
   supporting clinical practice. We assess the potential of ChatGPT as a
   clinical reasoning tool for mechanical thrombectomy in patients with
   stroke.MethodsAn internal validation of the abilities of ChatGPT was
   first performed using artificially created patient scenarios before
   assessment of real patient scenarios from the medical center's stroke
   database. All patients with large vessel occlusions who underwent
   mechanical thrombectomy at Tulane Medical Center between January 1, 2022
   and December 31, 2022 were included in the study. The performance of
   ChatGPT in evaluating which patients should undergo mechanical
   thrombectomy was compared with the decisions made by board-certified
   stroke neurologists and neurointerventionalists. The interpretation
   skills, clinical reasoning, and accuracy of ChatGPT were
   analyzed.Results102 patients with large vessel occlusions underwent
   mechanical thrombectomy. ChatGPT agreed with the physician's decision
   whether or not to pursue thrombectomy in 54.3% of the cases. ChatGPT had
   mistakes in 8.8% of the cases, consisting of mathematics, logic, and
   misinterpretation errors. In the internal validation phase, ChatGPT was
   able to provide nuanced clinical reasoning and was able to perform
   multi-step thinking, although with an increased rate of making
   mistakes.ConclusionChatGPT shows promise in clinical reasoning,
   including the ability to factor a patient's underlying comorbidities
   when considering mechanical thrombectomy. However, ChatGPT is prone to
   errors as well and should not be relied on as a sole decision-making
   tool in its present form, but it has potential to assist clinicians with
   more efficient work flow.
ZR 0
ZB 0
Z8 0
ZA 0
ZS 0
TC 8
Z9 8
DA 2024-01-21
UT WOS:001142621600001
PM 38184368
ER

PT J
AU Yamagishi, Yosuke
   Nakamura, Yuta
   Hanaoka, Shouhei
   Abe, Osamu
TI Large Language Model Approach for Zero-Shot Information Extraction and
   Clustering of Japanese Radiology Reports: Algorithm Development and
   Validation
SO JMIR CANCER
VL 11
AR e57275
DI 10.2196/57275
DT Article
PD 2025
PY 2025
AB Background: The application of natural language processing in medicine
   has increased significantly, including tasks such as information
   extraction and classification. Natural language processing plays a
   crucial role in structuring free-form radiology reports, facilitating
   the interpretation of textual content, and enhancing data utility
   through clustering techniques. Clustering allows for the identification
   of similar lesions and disease patterns across a broad dataset, making
   it useful for aggregating information and discovering new insights in
   medical imaging. However, most publicly available medical datasets are
   in English, with limited resources in other languages. This scarcity
   poses a challenge for development of models geared toward non-English
   downstream tasks. Objective: This study aimed to develop and evaluate an
   algorithm that uses large language models (LLMs) to extract information
   from Japanese lung cancer radiology reports and perform clustering
   analysis. The effectiveness of this approach was assessed and compared
   with previous supervised methods. Methods: This study employed the
   MedTxt-RR dataset, comprising 135 Japanese radiology reports from 9
   radiologists who interpreted the computed tomography images of 15 lung
   cancer patients obtained from Radiopaedia. Previously used in the
   NTCIR-16 (NII Testbeds and Community for Information Access Research)
   shared task for clustering performance competition, this dataset was
   ideal for comparing the clustering ability of our algorithm with those
   of previous methods. The dataset was split into 8 cases for development
   and 7 for testing, respectively. The study's approach involved using the
   LLM to extract information pertinent to lung cancer findings and
   transforming it into numeric features for clustering, using the K-means
   method. Performance was evaluated using 135 reports for information
   extraction accuracy and 63 test reports for clustering performance. This
   study focused on the accuracy of automated systems for extracting tumor
   size, location, and laterality from clinical reports. The clustering
   performance was evaluated using normalized mutual information, adjusted
   mutual information , and the Fowlkes-Mallows index for both the
   development and test data. Results: The tumor size was accurately
   identified in 99 out of 135 reports (73.3%), with errors in 36 reports
   (26.7%), primarily due to missing or incorrect size information. Tumor
   location and laterality were identified with greater accuracy in 112 out
   of 135 reports (83%); however, 23 reports (17%) contained errors mainly
   due to empty values or incorrect data. Clustering performance of the
   test data yielded an normalized mutual information of 0.6414, adjusted
   mutual information of 0.5598, and Fowlkes-Mallows index of 0.5354. The
   proposed method demonstrated superior performance across all evaluation
   metrics compared to previous methods. Conclusions: The unsupervised LLM
   approach surpassed the existing supervised methods in clustering
   Japanese radiology reports. These findings suggest that LLMs hold
   promise for extracting information from radiology reports and
   integrating it into disease-specific knowledge structures.
TC 0
ZS 0
Z8 0
ZR 0
ZA 0
ZB 0
Z9 0
DA 2025-02-20
UT WOS:001420173900001
PM 39864093
ER

PT J
AU Scuricini, Alessandro
   Ramoni, Davide
   Liberale, Luca
   Montecucco, Fabrizio
   Carbone, Federico
TI The role of artificial intelligence in cardiovascular research: Fear
   less and live bolder
SO EUROPEAN JOURNAL OF CLINICAL INVESTIGATION
VL 55
SI SI
AR e14364
DI 10.1111/eci.14364
SU 1
DT Review
PD APR 2025
PY 2025
AB BackgroundArtificial intelligence (AI) has captured the attention of
   everyone, including cardiovascular (CV) clinicians and scientists.
   Moving beyond philosophical debates, modern cardiology cannot overlook
   AI's growing influence but must actively explore its potential
   applications in clinical practice and research methodology.Methods and
   ResultsAI offers exciting possibilities for advancing CV medicine by
   uncovering disease heterogeneity, integrating complex multimodal data,
   and enhancing treatment strategies. In this review, we discuss the
   innovative applications of AI in cardiac electrophysiology, imaging,
   angiography, biomarkers, and genomic data, as well as emerging tools
   like face recognition and speech analysis. Furthermore, we focus on the
   expanding role of machine learning (ML) in predicting CV risk and
   outcomes, outlining a roadmap for the implementation of AI in CV care
   delivery. While the future of AI holds great promise, technical
   limitations and ethical challenges remain significant barriers to its
   widespread clinical adoption.ConclusionsAddressing these issues through
   the development of high-quality standards and involving key stakeholders
   will be essential for AI to transform cardiovascular care safely and
   effectively.
ZS 0
ZR 0
TC 1
ZB 0
Z8 0
ZA 0
Z9 1
DA 2025-04-12
UT WOS:001460920400008
PM 40191936
ER

PT J
AU Zhu, Zirui
   Zeng, Zhuo
   Zeng, Huiqing
   Luo, Xiongbiao
TI Research progress on artificial intelligence driving precision diagnosis
   and treatment of chronic obstructive pulmonary disease
SO Xiamen Daxue Xuebao (Ziran Kexue Ban)
VL 63
IS 5
BP 894
EP 905
DI 10.6043/j.issn.0438-0479.202402019
DT Article
PD SEP 2024
PY 2024
AB [Background] Chronic obstructive pulmonary disease (COPD) is a complex
   and prevalent respiratory disorder with irreversible airflow limitation
   worldwide, Precision diagnosis and treatment at its early stage
   significantly improve the quality of life of patients, COPD symptoms are
   diverse and progressive, e. g. chronic cough, sputum production, dyspnea
   and chest tightness. indicating advances in COPD, While the
   pathophysiology of COPD is multifaceted with persistent airway
   inflammation, airway remodeling, and alveolar destruction, the etiology
   of COPD is multifactorial, including prolonged smoking, environmental
   pollutants. occupational hazards, and genetic predispositions. These
   factors collectively result in airflow obstruction and pathological
   changes in the respiratory tract, Specifically, the progression of COPD
   is often accompanied with persistent inflammatory responses, oxidative
   stress and intensive pulmonary damage, [Progress] Pulmonary function
   tests (PFTs) are routinely performed to examine COPD. providing
   physicians with a ratio of the forced expiratory volume in one second by
   the forced vital capacity to evaluate COPD, Unfortunately, the results
   of PFTs critically affected by the effort of patients, and the
   interpretation of PFTs also depends on experience and skills of
   physicians. While PFTs allow physicians to quantify the severity of
   COPD, they do not reach a specific diagnosis and are commonly associated
   with medical history, physical examination such as CT imaging,
   functional MR imaging and respiratory sound, and laboratory data to
   determine a diagnosis. Therefore, physicians expect more precise COPD
   diagnosis and treatment methods than conventional ones to improve
   patient's quality of life. Nowadays artificial intelligence (AI) is
   widely discussed in precision medicine. Specifically, Al techniques or
   mathematical models also increasingly used in COPD diagnosis, treatment,
   monitoring, and management, These models are generally categorized into
   unimodal and multimodal Al models in accordance with clinical COPD data.
   While the unimodal model uses only a single one modality such as PFTs or
   CT images, the multimodal model fuses a diversity of data ineluding
   imaging, biomedical information, and clinical records, All these models
   generally provide physicians with a holistic assessment of COPD,
   patient-specific treatment for precision medicine, [Perspective] In
   general, Al techniques provide a promising way to precisely diagnose and
   treat COPD in its early stage, as well as COPD management and
   monitoring. Specifically, artificial general intelligence, generative
   artificial intelligence, multimodal large language models are innovating
   clinical methods in diagnosis, treatment, monitoring, and management of
   pulmonary diseases, although they still suffer from medical data privacy
   and security, model generalizability, interpretability and complexity,
   legal and ethical issues. Future research should address these issues in
   various angles, It is essential to strengthen privacy protection and
   security measures, Moreover, it is vital to improve the
   generalizability, transparency and interpretability and reduce the
   complexity of various Al models in clinical applications, Additionally,
   medical ethics are important when applying Al techniques to precision
   pulmonary medicine.
TC 0
ZA 0
ZR 0
Z8 0
ZB 0
ZS 0
Z9 0
DA 2025-03-21
UT BCI:BCI202500316530
ER

PT J
AU Isaksson, Lars Johannes
   Summers, Paul
   Mastroleo, Federico
   Marvaso, Giulia
   Corrao, Giulia
   Vincini, Maria Giulia
   Zaffaroni, Mattia
   Ceci, Francesco
   Petralia, Giuseppe
   Orecchia, Roberto
   Jereczek-Fossa, Barbara Alicja
TI Automatic Segmentation with Deep Learning in Radiotherapy
SO CANCERS
VL 15
IS 17
AR 4389
DI 10.3390/cancers15174389
DT Review
PD SEP 2023
PY 2023
AB Simple Summary Automatic segmentation of organs and other regions of
   interest is a promising approach for reducing the workload of doctors in
   radiotherapeutic planning, but it can be hard for doctors and
   researchers to keep up with current developments. This review evaluates
   807 papers and reveals trends, commonalities, and gaps in the existing
   corpus. A set of recommendations for conducting effective segmentation
   studies is also provided.Abstract This review provides a formal overview
   of current automatic segmentation studies that use deep learning in
   radiotherapy. It covers 807 published papers and includes multiple
   cancer sites, image types (CT/MRI/PET), and segmentation methods. We
   collect key statistics about the papers to uncover commonalities,
   trends, and methods, and identify areas where more research might be
   needed. Moreover, we analyzed the corpus by posing explicit questions
   aimed at providing high-quality and actionable insights, including:
   "What should researchers think about when starting a segmentation
   study?", "How can research practices in medical image segmentation be
   improved?", "What is missing from the current corpus?", and more. This
   allowed us to provide practical guidelines on how to conduct a good
   segmentation study in today's competitive environment that will be
   useful for future research within the field, regardless of the specific
   radiotherapeutic subfield. To aid in our analysis, we used the large
   language model ChatGPT to condense information.
ZB 4
TC 20
ZR 0
ZS 0
ZA 0
Z8 1
Z9 21
DA 2023-09-21
UT WOS:001060516700001
PM 37686665
ER

PT J
AU Rajendran, Praveenbalaji
   Chen, Yizheng
   Qiu, Liang
   Niedermayr, Thomas
   Liu, Wu
   Buyyounouski, Mark
   Bagshaw, Hilary
   Han, Bin
   Yang, Yong
   Kovalchuk, Nataliya
   Gu, Xuejun
   Hancock, Steven
   Xing, Lei
   Dai, Xianjin
TI Autodelineation of Treatment Target Volume for Radiation Therapy Using
   Large Language Model-Aided Multimodal Learning
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 121
IS 1
BP 230
EP 240
DI 10.1016/j.ijrobp.2024.07.2149
EA DEC 2024
DT Article
PD JAN 1 2025
PY 2025
AB Purpose: Artificial intelligence-aided methods have made significant
   progress in the auto-delineation of normal tissues. However, these
   approaches struggle with the auto-contouring of radiation therapy target
   volume. Our goal was to model the delineation of target volume as a
   clinical decision-making problem, resolved by leveraging large language
   model-aided multimodal learning approaches. Methods and Materials: A
   vision-language model, termed Medformer, has been developed, employing
   the hierarchical vision transformer as its backbone and incorporating
   large language models to extract text-rich features. The contextually
   embedded linguistic features are seamlessly integrated into visual
   features for language-aware visual encoding through the visual language
   attention module. Metrics, including Dice similarity coefficient (DSC),
   intersection over union (IOU), and 95th percentile Hausdorff distance
   (HD95), were used to quantitatively evaluate the performance of our
   model. The evaluation was conducted on an in-house prostate cancer data
   set and a public oropharyngeal carcinoma data set, totaling 668
   subjects. Results: Our Medformer achieved a DSC of 0.81 f 0.10 versus
   0.72 f 0.10, IOU of 0.73 f 0.12 versus 0.65 f 0.09, and HD95 of 9.86 f
   9.77 mm versus 19.13 f 12.96 mm for delineation of gross tumor volume on
   the prostate cancer dataset. Similarly, on the oropharyngeal carcinoma
   dataset, it achieved a DSC of 0.77 f 0.11 versus 0.72 f 0.09, IOU of
   0.70 f 0.09 versus 0.65 f 0.07, and HD95 of 7.52 f 4.8 mm versus 13.63 f
   7.13 mm, representing significant improvements (P <0.05). For
   delineating the clinical target volume, Medformer achieved a DSC of 0.91
   f 0.04, IOU of 0.85 f 0.05, and HD95 of 2.98 f 1.60 mm, comparable with
   other state-of-the-art algorithms. Conclusions: Auto-delineation of the
   treatment target based on multimodal learning outperforms conventional
   approaches that rely purely on visual features. Our method could be
   adopted into routine practice to rapidly contour clinical target
   volume/gross tumor volume. (c) 2024 Elsevier Inc. All rights are
   reserved, including those for text and data mining, AI training, and
   similar technologies.
ZR 0
TC 3
ZA 0
ZB 0
ZS 0
Z8 0
Z9 3
DA 2025-02-10
UT WOS:001413606000001
PM 39117164
ER

PT J
AU Wu, Wanying
   Guo, Yuhu
   Li, Qi
   Jia, Congzhuo
TI Exploring the potential of large language models in identifying
   metabolic dysfunction-associated steatotic liver disease: A comparative
   study of non-invasive tests and artificial intelligence-generated
   responses
SO LIVER INTERNATIONAL
VL 45
IS 4
DI 10.1111/liv.16112
EA NOV 2024
DT Article
PD APR 2025
PY 2025
AB Background and AimsThis study sought to assess the capabilities of large
   language models (LLMs) in identifying clinically significant metabolic
   dysfunction-associated steatotic liver disease (MASLD).MethodsWe
   included individuals from NHANES 2017-2018. The validity and reliability
   of MASLD diagnosis by GPT-3.5 and GPT-4 were quantitatively examined and
   compared with those of the Fatty Liver Index (FLI) and United States FLI
   (USFLI). A receiver operating characteristic curve was conducted to
   assess the accuracy of MASLD diagnosis via different scoring systems.
   Additionally, GPT-4V's potential in clinical diagnosis using ultrasound
   images from MASLD patients was evaluated to provide assessments of LLM
   capabilities in both textual and visual data interpretation.ResultsGPT-4
   demonstrated comparable performance in MASLD diagnosis to FLI and USFLI
   with the AUROC values of .831 (95% CI .796-.867), .817 (95% CI
   .797-.837) and .827 (95% CI .807-.848), respectively. GPT-4 exhibited a
   trend of enhanced accuracy, clinical relevance and efficiency compared
   to GPT-3.5 based on clinician evaluation. Additionally, Pearson's r
   values between GPT-4 and FLI, as well as USFLI, were .718 and .695,
   respectively, indicating robust and moderate correlations. Moreover,
   GPT-4V showed potential in understanding characteristics from hepatic
   ultrasound imaging but exhibited limited interpretive accuracy in
   diagnosing MASLD compared to skilled radiologists.ConclusionsGPT-4
   achieved performance comparable to traditional risk scores in diagnosing
   MASLD and exhibited improved convenience, versatility and the capacity
   to offer user-friendly outputs. The integration of GPT-4V highlights the
   capacities of LLMs in handling both textual and visual medical data,
   reinforcing their expansive utility in healthcare practice.
TC 0
ZA 0
ZR 0
Z8 0
ZB 0
ZS 0
Z9 0
DA 2024-11-23
UT WOS:001354198800001
PM 39526465
ER

PT J
AU Ghorbian, Mohsen
   Ghobaei-Arani, Mostafa
   Ghorbian, Saied
TI Transforming breast cancer diagnosis and treatment with large language
   Models: A comprehensive survey
SO METHODS
VL 239
BP 85
EP 110
DI 10.1016/j.ymeth.2025.04.001
EA APR 2025
DT Article
PD JUL 2025
PY 2025
AB Breast cancer (BrCa), being one of the most prevalent forms of cancer in
   women, poses many challenges in the field of treatment and diagnosis due
   to its complex biological mechanisms. Early and accurate diagnosis plays
   a fundamental role in improving survival rates, but the limitations of
   existing imaging methods and clinical data interpretation often prevent
   optimal results. Large Language Models (LLMs), which are developed based
   on advanced architectures such as transformers, have brought about a
   significant revolution in data processing and medical decision-making.
   By analyzing a large volume of medical and clinical data, these models
   enable early diagnosis by identifying patterns in images and medical
   records and provide personalized treatment strategies by integrating
   genetic markers and clinical guidelines. Despite the transformative
   potential of these models, their use in BrCa management faces challenges
   such as data sensitivity, algorithm transparency, ethical
   considerations, and model compatibility with the details of medical
   applications that need to be addressed to achieve reliable results. This
   review systematically reviews the impact of LLMs on BrCa treatment and
   diagnosis. This study's objectives include analyzing the role of LLM
   technology in diagnosing and treating this disease. The findings
   indicate that the application of LLMs has resulted in significant
   improvements in various aspects of BrCa management, such as a 35%
   increase in the Efficiency of Diagnosis and BrCa Treatment (EDBC), a 30%
   enhancement in the System's Clinical Trust and Reliability (SCTR), and a
   20% improvement in the quality of patient education and information
   (IPEI). Ultimately, this study demonstrates the importance of LLMs in
   advancing precision medicine for BrCa and paves the way for effective
   patient-centered care solutions.
ZS 0
ZR 0
TC 0
ZB 0
ZA 0
Z8 0
Z9 0
DA 2025-04-20
UT WOS:001466448900001
PM 40199412
ER

PT J
AU Shenoy, Ujwala
   Zhang, Lu
   Jha, Mawra
   Kwong, Raymond
   Manning, Warren
   Nezafat, Reza
   Tsao, Connie
TI Development and Accuracy of Natural Language Processing-based Expression
   Matching to Identify and Classify Cardiomyopathy from Cardiovascular
   Magnetic Resonance Reports
SO CIRCULATION
VL 150
MA 4140236
DI 10.1161/circ.150.suppl_1.4140236
SU 1
DT Meeting Abstract
PD NOV 12 2024
PY 2024
ZS 0
TC 0
Z8 0
ZR 0
ZB 0
ZA 0
Z9 0
DA 2025-02-10
UT WOS:001398742703409
ER

PT J
AU Chen, Kun
   Xu, Wengui
   Li, Xiaofeng
TI The Potential of Gemini and GPTs for Structured Report Generation based
   on Free-Text <SUP>18</SUP>F-FDG PET/CT Breast Cancer Reports
SO ACADEMIC RADIOLOGY
VL 32
IS 2
BP 624
EP 633
DI 10.1016/j.acra.2024.08.052
EA FEB 2025
DT Article
PD FEB 2025
PY 2025
AB Rationale and objective: To compare the performance of large language
   model (LLM) based Gemini and Generative Pre-trained Transformers (GPTs)
   in data mining and generating structured reports based on free-text
   PET/CT reports for breast cancer after user-defined tasks.
   Materials and methods: Breast cancer patients (mean age, 50 years +/- 11
   [SD]; all female) who underwent consecutive F-18-FDG PET/ CT for
   follow-up between July 2005 and October 2023 were retrospectively
   included in the study. A total of twenty reports from 10 patients were
   used to train user-defined text prompts for Gemini and GPTs, by which
   structured PET/CT reports were generated. The natural language
   processing (NLP) generated structured reports and the structured reports
   annotated by nuclear medicine physicians were compared in terms of data
   extraction accuracy and capacity of progress decision-making.
   Statistical methods, including chisquare test, McNemar test and paired
   samples t-test, were employed in the study. Results: The
   structured PET/CT reports for 131 patients were generated by using the
   two NLP techniques, including Gemini and GPTs. In general, GPTs
   exhibited superiority over Gemini in data mining in terms of primary
   lesion size (89.6% vs. 53.8%, p < 0.001) and metastatic lesions (96.3%
   vs 89.6%, p < 0.001). Moreover, GPTs outperformed Gemini in making
   decision for progress (p < 0.001) and semantic similarity (F1 score
   0.930 vs 0.907, p < 0.001) for reports. Conclusion: GPTs
   outperformed Gemini in generating structured reports based on free-text
   PET/CT reports, which is potentially applied in clinical practice.
ZB 2
TC 4
ZS 0
ZR 0
Z8 1
ZA 0
Z9 4
DA 2025-02-26
UT WOS:001426380600001
PM 39245597
ER

PT J
AU Wu, Xuzhou
   Li, Guangxin
   Wang, Xing
   Xu, Zeyu
   Wang, Yingni
   Lei, Shuge
   Xian, Jianming
   Wang, Xueyu
   Zhang, Yibao
   Li, Gong
   Yuan, Kehong
TI Diagnosis assistant for liver cancer utilizing a large language model
   with three types of knowledge
SO PHYSICS IN MEDICINE AND BIOLOGY
VL 70
IS 9
AR 095009
DI 10.1088/1361-6560/adcb17
DT Article
PD MAY 4 2025
PY 2025
AB Objective. Liver cancer has a high incidence rate, but experienced
   doctors are lacking in primary healthcare settings. The development of
   large models offers new possibilities for diagnosis. However, in liver
   cancer diagnosis, large models face certain limitations, such as
   insufficient understanding of specific medical images, inadequate
   consideration of liver vessel factors, and inaccuracies in reasoning
   logic. Therefore, this study proposes a diagnostic assistance tool
   specific to liver cancer to enhance the diagnostic capabilities of
   primary care doctors. Approach. A liver cancer diagnosis framework
   combining large and small models is proposed. A more accurate model for
   liver tumor segmentation and a more precise model for liver vessel
   segmentation are developed. The features extracted from the segmentation
   results of the small models are combined with the patient's medical
   records and then provided to the large model. The large model employs
   chain of thought prompts to simulate expert diagnostic reasoning and
   uses Retrieval-Augmented Generation to provide reliable answers based on
   trusted medical knowledge and cases. Main results. In the small model
   part, the proposed liver tumor and liver vessel segmentation methods
   achieve improved performance. In the large model part, this approach
   receives higher evaluation scores from doctors when analyzing patient
   imaging and medical records. Significance. First, a diagnostic framework
   combining small models and large models is proposed to optimize the
   liver cancer diagnosis process. Second, two segmentation models are
   introduced to compensate for the large model's shortcomings in
   extracting semantic information from images. Third, by simulating
   doctors' reasoning and integrating trusted knowledge, the framework
   enhances the reliability and interpretability of the large model's
   responses while reducing hallucination phenomena.
Z8 0
ZS 0
ZA 0
ZR 0
TC 0
ZB 0
Z9 0
DA 2025-05-08
UT WOS:001480266600001
PM 40203862
ER

PT J
AU Su, Ziqing
   Tang, Guozhang
   Huang, Rui
   Qiao, Yang
   Zhang, Zheng
   Dai, Xingliang
TI Based on Medicine, The Now and Future of Large Language Models
SO CELLULAR AND MOLECULAR BIOENGINEERING
VL 17
IS 4
BP 263
EP 277
DI 10.1007/s12195-024-00820-3
EA SEP 2024
DT Review
PD AUG 2024
PY 2024
AB ObjectivesThis review explores the potential applications of large
   language models (LLMs) such as ChatGPT, GPT-3.5, and GPT-4 in the
   medical field, aiming to encourage their prudent use, provide
   professional support, and develop accessible medical AI tools that
   adhere to healthcare standards.MethodsThis paper examines the impact of
   technologies such as OpenAI's Generative Pre-trained Transformers (GPT)
   series, including GPT-3.5 and GPT-4, and other large language models
   (LLMs) in medical education, scientific research, clinical practice, and
   nursing. Specifically, it includes supporting curriculum design, acting
   as personalized learning assistants, creating standardized simulated
   patient scenarios in education; assisting with writing papers, data
   analysis, and optimizing experimental designs in scientific research;
   aiding in medical imaging analysis, decision-making, patient education,
   and communication in clinical practice; and reducing repetitive tasks,
   promoting personalized care and self-care, providing psychological
   support, and enhancing management efficiency in nursing.ResultsLLMs,
   including ChatGPT, have demonstrated significant potential and
   effectiveness in the aforementioned areas, yet their deployment in
   healthcare settings is fraught with ethical complexities, potential lack
   of empathy, and risks of biased responses.ConclusionDespite these
   challenges, significant medical advancements can be expected through the
   proper use of LLMs and appropriate policy guidance. Future research
   should focus on overcoming these barriers to ensure the effective and
   ethical application of LLMs in the medical field.
Z8 0
ZB 0
ZR 0
ZA 0
ZS 0
TC 2
Z9 2
DA 2024-09-25
UT WOS:001313521600001
PM 39372551
ER

PT J
AU Muntean, George Adrian
   Marginean, Anca
   Groza, Adrian
   Damian, Ioana
   Roman, Sara Alexia
   Hapca, Madalina Claudia
   Sere, Anca Madalina
   Manoiu, Roxana Mihaela
   Muntean, Maximilian Vlad
   Nicoara, Simona Delia
TI A Qualitative Evaluation of ChatGPT4 and PaLM2's Response to Patient's
   Questions Regarding Age-Related Macular Degeneration
SO DIAGNOSTICS
VL 14
IS 14
AR 1468
DI 10.3390/diagnostics14141468
DT Article
PD JUL 2024
PY 2024
AB Patient compliance in chronic illnesses is essential for disease
   management. This also applies to age-related macular degeneration (AMD),
   a chronic acquired retinal degeneration that needs constant monitoring
   and patient cooperation. Therefore, patients with AMD can benefit by
   being properly informed about their disease, regardless of the
   condition's stage. Information is essential in keeping them compliant
   with lifestyle changes, regular monitoring, and treatment. Large
   language models have shown potential in numerous fields, including
   medicine, with remarkable use cases. In this paper, we wanted to assess
   the capacity of two large language models (LLMs), ChatGPT4 and PaLM2, to
   offer advice to questions frequently asked by patients with AMD. After
   searching on AMD-patient-dedicated websites for frequently asked
   questions, we curated and selected a number of 143 questions. The
   questions were then transformed into scenarios that were answered by
   ChatGPT4, PaLM2, and three ophthalmologists. Afterwards, the answers
   provided by the two LLMs to a set of 133 questions were evaluated by two
   ophthalmologists, who graded each answer on a five-point Likert scale.
   The models were evaluated based on six qualitative criteria: (C1)
   reflects clinical and scientific consensus, (C2) likelihood of possible
   harm, (C3) evidence of correct reasoning, (C4) evidence of correct
   comprehension, (C5) evidence of correct retrieval, and (C6) missing
   content. Out of 133 questions, ChatGPT4 received a score of five from
   both reviewers to 118 questions (88.72%) for C1, to 130 (97.74%) for C2,
   to 131 (98.50%) for C3, to 133 (100%) for C4, to 132 (99.25%) for C5,
   and to 122 (91.73%) for C6, while PaLM2 to 81 questions (60.90%) for C1,
   to 114 (85.71%) for C2, to 115 (86.47%) for C3, to 124 (93.23%) for C4,
   to 113 (84.97%) for C5, and to 93 (69.92%) for C6. Despite the overall
   high performance, there were answers that are incomplete or inaccurate,
   and the paper explores the type of errors produced by these LLMs. Our
   study reveals that ChatGPT4 and PaLM2 are valuable instruments for
   patient information and education; however, since there are still some
   limitations to these models, for proper information, they should be used
   in addition to the advice provided by the physicians.
ZA 0
ZR 0
TC 1
ZS 0
ZB 0
Z8 0
Z9 1
DA 2024-08-01
UT WOS:001276597300001
PM 39061606
ER

PT J
AU Ra, Sinyoung
   Kim, Jonghun
   Na, Inye
   Ko, Eun Sook
   Park, Hyunjin
TI Enhancing radiomics features via a large language model for classifying
   benign and malignant breast tumors in mammography
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
VL 265
AR 108765
DI 10.1016/j.cmpb.2025.108765
EA APR 2025
DT Article
PD JUN 2025
PY 2025
AB Background and Objectives: Radiomics is widely used to assist in
   clinical decision-making, disease diagnosis, and treatment planning for
   various target organs, including the breast. Recent advances in large
   language models (LLMs) have helped enhance radiomics analysis. Materials
   and Methods: Herein, we sought to improve radiomics analysis by
   incorporating LLM-learned clinical knowledge, to classify benign and
   malignant tumors in breast mammography. We extracted radiomics features
   from the mammograms based on the region of interest and retained the
   features related to the target task. Using prompt engineering, we
   devised an input sequence that reflected the selected features and the
   target task. The input sequence was fed to the chosen LLM (LLaMA
   variant), which was fine-tuned using low-rank adaptation to enhance
   radiomics features. This was then evaluated on two mammogram datasets
   (VinDr-Mammo and INbreast) against conventional baselines. Results: The
   enhanced radiomics-based method performed better than baselines using
   conventional radiomics features tested on two mammogram datasets,
   achieving accuracies of 0.671 for the VinDr-Mammo dataset and 0.839 for
   the INbreast dataset. Conventional radiomics models require retraining
   from scratch for an unseen dataset using a new set of features. In
   contrast, the model developed in this study effectively reused the
   common features between the training and unseen datasets by explicitly
   linking feature names with feature values, leading to extensible
   learning across datasets. Our method performed better than the baseline
   method in this retraining setting using an unseen dataset. Conclusions:
   Our method, one of the first to incorporate LLM into radiomics, has the
   potential to improve radiomics analysis.
ZS 0
Z8 0
ZR 0
TC 0
ZA 0
ZB 0
Z9 0
DA 2025-04-21
UT WOS:001466026900001
PM 40203779
ER

PT J
AU Bhayana, Rajesh
   Alwahbi, Omar
   Ladak, Aly Muhammad
   Deng, Yangqing
   Dias, Adriano Basso
   Elbanna, Khaled
   Gomez, Jorge Abreu
   Jajodia, Ankush
   Jhaveri, Kartik
   Johnson, Sarah
   Kajal, Dilkash
   Wang, David
   Soong, Christine
   Kielar, Ania
   Krishna, Satheesh
TI Leveraging Large Language Models to Generate Clinical Histories for
   Oncologic Imaging Requisitions
SO RADIOLOGY
VL 314
IS 2
AR e242134
DI 10.1148/radiol.242134
DT Article
PD FEB 2025
PY 2025
AB Background: Clinical information improves imaging interpretation, but
   physician-provided histories on requisitions for oncologic imaging often
   lack key details. Purpose: To evaluate large language models (LLMs) for
   automatically generating clinical histories for oncologic imaging
   requisitions from clinical notes and compare them with original
   requisition histories. Materials and Methods: In total, 207 patients
   with CT performed at a cancer center from January to November 2023 and
   with an electronic health record clinical note coinciding with ordering
   date were randomly selected. A multidisciplinary team informed selection
   of 10 parameters important for oncologic imaging history, including
   primary oncologic diagnosis, treatment history, and acute symptoms.
   Clinical notes were independently reviewed to establish the reference
   standard regarding presence of each parameter. After prompt engineering
   with seven patients, GPT-4 (version 0613; OpenAI) was prompted on April
   9, 2024, to automatically generate structured clinical histories for the
   200 remaining patients. Using the reference standard, LLM extraction
   performance was calculated (recall, precision, F1 score). LLM-generated
   and original requisition histories were compared for completeness
   (proportion including each parameter), and 10 radiologists performed
   pairwise comparison for quality, preference, and subjective likelihood
   of harm. Results: For the 200 LLM-generated histories, GPT-4 performed
   well, extracting oncologic parameters from clinical notes (F1 = 0.983).
   Compared with original requisition histories, LLM-generated histories
   more frequently included parameters critical for radiologist
   interpretation, including primary oncologic diagnosis (99.5% vs 89% [199
   and 178 of 200 histories, respectively]; P < .001), acute or worsening
   symptoms (15% vs 4% [29 and seven of 200]; P < .001), and relevant
   surgery (61% vs 12% [122 and 23 of 200]; P < .001). Radiologists
   preferred LLM-generated histories for imaging interpretation (89% vs 5%,
   7% equal; P < .001), indicating they would enable more complete
   interpretation (86% vs 0%, 15% equal; P < .001) and have a lower
   likelihood of harm (3% vs 55%, 42% neither; P < .001). Conclusion: An
   LLM enabled accurate automated clinical histories for oncologic imaging
   from clinical notes. Compared with original requisition histories,
   LLM-generated histories were more complete and were preferred by
   radiologists for imaging interpretation and perceived safety.
ZA 0
Z8 0
ZR 0
ZB 0
ZS 0
TC 1
Z9 1
DA 2025-03-08
UT WOS:001434851700023
PM 39903072
ER

PT J
AU Sheikh, Mohammad S.
   Dreesman, Benjamin
   Barreto, Erin F.
   Thongprayoon, Charat
   Miao, Jing
   Suppadungsuk, Supawadee
   Mao, Michael A.
   Qureshi, Fawad
   Pham, Justin H.
   Craici, Iasmina M.
   Kashani, Kianoush B.
   Cheungpasitporn, Wisit
TI Identification of kidney-related medications using AI from self-captured
   pill images
SO RENAL FAILURE
VL 46
IS 2
AR 2402075
DI 10.1080/0886022X.2024.2402075
DT Article
PD DEC 31 2024
PY 2024
AB IntroductionChatGPT, a state-of-the-art large language model, has shown
   potential in analyzing images and providing accurate information. This
   study aimed to explore ChatGPT-4 as a tool for identifying commonly
   prescribed nephrology medications across different versions and testing
   dates.Methods25 nephrology medications were obtained from an
   institutional pharmacy. High-quality images of each medication were
   captured using an iPhone 13 Pro Max and uploaded to ChatGPT-4 with the
   query, 'What is this medication?' The accuracy of ChatGPT-4's responses
   was assessed for medication name, dosage, and imprint. The process was
   repeated after 2 weeks to evaluate consistency across different
   versions, including GPT-4, GPT-4 Legacy, and GPT-4.&
   Oslash;.ResultsChatGPT-4 correctly identified 22 out of 25 (88%)
   medications across all versions. However, it misidentified
   Hydrochlorothiazide, Nifedipine, and Spironolactone due to misreading
   imprints. For instance, Nifedipine ER 90 mg was mistaken for Metformin
   Hydrochloride ER 500 mg because 'NF 06' was misread as 'NF 05'.
   Hydrochlorothiazide 50 mg was confused with the 25 mg version due to
   imprint errors, and Spironolactone 25 mg was misidentified as Naproxen
   Sodium or Diclofenac Sodium. Despite these errors, ChatGPT-4 showed 100%
   consistency when retested, correcting misidentifications after receiving
   feedback on the correct imprints.ConclusionChatGPT-4 shows strong
   potential in identifying nephrology medications from self-captured
   images, though challenges with difficult-to-read imprints remain.
   Providing feedback improved accuracy, suggesting ChatGPT-4 could be a
   valuable tool in digital health for medication identification. Future
   research should enhance the model's ability to distinguish similar
   imprints and explore broader integration into digital health platforms.
ZR 0
ZA 0
ZS 0
Z8 0
ZB 1
TC 1
Z9 1
DA 2024-09-21
UT WOS:001310090100001
PM 39258385
ER

PT J
AU Rizvi, Sara
   Ramesh, Ramya
   Sehgal, Vibhor
   Gurusamy, Brinda
   Tran, Jeffrey
   Mackensen, G. Burkhard
   Arnaout, Rima
TI EchoMap Automatically Maps Echocardiogram Report Text to Ontology
SO CIRCULATION
VL 148
MA A19161
DI 10.1161/circ.148.suppl_1.19161
SU 1
DT Meeting Abstract
PD NOV 7 2023
PY 2023
CT American-Heart-Association's Epidemiology and Prevention/Lifestyle and
   Cardiometabolic Health Scientific Sessions
CY NOV 11-13, 2023
CL Philadelphia, PA
SP Amer Heart Assoc
Z8 0
ZB 0
ZA 0
TC 0
ZR 0
ZS 0
Z9 0
DA 2024-03-04
UT WOS:001157891309291
ER

PT J
AU Ghalibafan, Seyyedehfatemeh
   Gonzalez, David J. Taylor
   Cai, Louis Z.
   Chou, Brandon Graham
   Panneerselvam, Sugi
   Barrett, Spencer Conrad
   Djulbegovic, Mak B.
   Yannuzzi, Nicolas A.
TI APPLICATIONS OF MULTIMODAL GENERATIVE ARTIFICIAL INTELLIGENCE IN A
   REAL-WORLD RETINA CLINIC SETTING
SO RETINA-THE JOURNAL OF RETINAL AND VITREOUS DISEASES
VL 44
IS 10
BP 1732
EP 1740
DI 10.1097/IAE.0000000000004204
DT Article
PD OCT 2024
PY 2024
AB Supplemental Digital Content is Available in the Text.Generative
   Pre-trained Transformer 4 with vision aids clinical care and medical
   record keeping using standardized multiple-choice questions. Its
   effectiveness in complex, open-ended medical scenarios, especially in
   retina clinics, is limited, highlighting constraints in offering ocular
   health advice.
   Purpose:This study evaluates a large language model, Generative
   Pre-trained Transformer 4 with vision, for diagnosing vitreoretinal
   diseases in real-world ophthalmology settings.Methods:A retrospective
   cross-sectional study at Bascom Palmer Eye Clinic, analyzing patient
   data from January 2010 to March 2023, assesses Generative Pre-trained
   Transformer 4 with vision's performance on retinal image analysis and
   International Classification of Diseases 10th revision coding across 2
   patient groups: simpler cases (Group A) and complex cases (Group B)
   requiring more in-depth analysis. Diagnostic accuracy was assessed
   through open-ended questions and multiple-choice questions independently
   verified by three retina specialists.Results:In 256 eyes from 143
   patients, Generative Pre-trained Transformer 4-V demonstrated a 13.7%
   accuracy for open-ended questions and 31.3% for multiple-choice
   questions, with International Classification of Diseases 10th revision
   code accuracies at 5.5% and 31.3%, respectively. Accurately diagnosed
   posterior vitreous detachment, nonexudative age-related macular
   degeneration, and retinal detachment. International Classification of
   Diseases 10th revision coding was most accurate for nonexudative
   age-related macular degeneration, central retinal vein occlusion, and
   macular hole in OEQs, and for posterior vitreous detachment,
   nonexudative age-related macular degeneration, and retinal detachment in
   multiple-choice questions. No significant difference in diagnostic or
   coding accuracy was found in Groups A and B.Conclusion:Generative
   Pre-trained Transformer 4 with vision has potential in clinical care and
   record keeping, particularly with standardized questions. Its
   effectiveness in open-ended scenarios is limited, indicating a
   significant limitation in providing complex medical advice.
Z8 0
TC 3
ZR 0
ZA 0
ZB 0
ZS 0
Z9 3
DA 2024-10-23
UT WOS:001334163300013
PM 39287535
ER

PT J
AU Khanmohammadi, R.
   Ghanem, A. I.
   Verdecchia, K.
   Hall, R.
   Elshaikh, M. A.
   Movsas, B.
   Bagher-Ebadian, H.
   Chetty, I. J.
   Ghassemi, M. M.
   Thind, K.
TI A Novel Localized Student-Teacher LLM for Enhanced Toxicity Extraction
   in Radiation Oncology
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3388
BP E632
EP E633
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
Z8 0
TC 0
ZB 0
ZR 0
ZA 0
ZS 0
Z9 0
DA 2024-12-16
UT WOS:001325892302069
ER

PT J
AU El Hajjar, Abdel Hadi
   Motairek, Issam
   Rosenzveig, Akiva
   Syed, Alveena
   Kassab, Joseph
   Al-Dalakta, Astefanos
   Klein, Allan
TI Innovative Use of Large Language Models to Diagnose Pericarditis in
   Patients Referred to a Tertiary Center for Uncertain Diagnosis
SO CIRCULATION
VL 150
MA 4147030
DI 10.1161/circ.150.suppl_1.4147030
SU 1
DT Meeting Abstract
PD NOV 12 2024
PY 2024
CT American-Heart-Association Resuscitation Science Symposium
CY NOV 16-18, 2024
CL Chicago, IL
SP Amer Heart Assoc
Z8 0
ZS 0
ZA 0
ZR 0
ZB 0
TC 0
Z9 0
DA 2025-02-13
UT WOS:001400066403364
ER

PT J
AU Suh, Pae Sun
   Shim, Woo Hyun
   Suh, Chong Hyun
   Heo, Hwon
   Park, Kye Jin
   Kim, Pyeong Hwa
   Choi, Se Jin
   Ahn, Yura
   Park, Sohee
   Park, Ho Young
   Oh, Na Eun
   Han, Min Woo
   Cho, Sung Tan
   Woo, Chang-Yun
   Park, Hyungjun
TI Comparing Large Language Model and Human Reader Accuracy with New
   England Journal of Medicine Image Challenge Case Image Inputs
SO RADIOLOGY
VL 313
IS 3
AR e241668
DI 10.1148/radiol.241668
DT Article
PD DEC 2024
PY 2024
AB Background: Application of multimodal large language models (LLMs) with
   both textual and visual capabilities has been steadily increasing, but
   their ability to interpret radiologic images is still doubted. Purpose:
   To evaluate the accuracy of LLMs and compare it with that of human
   readers with varying levels of experience and to assess the factors
   affecting LLM accuracy in answering New England Journal of Medicine
   Image Challenge cases. Materials and Methods: Radiologic images of cases
   from October 13, 2005, to April 18, 2024, were retrospectively reviewed.
   Using text and image inputs, LLMs (Open AI's GPT-4 Turbo with Vision
   [GPT-4V] and GPT-4 Omni [GPT-4o], Google's DeepMind Gemini 1.5 Pro, and
   Anthropic's Claude 3) provided answers. Human readers (seven junior
   faculty radiologists, two clinicians, one in-training radiologist, and
   one medical student), blinded to the published answers, also answered.
   LLM accuracy with and without image inputs and short (cases from 2005 to
   2015) versus long text inputs (from 2016 to 2024) was evaluated in
   subgroup analysis to determine the effect of these factors. Factor
   analysis was assessed using multivariable logistic regression. Accuracy
   was compared with generalized estimating equations, with multiple
   comparisons adjusted by using Bonferroni correction. Results: A total of
   272 cases were included. GPT-4o achieved the highest overall accuracy
   among LLMs (59.6%; 162 of 272), outperforming a medical student (47.1%;
   128 of 272; P < .001) but not junior faculty (80.9%; 220 of 272; P <
   .001) or the in-training radiologist (70.2%; 191 of 272; P = .003).
   GPT-4o exhibited similar accuracy regardless of image inputs (without
   images vs with images, 54.0% [147 of 272] vs 59.6% [162 of 272],
   respectively; P = .59). Human reader accuracy was unaffected by text
   length, whereas LLMs demonstrated higher accuracy with long text inputs
   (all P < .001). Text input length affected LLM accuracy (odds ratio
   range, 3.2 [95% CI: 1.9, 5.5] to 6.6 [95% CI: 3.7, 12.0]). Conclusion:
   LLMs demonstrated substantial accuracy with text and image inputs,
   outperforming a medical student. However, their accuracy decreased with
   shorter text lengths, regardless of image input.
TC 5
ZR 0
ZB 1
ZA 0
Z8 0
ZS 0
Z9 5
DA 2024-12-21
UT WOS:001377276300006
PM 39656125
ER

PT J
AU Bhayana, Rajesh
   Jajodia, Ankush
   Chawla, Tanya
   Deng, Yangqing
   Bouchard-Fortier, Genevieve
   Haider, Masoom
   Krishna, Satheesh
TI Accuracy of Large Language Model-based Automatic Calculation of
   Ovarian-Adnexal Reporting and Data System MRI Scores from Pelvic MRI
   Reports
SO RADIOLOGY
VL 315
IS 1
AR e241554
DI 10.1148/radiol.241554
DT Article
PD APR 2025
PY 2025
AB Background: Ovarian-Adnexal Reporting and Data System (O-RADS) for MRI
   helps assign malignancy risk, but radiologist adoption is inconsistent.
   Automatic assignment of O-RADS scores from reports could increase
   adoption and accuracy. Purpose: To evaluate the accuracy of large
   language models (LLMs), after strategic optimization, for automatically
   calculating O-RADS scores from reports. Materials and Methods: This
   retrospective single-center study from a large quaternary care cancer
   center included consecutive gadolinium chelate-enhanced pelvic MRI
   reports with at least one assigned O-RADS score from July 2021 to
   October 2023. Reports from January 2018 to October 2019 (before O-RADS
   MRI implementation) were randomly selected for additional testing.
   Reference standard O-RADS scores were determined by radiologists
   interpreting reports. After prompt optimization using a subset of
   reports, two LLM-based strategies were evaluated: few-shot learning with
   GPT-4 (version 0613; OpenAI) prompted with O-RADS rules ("LLM only") and
   a hybrid strategy leveraging GPT-4 to classify features fed into a
   deterministic formula ("hybrid"). Accuracy of each model and originally
   reported scores were calculated and compared using the McNemar test.
   Results: A total of 284 reports from 284 female patients (mean age, 53.2
   years +/- 16.3 [SD]) with 372 adnexal lesions were included: 10 reports
   in the training set (16 lesions), 134 reports in the internal test set 1
   (173 lesions; 158 O-RADS assigned), and 140 reports in internal test set
   2 (183 lesions). For assigning O-RADS MRI scores, the hybrid model
   accuracy (97%; 168 of 173) outperformed LLM-only model (90%; 155 of 173;
   P = .006). For lesions with an originally reported O-RADS score, hybrid
   model accuracy exceeded that of reporting radiologists (97% [153 of 158]
   vs 88% [139 of 158]; P = .004). Hybrid model also outperformed LLM-only
   model for 183 lesions from before O-RADS implementation (95% [173 of
   183] vs 87% [159 of 183], respectively; P = .01). Conclusion: A hybrid
   LLM-based application, combining LLM feature classification with
   deterministic elements, accurately assigned O-RADS MRI scores from
   report descriptions, exceeding both an LLM-only strategy and the
   original reporting radiologist. (c) RSNA, 2025
ZR 0
ZS 0
TC 1
ZA 0
ZB 0
Z8 0
Z9 1
DA 2025-04-20
UT WOS:001464808700007
PM 40167432
ER

PT J
AU Cao, Jennie J.
   Kwon, Daniel H.
   Ghaziani, Tara T.
   Kwo, Paul
   Tse, Gary
   Kesselman, Andrew
   Kamaya, Aya
   Tse, Justin R.
TI Large language models' responses to liver cancer surveillance,
   diagnosis, and management questions: accuracy, reliability, readability
SO ABDOMINAL RADIOLOGY
VL 49
IS 12
BP 4286
EP 4294
DI 10.1007/s00261-024-04501-7
EA AUG 2024
DT Article
PD DEC 2024
PY 2024
AB Purpose To assess the accuracy, reliability, and readability of publicly
   available large language models in answering fundamental questions on
   hepatocellular carcinoma diagnosis and management. Methods Twenty
   questions on liver cancer diagnosis and management were asked in
   triplicate to ChatGPT-3.5 (OpenAI), Gemini (Google), and Bing
   (Microsoft). Responses were assessed by six fellowship-trained
   physicians from three academic liver transplant centers who actively
   diagnose and/or treat liver cancer. Responses were categorized as
   accurate (score 1; all information is true and relevant), inadequate
   (score 0; all information is true, but does not fully answer the
   question or provides irrelevant information), or inaccurate (score - 1;
   any information is false). Means with standard deviations were recorded.
   Responses were considered as a whole accurate if mean score was > 0 and
   reliable if mean score was > 0 across all responses for the single
   question. Responses were also quantified for readability using the
   Flesch Reading Ease Score and Flesch-Kincaid Grade Level. Readability
   and accuracy across 60 responses were compared using one-way ANOVAs with
   Tukey's multiple comparison tests. Results Of the twenty questions,
   ChatGPT answered nine (45%), Gemini answered 12 (60%), and Bing answered
   six (30%) questions accurately; however, only six (30%), eight (40%),
   and three (15%), respectively, were both accurate and reliable. There
   were no significant differences in accuracy between any chatbot. ChatGPT
   responses were the least readable (mean Flesch Reading Ease Score 29;
   college graduate), followed by Gemini (30; college) and Bing (40;
   college; p < 0.001). Conclusion Large language models provide complex
   responses to basic questions on hepatocellular carcinoma diagnosis and
   management that are seldomly accurate, reliable, or readable.
TC 8
ZB 1
ZA 0
ZR 0
Z8 1
ZS 0
Z9 8
DA 2024-08-11
UT WOS:001285078800003
PM 39088019
ER

PT J
AU Sorin, Vera
   Kapelushnik, Noa
   Hecht, Idan
   Zloto, Ofira
   Glicksberg, Benjamin S.
   Bufman, Hila
   Livne, Adva
   Barash, Yiftach
   Nadkarni, Girish N.
   Klang, Eyal
TI Integrated visual and text-based analysis of ophthalmology clinical
   cases using a large language model
SO SCIENTIFIC REPORTS
VL 15
IS 1
AR 4999
DI 10.1038/s41598-025-88948-8
DT Article
PD FEB 10 2025
PY 2025
AB Recent advancements in generative artificial intelligence have enabled
   analysis of text with visual data, which could have important
   implications in healthcare. Diagnosis in ophthalmology is often based on
   a combination of ocular examination, and clinical context. The aim of
   this study was to evaluate the performance of multimodal GPT-4 (GPT-4 V)
   in an integrated analysis of ocular images and clinical text. This
   retrospective study included 40 patients seen in our institution with
   images of their ocular examinations. Cases were selected by a
   board-certified ophthalmologist, to represent various pathologies. We
   provided the model with each patient image, without and then with the
   clinical context. We also asked two non-ophthalmology physicians to
   write diagnoses for each image, without and then with the clinical
   context. Answers for both GPT-4 V and the non-ophthalmologists were
   evaluated by two board-certified ophthalmologists. Performance
   accuracies were calculated and compared. GPT-4 V provided the correct
   diagnosis in 19/40 (47.5%) cases based on images without clinical
   context, and in 27/40 (67.5%) cases when clinical context was provided.
   Non-ophthalmologist physicians provided the correct diagnoses in 24/40
   (60.0%), and 23/40 (57.5%) of cases without clinical context, and in
   29/40 (72.5%) and 27/40 (67.5%) with clinical context. For all study
   participants adding context improved accuracy (p = 0.033). GPT-4 V is
   currently able to simultaneously analyze and integrate visual and
   textual data, and arrive at accurate clinical diagnoses in the majority
   of cases. Multimodal large language models like GPT-4 V have significant
   potential to advance both patient care and research in ophthalmology.
TC 2
ZA 0
ZR 0
Z8 0
ZS 0
ZB 0
Z9 2
DA 2025-02-17
UT WOS:001418722300017
PM 39930078
ER

PT J
AU Gilbert, M.
   Crutchfield, A.
   Luo, B.
   Thind, K.
   Ghanem, A. I.
   Siddiqui, F.
TI Using a Large Language Model (LLM) for Automated Extraction of Discrete
   Elements from Clinical Notes for Creation of Cancer Databases
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3371
BP E625
EP E625
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
Z8 0
ZA 0
ZS 0
ZB 0
ZR 0
TC 1
Z9 1
DA 2024-12-16
UT WOS:001325892302054
ER

PT J
AU Wei, Qiuhong
   Yao, Zhengxiong
   Cui, Ying
   Wei, Bo
   Jin, Zhezhen
   Xu, Ximing
TI Evaluation of ChatGPT-generated medical responses: A systematic review
   and meta-analysis
SO JOURNAL OF BIOMEDICAL INFORMATICS
VL 151
AR 104620
DI 10.1016/j.jbi.2024.104620
EA MAR 2024
DT Article
PD MAR 2024
PY 2024
AB Objective: Large language models (LLMs) such as ChatGPT are increasingly
   explored in medical domains. However, the absence of standard guidelines
   for performance evaluation has led to methodological inconsistencies.
   This study aims to summarize the available evidence on evaluating
   ChatGPT's performance in answering medical questions and provide
   direction for future research. Methods: An extensive literature search
   was conducted on June 15, 2023, across ten medical databases. The
   keyword used was "ChatGPT," without restrictions on publication type,
   language, or date. Studies evaluating ChatGPT's performance in answering
   medical questions were included. Exclusions comprised review articles,
   comments, patents, non-medical evaluations of ChatGPT, and preprint
   studies. Data was extracted on general study characteristics, question
   sources, conversation processes, assessment metrics, and performance of
   ChatGPT. An evaluation framework for LLM in medical inquiries was
   proposed by integrating insights from selected literature. This study is
   registered with PROSPERO, CRD42023456327. Results: A total of 3520
   articles were identified, of which 60 were reviewed and summarized in
   this paper and 17 were included in the meta-analysis. ChatGPT displayed
   an overall integrated accuracy of 56 % (95 % CI: 51 %- 60 %, I2 = 87 %)
   in addressing medical queries. However, the studies varied in question
   resource, questionasking process, and evaluation metrics. As per our
   proposed evaluation framework, many studies failed to report
   methodological details, such as the date of inquiry, version of ChatGPT,
   and inter-rater consistency. Conclusion: This review reveals ChatGPT's
   potential in addressing medical inquiries, but the heterogeneity of the
   study design and insufficient reporting might affect the results'
   reliability. Our proposed evaluation framework provides insights for the
   future study design and transparent reporting of LLM in responding to
   medical questions.
Z8 0
ZS 0
TC 31
ZB 4
ZA 0
ZR 0
Z9 31
DA 2024-05-17
UT WOS:001218826900001
PM 38462064
ER

PT J
AU Cairns, James
   Frood, Russell
   Patel, Chirag
   Scarsbrook, Andrew
TI The Role of AI in Lymphoma: An Update
SO SEMINARS IN NUCLEAR MEDICINE
VL 55
IS 3
BP 377
EP 386
DI 10.1053/j.semnuclmed.2025.02.007
EA APR 2025
DT Review
PD MAY 2025
PY 2025
AB Malignant lymphomas encompass a range of malignancies with incidence
   rising globally, particularly with age. In younger populations, Hodgkin
   and Burkitt lymphomas predominate, while older populations more commonly
   experience subtypes such as diffuse large B-cell, follicular, marginal
   zone, and mantle cell lymphomas. Positron emission tomography/computed
   tomography (PET/CT) using [18F] fluorodeoxyglucose (FDG) is the gold
   standard for staging, treatment response assessment, and prognostication
   in lymphoma. However, interpretation of PET/CT is complex,
   time-consuming, and reliant on expert imaging specialists, exacerbating
   challenges associated with workforce shortages worldwide. Artificial
   intelligence (AI) offers transformative potential across multiple
   aspects of PET/CT imaging in this setting. AI applications in
   appointment planning have demonstrated utility in reducing nonattendance
   rates and improving departmental efficiency. Advanced reconstruction
   techniques leveraging convolutional neural networks (CNNs) enable
   reduced injected activities of radiopharmaceutical and patient dose
   whilst maintaining diagnostic accuracy, particularly benefiting younger
   patients requiring multiple scans. Automated segmentation tools,
   predominantly using 3D U-Net architectures, have improved quantification
   of metrics such as total metabolic tumour volume (TMTV) and total lesion
   glycolysis (TLG), facilitating prognostication and treatment
   stratification. Despite these advancements, challenges remain, including
   variability in segmentation performance, impact on Deauville Score
   interpretation, and standardization of TMTV/TLG measurements. Emerging
   large language models (LLMs) also show promise in enhancing PET/CT
   reporting, converting free-text reports into structured formats, and
   improving patient communication. Further research is required to address
   limitations such as AI-induced errors, physiological uptake
   differentiation, and the integration of AI models into clinical
   workflows. With robust validation and harmonization, AI integration
   could significantly enhance lymphoma care, improving diagnostic
   precision, workflow efficiency, and patient outcomes. Semin Nucl Med
   55:377-386 (c) 2025 The Author(s). Published by Elsevier Inc. This is an
   open access article under the CC BY license
   (http://creativecommons.org/licenses/by/4.0/)
TC 2
ZR 0
ZS 0
ZB 0
ZA 0
Z8 0
Z9 2
DA 2025-05-01
UT WOS:001472935800001
PM 40069036
ER

PT J
AU Liu, ChaoXu
   Wei, MinYan
   Qin, Yu
   Zhang, MeiXiang
   Jiang, Huan
   Xu, JiaLe
   Zhang, YuNing
   Hua, Qing
   Hou, YiQing
   Dong, YiJie
   Xia, ShuJun
   Li, Ning
   Zhou, JianQiao
TI Harnessing Large Language Models for Structured Reporting in Breast
   Ultrasound: A Comparative Study of Open AI (GPT-4.0) and Microsoft Bing
   (GPT-4)
SO ULTRASOUND IN MEDICINE AND BIOLOGY
VL 50
IS 11
BP 1697
EP 1703
DI 10.1016/j.ultrasmedbio.2024.07.007
EA SEP 2024
DT Article
PD NOV 2024
PY 2024
AB Objectives To assess the capabilities of large language models (LLMs),
   including Open AI (GPT-4.0) and Microsoft Bing (GPT-4), in generating
   structured reports, the Breast Imaging Reporting and Data System
   (BI-RADS) categories, and management recommendations from free-text
   breast ultrasound reports. Materials and Methods In this retrospective
   study, 100 free-text breast ultrasound reports from patients who
   underwent surgery between January and May 2023 were gathered. The
   capabilities of Open AI (GPT-4.0) and Microsoft Bing (GPT-4) to convert
   these unstructured reports into structured ultrasound reports were
   studied. The quality of structured reports, BI-RADS categories, and
   management recommendations generated by GPT-4.0 and Bing were evaluated
   by senior radiologists based on the guidelines. Results Open AI
   (GPT-4.0) was better than Microsoft Bing (GPT-4) in terms of performance
   in generating structured reports (88% vs. 55%; p < 0.001), giving
   correct BI-RADS categories (54% vs. 47%; p = 0.013) and providing
   reasonable management recommendations (81% vs. 63%; p < 0.001). As the
   ability to predict benign and malignant characteristics, GPT-4.0
   performed significantly better than Bing (AUC, 0.9317 vs. 0.8177; p <
   0.001), while both performed significantly inferior to senior
   radiologists (AUC, 0.9763; both p < 0.001). Conclusion This study
   highlights the potential of LLMs, specifically Open AI (GPT-4.0), in
   converting unstructured breast ultrasound reports into structured ones,
   offering accurate diagnoses and providing reasonable recommendations.
TC 1
ZA 0
ZB 0
Z8 1
ZR 0
ZS 0
Z9 2
DA 2024-10-05
UT WOS:001322000900001
PM 39138026
ER

PT J
AU Yang, Xintian
   Li, Tongxin
   Su, Qin
   Liu, Yaling
   Kang, Chenxi
   Lyu, Yong
   Zhao, Lina
   Nie, Yongzhan
   Pan, Yanglin
TI Application of large language models in disease diagnosis and treatment
SO CHINESE MEDICAL JOURNAL
VL 138
IS 2
BP 130
EP 142
DI 10.1097/CM9.0000000000003456
DT Review
PD JAN 20 2025
PY 2025
AB Large language models (LLMs) such as ChatGPT, Claude, Llama, and Qwen
   are emerging as transformative technologies for the diagnosis and
   treatment of various diseases. With their exceptional long-context
   reasoning capabilities, LLMs are proficient in clinically relevant
   tasks, particularly in medical text analysis and interactive dialogue.
   They can enhance diagnostic accuracy by processing vast amounts of
   patient data and medical literature and have demonstrated their utility
   in diagnosing common diseases and facilitating the identification of
   rare diseases by recognizing subtle patterns in symptoms and test
   results. Building on their image-recognition abilities, multimodal LLMs
   (MLLMs) show promising potential for diagnosis based on radiography,
   chest computed tomography (CT), electrocardiography (ECG), and common
   pathological images. These models can also assist in treatment planning
   by suggesting evidence-based interventions and improving clinical
   decision support systems through integrated analysis of patient records.
   Despite these promising developments, significant challenges persist
   regarding the use of LLMs in medicine, including concerns regarding
   algorithmic bias, the potential for hallucinations, and the need for
   rigorous clinical validation. Ethical considerations also underscore the
   importance of maintaining the function of supervision in clinical
   practice. This paper highlights the rapid advancements in research on
   the diagnostic and therapeutic applications of LLMs across different
   medical disciplines and emphasizes the importance of policymaking,
   ethical supervision, and multidisciplinary collaboration in promoting
   more effective and safer clinical applications of LLMs. Future
   directions include the integration of proprietary clinical knowledge,
   the investigation of open-source and customized models, and the
   evaluation of real-time effects in clinical diagnosis and treatment
   practices.
TC 2
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
Z9 2
DA 2025-01-25
UT WOS:001400214400011
PM 39722188
ER

PT J
AU Wu, Yuqi
   Mao, Kaining
   Zhang, Yanbo
   Chen, Jie
TI CALLM: Enhancing Clinical Interview Analysis Through Data Augmentation
   With Large Language Models
SO IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS
VL 28
IS 12
BP 7531
EP 7542
DI 10.1109/JBHI.2024.3435085
DT Article
PD DEC 2024
PY 2024
AB The global prevalence of mental health disorders is increasing, leading
   to a significant economic burden estimated in trillions of dollars. In
   automated mental health diagnosis, the scarcity and imbalance of
   clinical data pose considerable challenges for researchers, limiting the
   effectiveness of machine learning algorithms. To cope with this issue,
   this paper aims to introduce a novel clinical transcript data
   augmentation framework by leveraging large language models (CALLM). The
   framework follows a "patient-doctor role-playing" intuition to generate
   realistic synthetic data. In addition, our study introduces a unique
   "Textbook-Assignment-Application" (T-A-A) partitioning approach to offer
   a systematic means of crafting synthetic clinical interview datasets.
   Concurrently, we have also developed a "Response-Reason" prompt
   engineering paradigm to generate highly authentic and diagnostically
   valuable transcripts. By leveraging a fine-tuned DistilBERT model on the
   E-DAIC PTSD dataset, we achieved a balanced accuracy of 0.77, an
   F1-score of 0.70, and an AUC of 0.78 during test set evaluations, which
   showcase robust adaptability in both Zero-Shot Learning (ZSL) and
   Few-Shot Learning (FSL) scenarios. We further compare the CALLM
   framework with other data augmentation methods and PTSD diagnostic works
   and demonstrates consistent improvements. Compared to conventional data
   collection methods, our synthetic dataset not only demonstrates superior
   performance but also incurs less than 1% of the associated costs.
Z8 0
ZS 0
ZB 0
ZR 0
TC 3
ZA 0
Z9 3
DA 2024-12-19
UT WOS:001373825400017
PM 39074002
ER

PT J
AU Peng, Jing-Jie
   Zhang, Yi-Yue
   Li, Rui-Feng
   Zhu, Wen-Jun
   Liu, Hong-Rui
   Li, Hui-Yin
   Liu, Bin
   Cao, Dong-Sheng
   Peng, Jun
   Luo, Xiu-Ju
TI Hybrid approach for drug-target interaction predictions in ischemic
   stroke models
SO ARTIFICIAL INTELLIGENCE IN MEDICINE
VL 161
AR 103067
DI 10.1016/j.artmed.2025.103067
EA JAN 2025
DT Article
PD MAR 2025
PY 2025
AB Multiple cell death mechanisms are triggered during ischemic stroke and
   they are interconnected in a complex network with extensive crosstalk,
   complicating the development of targeted therapies. We therefore propose
   a novel framework for identifying disease-specific drug-target
   interaction (DTI), named strokeDTI, to extract key nodes within an
   interconnected graph network of activated pathways via leveraging
   transcriptomic sequencing data. Our findings reveal that the drugs a
   model can predict are highly representative of the characteristics of
   the database the model is trained on. However, models with comparable
   performance yield diametrically opposite predictions in real testing
   scenarios. Our analysis reveals a correlation between the reported
   literature on drugtarget pairs and their binding scores. Leveraging this
   correlation, we introduced an additional module to assess the predictive
   validity of our model for each unique target, thereby improving the
   reliability of the framework's predictions. Our framework identified
   Cerdulatinib as a potential anti-stroke drug via targeting multiple cell
   death pathways, particularly necroptosis and apoptosis. Experimental
   validation in in vitro and in vivo models demonstrated that Cerdulatinib
   significantly attenuated stroke-induced brain injury via inhibiting
   multiple cell death pathways, improving neurological function, and
   reducing infarct volume. This highlights strokeDTI's potential for
   disease-specific drug-target identification and Cerdulatinib's potential
   as a potent anti-stroke drug.
TC 0
ZS 0
ZR 0
ZA 0
ZB 0
Z8 0
Z9 0
DA 2025-02-12
UT WOS:001413881900001
PM 39956766
ER

PT J
AU Anvari, Sama
   Lee, Yung
   Jin, David S.
   Malone, Sarah
   Collins, Matthew
TI AI IN HEPATOLOGY: A COMPARATIVE ANALYSIS OF CHATGPT-4, BING, AND BARD AT
   ANSWERING CLINICAL QUESTIONS
SO GASTROENTEROLOGY
VL 166
IS 5
MA Su1976
BP S888
EP S888
SU S
DT Meeting Abstract
PD MAY 2024
PY 2024
CT Digestive Disease Week (DDW)
CY MAY 18-21, 2024
CL Washington, DC
TC 0
ZR 0
ZS 0
ZB 0
ZA 0
Z8 0
Z9 0
DA 2024-10-30
UT WOS:001282837703477
ER

PT J
AU Ong, Hannah
   Ong, Joshua
   Cheng, Rebekah
   Wang, Calvin
   Lin, Murong
   Ong, Dennis
TI GPT Technology to Help Address Longstanding Barriers to Care in Free
   Medical Clinics
SO ANNALS OF BIOMEDICAL ENGINEERING
VL 51
IS 9
BP 1906
EP 1909
DI 10.1007/s10439-023-03256-4
EA JUN 2023
DT Article
PD SEP 2023
PY 2023
AB The implementation of technology in healthcare has revolutionized
   patient-centered decision making by providing contextualized information
   about a patient's healthcare journey, leading to increased efficiency
   (Keyworth et al. in BMC Med Inform Decis Mak 18:93, 2018,
   https://doi.org/10.1186/s12911-018-0661-3). Artificial intelligence has
   been integrated within Electronic Health Records (EHR) to prompt
   screenings or diagnostic tests based on a patient's holistic health
   profile. While larger hospitals have already widely adopted these
   technologies, free clinics hold lower utilization of these advanced
   capability EHRs. The patient population at a free clinic faces a
   multitude of factors that limits their access to comprehensive care,
   thus requiring necessary efforts and measures to close the gap in
   healthcare disparities. Emerging Artificial Intelligence (AI)
   technology, such as OpenAI's ChatGPT, GPT-4, and other large language
   models (LLMs) have remarkable potential to improve patient care
   outcomes, promote health equity, and enhance comprehensive and holistic
   care in resource-limited settings. This paper aims to identify areas in
   which integrating these LLM AI advancements into free clinics operations
   can optimize and streamline healthcare delivery to underserved patient
   populations. This paper also identifies areas of improvements in GPT
   that are necessary to deliver those services.
ZR 0
TC 11
ZA 0
ZB 4
Z8 0
ZS 0
Z9 11
DA 2023-07-14
UT WOS:001019903200001
PM 37355478
ER

PT J
AU Djulbegovic, Mak B.
   Bair, Henry
   Gonzalez, David J. Taylor
   Ishikawa, Hiroshi
   Wollstein, Gadi
   Schuman, Joel S.
TI Artificial Intelligence for Optical Coherence Tomography in Glaucoma
SO TRANSLATIONAL VISION SCIENCE & TECHNOLOGY
VL 14
IS 1
AR 27
DI 10.1167/tvst.14.1.27
DT Review
PD JAN 2025
PY 2025
AB Purpose: The integration of artificial intelligence (AI), particularly
   deep learning (DL), with optical coherence tomography (OCT) offers
   significant opportunities in the diagnosis and management of glaucoma.
   This article explores the application of various DL models in enhancing
   OCT capabilities and addresses the challenges associated with their
   clinical implementation. Methods: A review of articles utilizing DL
   models was conducted, including convolutional neural networks (CNNs),
   recurrent neural networks (RNNs), generative adversarial networks
   (GANs), autoencoders, and large language models (LLMs). Key developments
   and practical applications of these models in OCT image analysis were
   emphasized, particularly in the context of enhancing image quality,
   glaucoma diagnosis, and monitoring progression. Results: CNNs excel in
   segmenting retinal layers and detecting glaucomatous damage, whereas
   RNNs are effective in analyzing sequential OCT scans for disease
   progression. GANs enhance image quality and data augmentation, and
   autoencoders facilitate advanced feature extraction. LLMs show promise
   in integrating textual and visual data for comprehensive diagnostic
   assessments. Despite these advancements, challenges such as data
   availability, variability, potential biases, and the need for extensive
   validation persist. Conclusions: DL models are reshaping glaucoma
   management by enhancing OCT's diagnostic capabilities. However, the
   successful translation into clinical practice requires addressing major
   challenges related to data variability, biases, fairness, and model
   validation to ensure accurate and reliable patient care. Translational
   Relevance: This review bridges the gap between basic research and
   clinical care by demonstrating how AI, particularly DL models, can
   markedly enhance OCT's clinical utility in diagnosis, monitoring, and
   prediction, moving toward more individualized, personalized, and precise
   treatment strategies.
ZR 0
Z8 0
ZB 0
TC 0
ZA 0
ZS 0
Z9 0
DA 2025-04-11
UT WOS:001461389800004
PM 39854198
ER

PT J
AU Suh, Pae Sun
   Shim, Woo Hyun
   Suh, Chong Hyun
   Heo, Hwon
   Park, Chae Ri
   Eom, Hye Joung
   Park, Kye Jin
   Choe, Jooae
   Kim, Pyeong Hwa
   Park, Hyo Jung
   Ahn, Yura
   Park, Ho Young
   Choi, Yoonseok
   Woo, Chang-Yun
   Park, Hyungjun
TI Comparing Diagnostic Accuracy of Radiologists versus GPT-4V and Gemini
   Pro Vision Using Image Inputs from Diagnosis Please Cases
SO RADIOLOGY
VL 312
IS 1
AR e240273
DI 10.1148/radiol.240273
DT Article
PD JUL 2024
PY 2024
AB Background: The diagnostic abilities of multimodal large language models
   (LLMs) using direct image inputs and the impact of the temperature
   parameter of LLMs remain unexplored. Purpose: To investigate the ability
   of GPT-4V and Gemini Pro Vision in generating differential diagnoses at
   different temperatures compared with radiologists using Radiology
   Diagnosis Please cases. Materials and Methods: This retrospective study
   included Diagnosis Please cases published from January 2008 to October
   2023. Input images included original images and captures of the textual
   patient history and figure legends (without imaging findings) from PDF
   files of each case. The LLMs were tasked with providing three
   differential diagnoses, repeated five times at temperatures 0, 0.5, and
   1. Eight subspecialty-trained radiologists solved cases. An experienced
   radiologist compared generated and final diagnoses, considering the
   result correct if the generated diagnoses included the final diagnosis
   after five repetitions. Accuracy was assessed across models,
   temperatures, and radiology subspecialties, with statistical
   significance set at P < .007 after Bonferroni correction for multiple
   comparisons across the LLMs at the three temperatures and with
   radiologists. Results: A total of 190 cases were included in
   neuroradiology (n n = 53), multisystem (n n = 27), gastrointestinal (n n
   = 25), genitourinary (n n = 23), musculoskeletal (n n = 17), chest (n n
   = 16), cardiovascular (n n = 12), pediatric (n n = 12), and breast (n n
   = 5) subspecialties. Overall accuracy improved with increasing
   temperature settings (0, 0.5, 1) for both GPT-4V (41% [78 of 190 cases],
   45% [86 of 190 cases], 49% [93 of 190 cases], respectively) and Gemini
   Pro Vision (29% [55 of 190 cases], 36% [69 of 190 cases], 39% [74 of 190
   cases], respectively), although there was no evidence of a statistically
   significant difference after Bonferroni adjustment (GPT-4V, P = .12;
   Gemini Pro Vision, P = .04). The overall accuracy of radiologists (61%
   [115 of 190 cases]) was higher than that of Gemini Pro Vision at
   temperature 1 (T1) (P P < .001), while no statistically significant
   difference was observed between radiologists and GPT4V at T1 after
   Bonferroni adjustment (P P = .02). Radiologists (range, 45%-88%)
   outperformed the LLMs at T1 (range, 24%-75%) in most subspecialties.
   Conclusion: Using direct radiologic image inputs, GPT-4V and Gemini Pro
   Vision showed improved diagnostic accuracy with increasing temperature
   settings. Although GPT-4V slightly underperformed compared with
   radiologists, it nonetheless demonstrated promising potential as a
   supportive tool in diagnostic decision-making. (c) RSNA, 2024
ZA 0
TC 21
ZB 2
ZR 0
Z8 1
ZS 0
Z9 22
DA 2024-08-05
UT WOS:001279556500025
PM 38980179
ER

PT J
AU Rahsepar, Amir Ali
   Tavakoli, Neda
   Kim, Grace Hyun J.
   Hassani, Cameron
   Abtin, Fereidoun
   Bedayat, Arash
TI How AI Responds to Common Lung Cancer Questions: ChatGPT vs Google Bard
SO RADIOLOGY
VL 307
IS 5
DT Article
PD JUN 2023
PY 2023
AB Background: The recent release of large language models (LLMs) for
   public use, such as ChatGPT and Google Bard, has opened up a multitude
   of potential benefits as well as challenges.
   Purpose: To evaluate and compare the accuracy and consistency of
   responses generated by publicly available ChatGPT-3.5 and Google Bard to
   non-expert questions related to lung cancer prevention, screening, and
   terminology commonly used in radiology reports based on the
   recommendation of Lung Imaging Reporting and Data System (Lung-RADS)
   v2022 from American College of Radiology and Fleischner society.
   Materials and Methods: Forty of the exact same questions were created
   and presented to ChatGPT-3.5 and Google Bard experimental version as
   well as Bing and Google search engines by three different authors of
   this paper. Each answer was reviewed by two radiologists for accuracy.
   Responses were scored as correct, partially correct, incorrect, or
   unanswered. Consistency was also evaluated among the answers. Here,
   consistency was defined as the agreement between the three answers
   provided by ChatGPT-3.5, Google Bard experimental version, Bing, and
   Google search engines regardless of whether the concept conveyed was
   correct or incorrect. The accuracy among different tools were evaluated
   using Stata.
   Results: ChatGPT-3.5 answered 120 questions with 85 (70.8%) correct, 14
   (11.7%) partially correct, and 21 (17.5%) incorrect. Google Bard did not
   answer 23 (19.1%) questions. Among the 97 questions answered by Google
   Bard, 62 (51.7%) were correct, 11 (9.2%) were partially correct, and 24
   (20%) were incorrect. Bing answered 120 questions with 74 ( 61.7%)
   correct, 13 (10.8%) partially correct, and 33 (27.5%) incorrect. Google
   search engine answered 120 questions with 66 (55%) correct, 27 (22.5%)
   partially correct, and 27 (22.5%) incorrect. The ChatGPT-3.5 is more
   likely to provide correct or partially answer than Google Bard,
   approximately by 1.5 folds (OR = 1.55, P = 0.004). ChatGPT-3.5 and
   Google search engine were more likely to be consistent than Google Bard
   by approximately 7 and 29 folds (OR = 6.65, P = 0.002 for ChatGPT and OR
   = 28.83, P = 0.002 for Google search engine, respectively).
   Conclusion: Although ChatGPT-3.5 had a higher accuracy in comparison
   with the other tools, neither ChatGPT nor Google Bard, Bing and Google
   search engines answered all questions correctly and with 100%
   consistency.
ZA 0
Z8 1
ZR 0
TC 160
ZS 1
ZB 24
Z9 161
DA 2023-08-19
UT WOS:001022483100018
PM 37310252
ER

PT J
AU Arasteh, Soroosh Tayebi
   Han, Tianyu
   Lotfinia, Mahshad
   Kuhl, Christiane
   Kather, Jakob Nikolas
   Truhn, Daniel
   Nebelung, Sven
TI Large language models streamline automated machine learning for clinical
   studies
SO NATURE COMMUNICATIONS
VL 15
IS 1
AR 1603
DI 10.1038/s41467-024-45879-8
DT Article
PD FEB 21 2024
PY 2024
AB A knowledge gap persists between machine learning (ML) developers (e.g.,
   data scientists) and practitioners (e.g., clinicians), hampering the
   full utilization of ML for clinical data analysis. We investigated the
   potential of the ChatGPT Advanced Data Analysis (ADA), an extension of
   GPT-4, to bridge this gap and perform ML analyses efficiently.
   Real-world clinical datasets and study details from large trials across
   various medical specialties were presented to ChatGPT ADA without
   specific guidance. ChatGPT ADA autonomously developed state-of-the-art
   ML models based on the original study's training data to predict
   clinical outcomes such as cancer development, cancer progression,
   disease complications, or biomarkers such as pathogenic gene sequences.
   Following the re-implementation and optimization of the published
   models, the head-to-head comparison of the ChatGPT ADA-crafted ML models
   and their respective manually crafted counterparts revealed no
   significant differences in traditional performance metrics (p >= 0.072).
   Strikingly, the ChatGPT ADA-crafted ML models often outperformed their
   counterparts. In conclusion, ChatGPT ADA offers a promising avenue to
   democratize ML in medicine by simplifying complex data analyses, yet
   should enhance, not replace, specialized training and resources, to
   promote broader applications in medical research and practice.
   A knowledge gap persists between machine learning developers and
   clinicians. Here, the authors show that the Advanced Data Analysis
   extension of ChatGPT could bridge this gap and simplify complex data
   analyses, making them more accessible to clinicians.
ZA 0
ZS 0
ZB 8
TC 34
Z8 1
ZR 0
Z9 35
DA 2024-03-28
UT WOS:001173879300030
PM 38383555
ER

PT J
AU Yang, Fei
   Li, Xiaochun
   Wang, Xijuan
   Chen, Xuanling
   Niu, Yaqian
   Zhang, Yan
   Zhang, Chengxia
   Liu, Guangfeng
TI Analysis of Optic Disc Morphology and the Peripapillary Retinal and
   Choroidal Thickness by the Swept Source Optical Coherence Tomography in
   Patients with Moyamoya Disease
SO OPHTHALMIC RESEARCH
VL 68
IS 1
BP 61
EP 70
DI 10.1159/000542801
DT Article
PD JAN-DEC 2025
PY 2025
AB Introduction: The study evaluates the performance of large language
   model versions of ChatGPT - ChatGPT-3.5, ChatGPT-4, and ChatGPT-Omni -
   in addressing inquiries related to the diagnosis and treatment of
   gynecological cancers, including ovarian, endometrial, and cervical
   cancers. Methods: A total of 804 questions were equally distributed
   across four categories: true/false, multiple-choice, open-ended, and
   case-scenario, with each question type representing varying levels of
   complexity. Performance was assessed using a six-point Likert scale,
   focusing on accuracy, completeness, and alignment with established
   clinical guidelines. Results: For true/false queries, ChatGPT-Omni
   achieved accuracy rates of 100% for easy, 98% for medium, and 97% for
   complicated questions, higher than ChatGPT-4 (94%, 90%, 85%) and
   ChatGPT-3.5 (90%, 85%, 80%) (p = 0.041, 0.023, 0.014, respectively). In
   multiple-choice, ChatGPT-Omni maintained with 100% for MMD patients was
   significantly less than in controls, while the CDR in MMD patients was
   significantly larger than that in the control group. There was no
   statistically significant difference between the two groups regarding
   disc area, cup area, cup volume, rim volume, vertical and horizontal
   diameter of disc. The retinal thickness at the 7 o'clock position was
   significantly thinner in the MMD group compared to the control group and
   the temporal RNFL thickness, particularly at the 7 o'clock and 9 o'clock
   positions, was significantly reduced in the MMD group (p < 0.05). The
   GCL layer at the 7 o'clock position was thinner in the MMD group than in
   the control group (p < 0.05). The MMD group showed a notably reduced
   average choroidal thickness, particularly in the inferior-temporal
   region (p < 0.05). There was a correlation between peripapillary
   choroidal and GCL layer thickness in the MMD group, but no significant
   correlations were found with rim area, CDR, or RNFL. Conclusions: In
   patients with MMD, there is an increase in the CDR accompanied by a
   decrease in the rim area. Additionally, there is thinning of the
   temporal RNFL, GCL, and choroidal thickness, notably in the
   inferotemporal quadrant of the optic disc.
ZR 0
ZA 0
Z8 0
ZS 0
ZB 0
TC 0
Z9 0
DA 2025-02-01
UT WOS:001404667800001
PM 39586258
ER

PT J
AU Hirosawa, Takanobu
   Harada, Yukinori
   Tokumasu, Kazuki
   Ito, Takahiro
   Suzuki, Tomoharu
   Shimizu, Taro
TI Evaluating ChatGPT-4's Diagnostic Accuracy: Impact of Visual Data
   Integration
SO JMIR MEDICAL INFORMATICS
VL 12
AR e55627
DI 10.2196/55627
DT Article
PD 2024
PY 2024
AB Background: In the evolving field of health care, multimodal generative
   artificial intelligence (AI) systems, such as ChatGPT-4 with vision
   (ChatGPT-4V), represent a significant advancement, as they integrate
   visual data with text data. This integration has the potential to
   revolutionize clinical diagnostics by offering more comprehensive
   analysis capabilities. However, the impact on diagnostic accuracy of
   using image data to augment ChatGPT-4 remains unclear. Objective: This
   study aims to assess the impact of adding image data on ChatGPT-4's
   diagnostic accuracy and provide insights into how image data integration
   can enhance the accuracy of multimodal AI in medical diagnostics.
   Specifically, this study endeavored to compare the diagnostic accuracy
   between ChatGPT-4V, which processed both text and image data, and its
   counterpart, ChatGPT-4, which only uses text data. Methods: We
   identified a total of 557 case reports published in the American Journal
   of Case Reports from January 2022 to March 2023. After excluding cases
   that were nondiagnostic, pediatric, and lacking image data, we included
   363 case descriptions with their final diagnoses and associated images.
   We compared the diagnostic accuracy of ChatGPT-4V and ChatGPT-4 without
   vision based on their ability to include the final diagnoses within
   differential diagnosis lists. Two independent physicians evaluated their
   accuracy, with a third resolving any discrepancies, ensuring a rigorous
   and objective analysis. Results: The integration of image data into
   ChatGPT-4V did not significantly enhance diagnostic accuracy, showing
   that final diagnoses were included in the top 10 differential diagnosis
   lists at a rate of 85.1% (n=309), comparable to the rate of 87.9%
   (n=319) for the text -only version ( P =.33). Notably, ChatGPT-4V's
   performance in correctly identifying the top diagnosis was inferior, at
   44.4% (n=161), compared with 55.9% (n=203) for the text -only version (
   P =.002, chi 2 test). Additionally, ChatGPT-4's self -reports showed
   that image data accounted for 30% of the weight in developing the
   differential diagnosis lists in more than half of cases. Conclusions:
   Our findings reveal that currently, ChatGPT-4V predominantly relies on
   textual data, limiting its ability to fully use the diagnostic potential
   of visual information. This study underscores the need for further
   development of multimodal generative AI systems to effectively integrate
   and use clinical image data. Enhancing the diagnostic performance of
   such AI systems through improved multimodal data integration could
   significantly benefit patient care by providing more accurate and
   comprehensive diagnostic insights. Future research should focus on
   overcoming these limitations, paving the way for the practical
   application of advanced AI in medicine.
ZB 2
Z8 1
TC 11
ZA 0
ZR 0
ZS 0
Z9 11
DA 2024-05-16
UT WOS:001217446200001
PM 38592758
ER

PT J
AU Demir, Suleyman
TI Investigating the role of large language models on questions about
   refractive surgery
SO INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
VL 195
AR 105787
DI 10.1016/j.ijmedinf.2025.105787
EA JAN 2025
DT Article
PD MAR 2025
PY 2025
AB Background: Large language models (LLMs) are becoming increasingly
   popular and are playing an important role in providing accurate clinical
   information to both patients and physicians. This study aimed to
   investigate the effectiveness of ChatGPT-4.0, Google Gemini, and
   Microsoft Copilot LLMs for responding to patient questions regarding
   refractive surgery. Methods: The LLMs' responses to 25 questions about
   refractive surgery, which are frequently asked by patients, were
   evaluated by two ophthalmologists using a 5-point Likert scale, with
   scores ranging from 1 to 5. Furthermore, the DISCERN scale was used to
   assess the reliability of the language models' responses, whereas the
   Flesch Reading Ease and Flesch-Kincaid Grade Level indices were used to
   evaluate readability. Results: Significant differences were found among
   all three LLMs in the Likert scores (p = 0.022). Pairwise comparisons
   revealed that ChatGPT-4.0 ' s Likert score was significantly higher than
   that of Microsoft Copilot, while no significant difference was found
   when compared to Google Gemini (p = 0.005 and p = 0.087, respectively).
   In terms of reliability, ChatGPT-4.0 stood out, receiving the highest
   DISCERN scores among the three LLMs. However, in terms of readability,
   ChatGPT-4.0 received the lowest score. Conclusions: ChatGPT-4.0 ' s
   responses to inquiries regarding refractive surgery were more intricate
   for patients compared to other language models; however, the information
   provided was more dependable and accurate.
ZR 0
Z8 0
ZA 0
ZB 0
TC 0
ZS 0
Z9 0
DA 2025-02-12
UT WOS:001414274100001
PM 39787660
ER

PT J
AU Panagoulias, Dimitrios P.
   Tsoureli-Nikita, Evridiki
   Virvou, Maria
   Tsihrintzis, George A.
TI Dermacen analytica: A novel methodology integrating multi-modal large
   language models with machine learning in dermatology
SO INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
VL 199
AR 105898
DI 10.1016/j.ijmedinf.2025.105898
EA MAR 2025
DT Article
PD JUL 2025
PY 2025
AB Objective: To design, implement, evaluate, and quantify a novel and
   adaptable Artificial Intelligence-empowered methodology aimed at
   supporting a dermatologist's workflow in assessing and diagnosing skin
   conditions, leveraging AI's deep image analytic power and reasoning.
   Skin presents diverse conditions that no single AI solution can
   comprehensively address, suggesting that mimicking a medical
   professional's diagnostic process and creating strategic AI
   interventions may enhance decision-making. Patients and Methods: We
   employ large language, transformer-based vision models for image
   analysis, sophisticated machine learning tools for guideline-based
   segmentation, and measuring tasks in our system. As no single technology
   is sufficient on its own for efficient use by dermatologists, we apply a
   sequential logic with agency to improve outcomes. Results: Using natural
   language processing methods and incorporating human expert evaluation,
   our system achieved a weighted accuracy of 87% on the dataset used,
   demonstrating its reasoning and diagnostic capabilities. Conclusions:
   This study serves as a proof of concept for the application of AI in
   dermatology, highlighting its potential to enhance the patient journey
   for which we approximate the value of such interventions in healthcare
   using graph theory with an associated cost-optimization objective
   function.
Z8 0
TC 0
ZR 0
ZA 0
ZB 0
ZS 0
Z9 0
DA 2025-04-08
UT WOS:001458155100001
PM 40153891
ER

PT J
AU Bhayana, Rajesh
   Nanda, Bipin
   Dehkharghanian, Taher
   Deng, Yangqing
   Bhambra, Nishaant
   Elias, Gavin
   Datta, Daksh
   Kambadakone, Avinash
   Shwaartz, Chaya G.
   Moulton, Carol-Anne
   Henault, David
   Gallinger, Steven
   Krishna, Satheesh
TI Large Language Models for Automated Synoptic Reports and Resectability
   Categorization in Pancreatic Cancer
SO RADIOLOGY
VL 311
IS 3
AR e233117
DI 10.1148/radiol.233117
DT Article
PD JUN 2024
PY 2024
AB Background: Structured radiology reports for pancreatic ductal
   adenocarcinoma (PDAC) improve surgical decision-making over free-text
   reports, but radiologist adoption is variable. Resectability criteria
   are applied inconsistently. Purpose: To evaluate the performance of
   large language models (LLMs) in automatically creating PDAC synoptic
   reports from original reports and to explore performance in categorizing
   tumor resectability. Materials and Methods: In this institutional review
   board-approved retrospective study, 180 consecutive PDAC staging CT
   reports on patients referred to the authors' European Society for
   Medical Oncology-designated cancer center from January to December 2018
   were included. Reports were reviewed by two radiologists to establish
   the reference standard for 14 key findings and National Comprehensive
   Cancer Network (NCCN) resectability category. GPT-3.5 and GPT-4
   (accessed September 18-29, 2023) were prompted to create synoptic
   reports from original reports with the same 14 features, and their
   performance was evaluated (recall, precision, F1 score). To categorize
   resectability, three prompting strategies (default knowledge, in-context
   knowledge, chain-of-thought) were used for both LLMs.
   Hepatopancreaticobiliary surgeons reviewed original and artificial
   intelligence (AI)-generated reports to determine resectability, with
   accuracy and review time compared. The McNemar test, t test, Wilcoxon
   signed-rank test, and mixed effects logistic regression models were used
   where appropriate. Results: GPT-4 outperformed GPT-3.5 in the creation
   of synoptic reports (F1 score: 0.997 vs 0.967, respectively). Compared
   with GPT-3.5, GPT-4 achieved equal or higher F1 scores for all 14
   extracted features. GPT-4 had higher precision than GPT-3.5 for
   extracting superior mesenteric artery involvement (100% vs 88.8%,
   respectively). For categorizing resectability, GPT-4 outperformed
   GPT-3.5 for each prompting strategy. For GPT-4, chain-of-thought
   prompting was most accurate, outperforming in-context knowledge
   prompting (92% vs 83%, respectively; P = .002), which outperformed the
   default knowledge strategy (83% vs 67%, P < .001). Surgeons were more
   accurate in categorizing resectability using AI-generated reports than
   original reports (83% vs 76%, respectively; P = .03), while spending
   less time on each report (58%; 95% CI: 0.53, 0.62). Conclusion: GPT-4
   created near-perfect PDAC synoptic reports from original reports. GPT-4
   with chain-of-thought achieved high accuracy in categorizing
   resectability. Surgeons were more accurate and efficient using
   AI-generated reports. (c) RSNA, 2024 Supplemental material is available
   for this article .
ZS 0
ZA 0
ZR 0
ZB 2
Z8 2
TC 15
Z9 17
DA 2024-07-28
UT WOS:001272193800035
PM 38888478
ER

PT J
AU Schwieger, Arne
   Angst, Katrin
   de Bardeci, Mateo
   Burrer, Achim
   Cathomas, Flurin
   Ferrea, Stefano
   Gratz, Franziska
   Knorr, Marius
   Kronenberg, Golo
   Spiller, Tobias
   Troi, David
   Seifritz, Erich
   Weber, Samantha
   Olbrich, Sebastian
TI Large language models can support generation of standardized discharge
   summaries - A retrospective study utilizing ChatGPT-4 and electronic
   health records
SO INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
VL 192
AR 105654
DI 10.1016/j.ijmedinf.2024.105654
EA OCT 2024
DT Article
PD DEC 2024
PY 2024
AB Objective: To evaluate whether psychiatric discharge summaries (DS)
   generated with ChatGPT-4 from electronic health records (EHR) can match
   the quality of DS written by psychiatric residents. Methods: At a
   psychiatric primary care hospital, we compared 20 inpatient DS, written
   by residents, to those written with ChatGPT-4 from pseudonymized
   residents' notes of the patients' EHRs and a standardized prompt. 8
   blinded psychiatry specialists rated both versions on a custom Likert
   scale from 1 to 5 across 15 quality subcategories. The primary outcome
   was the overall rating difference between the two groups. The secondary
   outcomes were the rating differences at the level of individual
   question, case, and rater. Results: Human-written DS were rated
   significantly higher than AI (mean ratings: human 3.78, AI 3.12, p <
   0.05). They surpassed AI significantly in 12/15 questions and 16/20
   cases and were favored significantly by 7/8 raters. For "low expected
   correction effort", human DS were rated as 67 % favorable, 19 % neutral,
   and 14 % unfavorable, whereas AI-DS were rated as 22 % favorable, 33 %
   neutral, and 45 % unfavorable. Hallucinations were present in 40 % of
   AI-DS, with 37.5 % deemed highly clinically relevant. Minor content
   mistakes were found in 30 % of AI and 10 % of human DS. Raters correctly
   identified AI-DS with 81 % sensitivity and 75 % specificity. Discussion:
   Overall, AI-DS did not match the quality of resident-written DS but
   performed similarly in 20% of cases and were rated as favorable for "low
   expected correction effort" in 22% of cases. AI-DS lacked most in
   content specificity, ability to distill key case information, and
   coherence but performed adequately in conciseness, adherence to
   formalities, relevance of included content, and form. Conclusion:
   LLM-written DS show potential as templates for physicians to finalize,
   potentially saving time in the future.
ZA 0
ZR 0
Z8 0
ZS 0
ZB 1
TC 5
Z9 5
DA 2024-11-07
UT WOS:001343302900001
PM 39437512
ER

EF