FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Agaronnik, Nicole D
   Davis, Joshua
   Manz, Christopher R
   Tulsky, James A
   Lindvall, Charlotta
TI Feasibility Study for Using Large Language Models to Identify
   Goals-of-Care Documentation at Scale in Patients With Advanced Cancer.
SO JCO oncology practice
BP OP2400992
EP OP2400992
DI 10.1200/OP-24-00992
DT Journal Article
PD 2025-Apr-10
PY 2025
AB PURPOSE: The purpose of our study was to (1) use a large language model
   (LLM) to identify goals-of-care (GOC) conversations in a large volume of
   notes, and (2) explore the potential of LLMs for a novel summarization
   task.
   METHODS: We included patients diagnosed with advanced cancer between
   April 1, 2024, and June 30, 2024. A validated LLM prompt for GOC was
   applied to electronic health records (EHRs) using a Health Insurance
   Portability and Accountability Act (HIPPA)-secure version of GPT-4o, a
   LLM developed by OpenAI. Output included (1) presence or absence of GOC
   documentation, (2) explanations with source text used to inform the
   LLM's determination, and (3) a hallucination score, indicating
   proportion of source text generated by the LLM that did not perfectly
   match text in the EHR. Two LLM prompts were designed to generate
   structured and unstructured GOC summaries. We randomly selected five
   patients and applied the summarization task to notes flagged by LLM as
   containing GOC. We reviewed LLM summaries to examine for relevant
   information.
   RESULTS: Among 326 patients associated with nearly 1,400 clinical notes,
   LLM flagged approximately 40% of notes for GOC documentation. Subsequent
   review of explanation text identified that 128 patients (nearly 40% of
   the total patient population) had GOC documentation. The hallucination
   index for explanations was low, suggesting that the LLM did not produce
   text that was not found in EHRs. LLM prompts produced accurate summaries
   in less than 2 minutes per patient.
   CONCLUSION: LLMs can capture GOC at scale and generate clinically useful
   summaries. Future directions include real-time implementation in the
   clinical setting.
ZR 0
TC 0
Z8 0
ZS 0
ZB 0
ZA 0
Z9 0
DA 2025-04-13
UT MEDLINE:40209123
PM 40209123
ER

PT J
AU Hao, Yuexing
   Holmes, Jason
   Hobson, Jared
   Bennett, Alexandra
   McKone, Elizabeth L
   Ebner, Daniel K
   Routman, David M
   Shiraishi, Satomi
   Patel, Samir H
   Yu, Nathan Y
   Hallemeier, Chris L
   Ball, Brooke E
   Waddle, Mark
   Liu, Wei
TI Retrospective Comparative Analysis of Prostate Cancer In-Basket
   Messages: Responses From Closed-Domain Large Language Models Versus
   Clinical Teams.
SO Mayo Clinic proceedings. Digital health
VL 3
IS 1
DI 10.1016/j.mcpdig.2025.100198
DT Journal Article
PD 2025-Mar
PY 2025
AB Objective: To evaluate the effectiveness of RadOnc-generative pretrained
   transformer (GPT), a GPT-4 based large language model, in assisting with
   in-basket message response generation for prostate cancer treatment,
   with the goal of reducing the workload and time on clinical care teams
   while maintaining response quality.
   Patients and Methods: RadOnc-GPT was integrated with electronic health
   records from both Mayo Clinic-wide databases and a
   radiation-oncology-specific database. The model was evaluated on 158
   previously recorded in-basket message interactions, selected from 90
   patients with nonmetastatic prostate cancer from the Mayo Clinic
   Department of Radiation Oncology in-basket message database in the
   calendar years 2022-2024. Quantitative natural language processing
   analysis and 2 grading studies, conducted by 5 clinicians and 4 nurses,
   were used to assess RadOnc-GPT's responses. Three primary clinicians
   independently graded all messages, whereas a fourth senior clinician
   reviewed 41 responses with relevant discrepancies, and a fifth senior
   clinician evaluated 2 additional responses. The grading focused on 5 key
   areas: completeness, correctness, clarity, empathy, and editing time.
   The grading study was performed from July 20, 2024 to December 15, 2024.
   Results: The RadOnc-GPT slightly outperformed the clinical care team in
   empathy, whereas achieving comparable scores with the clinical care team
   in completeness, correctness, and clarity. Five clinician graders
   identified key limitations in RadOnc-GPT's responses, such as lack of
   context, insufficient domain-specific knowledge, inability to perform
   essential meta-tasks, and hallucination. It was estimated that
   RadOnc-GPT could save an average of 5.2 minutes per message for nurses
   and 2.4 minutes for clinicians, from reading the inquiry to sending the
   response.
   Conclusion: RadOnc-GPT has the potential to considerably reduce the
   workload of clinical care teams by generating high-quality, timely
   responses for in-basket message interactions. This could lead to
   improved efficiency in health care workflows and reduced costs while
   maintaining or enhancing the quality of communication between patients
   and health care providers.Abbreviations and AcronymsAI; artificial
   intelligence; LLM; large language model; NLP; natural language
   processing; RadOnc-GPT; radiation oncology generative pretrained
   transformer.
ZA 0
ZR 0
Z8 0
ZB 0
TC 0
ZS 0
Z9 0
DA 2025-03-28
UT MEDLINE:40130001
PM 40130001
ER

PT J
AU Wu, Xuzhou
   Li, Guangxin
   Wang, Xing
   Xu, Zeyu
   Wang, Yingni
   Lei, Shuge
   Xian, Jianming
   Wang, Xueyu
   Zhang, Yibao
   Li, Gong
   Yuan, Kehong
TI Diagnosis assistant for liver cancer utilizing a large language model
   with three types of knowledge
SO PHYSICS IN MEDICINE AND BIOLOGY
VL 70
IS 9
AR 095009
DI 10.1088/1361-6560/adcb17
DT Article
PD MAY 4 2025
PY 2025
AB Objective. Liver cancer has a high incidence rate, but experienced
   doctors are lacking in primary healthcare settings. The development of
   large models offers new possibilities for diagnosis. However, in liver
   cancer diagnosis, large models face certain limitations, such as
   insufficient understanding of specific medical images, inadequate
   consideration of liver vessel factors, and inaccuracies in reasoning
   logic. Therefore, this study proposes a diagnostic assistance tool
   specific to liver cancer to enhance the diagnostic capabilities of
   primary care doctors. Approach. A liver cancer diagnosis framework
   combining large and small models is proposed. A more accurate model for
   liver tumor segmentation and a more precise model for liver vessel
   segmentation are developed. The features extracted from the segmentation
   results of the small models are combined with the patient's medical
   records and then provided to the large model. The large model employs
   chain of thought prompts to simulate expert diagnostic reasoning and
   uses Retrieval-Augmented Generation to provide reliable answers based on
   trusted medical knowledge and cases. Main results. In the small model
   part, the proposed liver tumor and liver vessel segmentation methods
   achieve improved performance. In the large model part, this approach
   receives higher evaluation scores from doctors when analyzing patient
   imaging and medical records. Significance. First, a diagnostic framework
   combining small models and large models is proposed to optimize the
   liver cancer diagnosis process. Second, two segmentation models are
   introduced to compensate for the large model's shortcomings in
   extracting semantic information from images. Third, by simulating
   doctors' reasoning and integrating trusted knowledge, the framework
   enhances the reliability and interpretability of the large model's
   responses while reducing hallucination phenomena.
Z8 0
ZS 0
ZA 0
ZR 0
TC 0
ZB 0
Z9 0
DA 2025-05-08
UT WOS:001480266600001
PM 40203862
ER

PT J
AU Anonymous
TI Meeting of the Anaesthetic-Research-Society, London, UK, May 16 -17,
   2024 
SO British Journal of Anaesthesia
VL 133
IS 2
BP 458
EP 472
DT Meeting
PD AUG 2024
PY 2024
AB This "Abstracts from Anesthetic Research Society Meeting", which focuses
   on different anesthesia treatments to patient during various treatment
   interventions like surgery or other diagnostic or therapeutic
   procedures, contains approximately 23 abstract presentations, written in
   English. Topics include local anaesthetic treatment, cancer surgery,
   perioperative management, cell apoptosis, cell proliferation, general
   anaesthesia, colorectal cancer, quality-of-life, length of hospital
   stay, patient-reported ethnicity, postpartum hemorrhage. Other topics
   include large language model, hallucination, questionnaire,
   perioperative medication advice, proteomic analysis, lung resection,
   cardiac magnetic resonance imaging, extracellular volume, plasma
   protein, lung protective ventilation, conventional ventilation,
   postoperative pulmonary complication, major noncardiac surgery:,
   myocardial inflammation.
CT Meeting of the Anaesthetic-Research-Society
CY May 16 -17, 2024
CL London, UK
HO London, UK
SP Anaesthet Res Soc
Z8 0
TC 0
ZA 0
ZB 0
ZR 0
ZS 0
Z9 0
DA 2024-08-30
UT BCI:BCI202400741698
ER

PT J
AU Gibson, Damien
   Jackson, Stuart
   Shanmugasundaram, Ramesh
   Seth, Ishith
   Siu, Adrian
   Ahmadi, Nariman
   Kam, Jonathan
   Mehan, Nicholas
   Thanigasalam, Ruban
   Jeffery, Nicola
   Patel, Manish, I
   Leslie, Scott
TI Evaluating the Efficacy of ChatGPT as a Patient Education Toolin
   Prostate Cancer: Multimetric Assessment
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 26
AR e55939
DI 10.2196/55939
DT Article
PD AUG 14 2024
PY 2024
AB Background: Artificial intelligence (AI) chatbots, such as ChatGPT, have
   made significant progress. These chatbots, particularlypopular among
   health care professionals and patients, are transforming patient
   education and disease experience with personalizedinformation. Accurate,
   timely patient education is crucial for informed decision-making,
   especially regarding prostate-specificantigen screening and treatment
   options. However, the accuracy and reliability of AI chatbots'medical
   information must berigorously evaluated. Studies testing ChatGPT's
   knowledge of prostate cancer are emerging, but there is a need for
   ongoingevaluation to ensure the quality and safety of information
   provided to patients. Objective: This study aims to evaluate the
   quality, accuracy, and readability of ChatGPT-4's responses to common
   prostatecancer questions posed by patients. Methods: Overall, 8
   questions were formulated with an inductive approach based on
   information topics in peer-reviewedliterature and Google Trends data.
   Adapted versions of the Patient Education Materials Assessment Tool for
   AI (PEMAT-AI),Global Quality Score, and DISCERN-AI tools were used by 4
   independent reviewers to assess the quality of the AI responses.The 8 AI
   outputs were judged by 7 expert urologists, using an assessment
   framework developed to assess accuracy, safety,appropriateness,
   actionability, and effectiveness. The AI responses'readability was
   assessed using established algorithms (FleschReading Ease score, Gunning
   Fog Index, Flesch-Kincaid Grade Level, The Coleman-Liau Index, and
   Simple Measure ofGobbledygook [SMOG] Index). A brief tool (Reference
   Assessment AI [REF-AI]) was developed to analyze the referencesprovided
   by AI outputs, assessing for reference hallucination, relevance, and
   quality of references. Results: The PEMAT-AI understandability score was
   very good (mean 79.44%, SD 10.44%), the DISCERN-AI rating wasscored as
   "good" quality (mean 13.88, SD 0.93), and the Global Quality Score was
   high (mean 4.46/5, SD 0.50). Natural Language Assessment Tool for AI had
   pooled mean accuracy of 3.96 (SD 0.91), safety of 4.32 (SD 0.86),
   appropriateness of 4.45 (SD0.81), actionability of 4.05 (SD 1.15), and
   effectiveness of 4.09 (SD 0.98). The readability algorithm consensus was
   "difficult toread" (Flesch Reading Ease score mean 45.97, SD 8.69;
   Gunning Fog Index mean 14.55, SD 4.79), averaging an 11th-gradereading
   level, equivalent to 15- to 17-year-olds (Flesch-Kincaid Grade Level
   mean 12.12, SD 4.34; The Coleman-Liau Indexmean 12.75, SD 1.98; SMOG
   Index mean 11.06, SD 3.20). REF-AI identified 2 reference
   hallucinations, while the majority(28/30, 93%) of references
   appropriately supplemented the text. Most references (26/30, 86%) were
   from reputable governmentorganizations, while a handful were direct
   citations from scientific literature. Conclusions: Our analysis found
   that ChatGPT-4 provides generally good responses to common prostate
   cancer queries, makingit a potentially valuable tool for patient
   education in prostate cancer care. Objective quality assessment tools
   indicated that thenatural language processing outputs were generally
   reliable and appropriate, but there is room for improvement. (J Med
   Internet Res 2024;26:e55939) doi: 10.2196/55939
ZB 0
Z8 0
ZA 0
ZS 0
ZR 0
TC 11
Z9 11
DA 2024-09-15
UT WOS:001306488700008
PM 39141904
ER

PT J
AU Hou, Yu
   Bishop, Jeffrey R.
   Liu, Hongfang
   Zhang, Rui
TI Improving DietarySupplement Information Retrieval: Development of a
   Retrieval-Augmented Generation System With Large Language Models
SO JOURNAL OF MEDICAL INTERNET RESEARCH
VL 27
AR e67677
DI 10.2196/67677
DT Article
PD MAR 19 2025
PY 2025
AB Background: Dietary supplements (DSs) are widely used to improve health
   and nutrition, but challenges related to misinformation, safety, and
   efficacy persist dueto less stringent regulations compared with
   pharmaceuticals. Accurate and reliable DS information is critical for
   both consumers and health care providers to make informed decisions.
   Objective: This study aimed to enhance DS-related question answering by
   integrating an advanced retrieval-augmented generation (RAG) system with
   the integrated Dietary Supplement Knowledgebase 2.0 (iDISK2.0), a
   dietary supplement knowledge base, to improve accuracy and reliability.
   Methods: We developed iDISK2.0 by integrating updated data from
   authoritative sources, including the Natural Medicines Comprehensive
   Database, the Memorial Sloan Kettering Cancer Center database, Dietary
   Supplement Label Database, and Licensed Natural Health Products
   Database, and applied advanced data cleaning and standardization
   techniques to reduce noise. The RAG system combined the retrieval power
   of a biomedical knowledge graph with the generative capabilities of
   large language models (LLMs) to address limitations of stand-alone LLMs,
   such as hallucination. The system retrieves contextually relevant
   subgraphs from iDISK2.0 based on user queries, enabling accurate and
   evidence-based responses through a user-friendly interface. We evaluated
   the system using true-or-false and multiple-choice questions derived
   from the Memorial Sloan Kettering Cancer Center database and compared
   its performance with stand-alone LLMs. Results: iDISK2.0 integrates
   174,317 entitiesacross 7 categories, including 8091 dietary supplement
   ingredients; 163,806 dietary supplement products; 786 diseases; and 625
   drugs, along with 6 types of relationships. The RAG system achieved an
   accuracy of 99% (990/1000) for true-or-false questions on DS
   effectiveness and 95% (948/100) for multiple-choice questions on DS-drug
   interactions, substantially outperforming stand-alone LLMs like GPT-4o
   (OpenAI), which scored 62% (618/1000) and 52% (517/1000) on these
   respective tasks. The user interface enabled efficient interaction,
   supporting free-form text input and providing accurate responses.
   Integration strategies minimized data noise, ensuring access to
   up-to-date, DS-related information. Conclusions:By integrating a robust
   knowledge graph with RAG and LLM technologies, iDISK2.0 addresses the
   critical limitations of stand-alone LLMs in DS information retrieval.
   This study highlights the importance of combining structured data with
   advanced artificial intelligence methods to improve accuracy and reduce
   misinformation in health care applications. Future work includes
   extending the framework to broader biomedical domains and improving
   evaluation with real-world, open-ended queries.
ZR 0
ZA 0
Z8 0
TC 1
ZS 0
ZB 0
Z9 1
DA 2025-04-28
UT WOS:001471226600004
PM 40106799
ER

PT J
AU Wang, Qingxin
   Wang, Zhongqiu
   Li, Minghua
   Ni, Xinye
   Tan, Rong
   Zhang, Wenwen
   Wubulaishan, Maitudi
   Wang, Wei
   Yuan, Zhiyong
   Zhang, Zhen
   Liu, Cong
TI A feasibility study of automating radiotherapy planning with large
   language model agents
SO PHYSICS IN MEDICINE AND BIOLOGY
VL 70
IS 7
AR 075007
DI 10.1088/1361-6560/adbff1
DT Article
PD APR 6 2025
PY 2025
AB Objective. Radiotherapy planning requires significant expertise to
   balance tumor control and organ-at-risk (OAR) sparing. Automated
   planning can improve both efficiency and quality. This study introduces
   GPT-Plan, a novel multi-agent system powered by the GPT-4 family of
   large language models (LLMs), for automating the iterative radiotherapy
   plan optimization. Approach. GPT-Plan uses LLM-driven agents, mimicking
   the collaborative clinical workflow of a dosimetrist and physicist, to
   iteratively generate and evaluate text-based radiotherapy plans based on
   predefined criteria. Supporting tools assist the agents by leveraging
   historical plans, mitigating LLM hallucinations, and balancing
   exploration and exploitation. Performance was evaluated on 12 lung
   (IMRT) and 5 cervical (VMAT) cancer cases, benchmarked against the ECHO
   auto-planning method and manual plans. The impact of historical plan
   retrieval on efficiency was also assessed. Results. For IMRT lung cancer
   cases, GPT-Plan generated high-quality plans, demonstrating superior
   target coverage and homogeneity compared to ECHO while maintaining
   comparable or better OAR sparing. For VMAT cervical cancer cases, plan
   quality was comparable to a senior physicist and consistently superior
   to a junior physicist, particularly for OAR sparing. Retrieving
   historical plans significantly reduced the number of required
   optimization iterations for lung cases (p < 0.01) and yielded iteration
   counts comparable to those of the senior physicist for cervical cases (p
   = 0.313). Occasional LLM hallucinations have been mitigated by
   self-reflection mechanisms. One limitation was the inaccuracy of
   vision-based LLMs in interpreting dose images. Significance. This
   pioneering study demonstrates the feasibility of automating radiotherapy
   planning using LLM-powered agents for complex treatment decision-making
   tasks. While challenges remain in addressing LLM limitations, ongoing
   advancements hold potential for further refining and expanding
   GPT-Plan's capabilities.
ZS 0
TC 1
ZB 0
ZA 0
ZR 0
Z8 0
Z9 1
DA 2025-04-26
UT WOS:001469440600001
PM 40073507
ER

PT J
AU Zarfati, Mor
   Soffer, Shelly
   Nadkarni, Girish N.
   Klang, Eyal
TI Retrieval-Augmented Generation: Advancing personalized care and research
   in oncology
SO EUROPEAN JOURNAL OF CANCER
VL 220
AR 115341
DI 10.1016/j.ejca.2025.115341
EA MAR 2025
DT Article
PD MAY 2 2025
PY 2025
AB Retrieval-Augmented Generation (RAG) pairs large language models (LLMs)
   with recent data to produce more accurate, context-aware outputs. By
   converting text into numeric embeddings, RAG locates and retrieves
   relevant "chunks" of data, that along with the query, ground the model's
   responses in current, specific information. This process helps reduce
   outdated or fabricated answers. In oncology, RAG has shown particular
   promise. Studies have demonstrated its ability to improve treatment
   recommendations by integrating genetic profiles, strengthened clinical
   trial matching through biomarker analysis, and accelerated drug
   development by clarifying modeldriven insights. Despite its advantages,
   RAG depends on high-quality data. Biased or incomplete sources can lead
   to inaccurate outcomes. Careful implementation and human oversight are
   crucial for ensuring the effectiveness and reliability of RAG in
   oncology.
ZS 0
ZB 0
Z8 0
TC 0
ZR 0
ZA 0
Z9 0
DA 2025-03-21
UT WOS:001444559600001
PM 40068371
ER

PT J
AU Jang, B. S.
   Alcorn, S. R.
   McNutt, T. R.
   Ehsan, U.
TI Hype or Reality: Utility of Large Language Models in Radiation Oncology
SO INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS
VL 120
IS 2
MA 3382
BP E629
EP E630
SU S
DT Meeting Abstract
PD OCT 1 2024
PY 2024
CT 66th International Conference on American-Society-for-Radiation-Oncology
   (ASTRO)
CY SEP 29-OCT 02, 2024
CL Washington, DC
SP Amer Soc Radiat Oncol
ZB 0
Z8 0
ZR 0
ZA 0
TC 0
ZS 0
Z9 0
DA 2024-12-16
UT WOS:001325892302063
ER

EF